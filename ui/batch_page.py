"""
æ‰¹æ¬¡è™•ç†æ¨¡å¼é é¢
åŒ…å«ç‰¹å¾µæ˜ å°„é…ç½®å’Œæ‰¹æ¬¡è™•ç†é‚è¼¯
"""

import streamlit as st
import polars as pl
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# Import components
from .components import (
    show_file_list,
    show_data_metrics,
    get_analysis_numeric_cols,
    show_export_buttons,
)

# Try to import feature mapping
try:
    sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
    from config.feature_mapping_v2 import FeatureMapping, PREDEFINED_MAPPINGS, STANDARD_CATEGORIES
    FEATURE_MAPPING_AVAILABLE = True
except ImportError:
    FEATURE_MAPPING_AVAILABLE = False
    FeatureMapping = None
    PREDEFINED_MAPPINGS = {}
    STANDARD_CATEGORIES = {}

# Try to import ETL modules
try:
    sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
    from etl.parser import ReportParser
    from etl.cleaner import DataCleaner
    ETL_AVAILABLE = True
except ImportError:
    ETL_AVAILABLE = False
    ReportParser = None
    DataCleaner = None


def render_batch_page(selected_files: List[str]):
    """
    æ¸²æŸ“æ‰¹æ¬¡è™•ç†é é¢
    
    Args:
        selected_files: é¸æ“‡çš„æª”æ¡ˆåˆ—è¡¨
    """
    st.header("ğŸ“¦ æ‰¹æ¬¡è™•ç†æ¨¡å¼")
    st.info(f"æº–å‚™è™•ç† {len(selected_files)} å€‹æª”æ¡ˆ")
    
    # Show file list
    show_file_list(selected_files)
    
    st.markdown("---")
    
    # Create tabs for batch processing
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“‹ è§£æè³‡æ–™", 
        "ğŸ§¹ æ¸…æ´—è³‡æ–™", 
        "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
        "ğŸ“ˆ æ™‚é–“åºåˆ—",
        "ğŸ”— é—œè¯çŸ©é™£",
        "ğŸ¯ è³‡æ–™å“è³ª",
        "ğŸ’¾ åŒ¯å‡º"
    ])
    
    with tab1:
        _render_parse_tab(selected_files)
    
    with tab2:
        _render_clean_tab()
    
    with tab3:
        _render_stats_tab()
    
    with tab4:
        _render_timeseries_tab()
    
    with tab5:
        _render_correlation_tab()
    
    with tab6:
        _render_quality_tab()
    
    with tab7:
        _render_export_tab()


def _render_parse_tab(selected_files: List[str]):
    """æ¸²æŸ“è§£æè³‡æ–™æ¨™ç±¤é """
    st.header("åŸå§‹è³‡æ–™è§£æ")
    
    if not ETL_AVAILABLE:
        st.error("ETL æ¨¡çµ„ç„¡æ³•è¼‰å…¥")
        return
    
    # Parse first file to show preview
    if st.button("ğŸ“‚ è§£æä¸¦åˆä½µè³‡æ–™", type="primary"):
        try:
            with st.spinner(f"æ­£åœ¨è§£æ {len(selected_files)} å€‹æª”æ¡ˆ..."):
                data_dir = Path("data/CGMH-TY")
                file_paths = [str(data_dir / f) for f in selected_files]
                
                parser = ReportParser()
                
                # Parse each file and merge
                dfs = []
                for i, fp in enumerate(file_paths):
                    df = parser.parse_file(fp)
                    dfs.append(df)
                
                # Merge all dataframes
                if len(dfs) == 1:
                    merged_df = dfs[0]
                else:
                    merged_df = pl.concat(dfs, how='vertical_relaxed')
                
                st.session_state['batch_merged_df'] = merged_df
                st.session_state['batch_file_count'] = len(selected_files)
                
                st.success(f"âœ… æˆåŠŸè§£æä¸¦åˆä½µ {len(selected_files)} å€‹æª”æ¡ˆï¼Œå…± {len(merged_df):,} ç­†è³‡æ–™")
                st.rerun()
                
        except Exception as e:
            st.error(f"âŒ æ‰¹æ¬¡è™•ç†éŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    # Show preview if data exists
    if 'batch_merged_df' in st.session_state:
        merged_df = st.session_state['batch_merged_df']
        
        # Show basic metrics
        st.subheader("åˆä½µå¾Œè³‡æ–™æ¦‚è¦½")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç¸½åˆ—æ•¸", f"{len(merged_df):,}")
        with col2:
            st.metric("ç¸½æ¬„ä½æ•¸", f"{len(merged_df.columns):,}")
        with col3:
            st.metric("ä¾†æºæª”æ¡ˆæ•¸", st.session_state.get('batch_file_count', 0))
        
        # Data preview
        st.subheader("è³‡æ–™é è¦½ï¼ˆå‰ 50 ç­†ï¼‰")
        st.dataframe(
            merged_df.head(50).to_pandas(),
            use_container_width=True,
            height=400
        )
        
        # Column list
        st.subheader("æ¬„ä½æ¸…å–®")
        col_list = st.columns(4)
        for i, col in enumerate(merged_df.columns):
            with col_list[i % 4]:
                st.text(f"â€¢ {col}")


def _render_clean_tab():
    """æ¸²æŸ“æ¸…æ´—è³‡æ–™æ¨™ç±¤é """
    st.header("è³‡æ–™æ¸…æ´—")
    
    if 'batch_merged_df' not in st.session_state:
        st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
        return
    
    if not ETL_AVAILABLE:
        st.error("ETL æ¨¡çµ„ç„¡æ³•è¼‰å…¥")
        return
    
    merged_df = st.session_state['batch_merged_df']
    
    # Cleaning options
    st.subheader("æ¸…æ´—é¸é …")
    
    col1, col2 = st.columns(2)
    with col1:
        resample_interval = st.selectbox(
            "é‡æ¡æ¨£é–“éš”",
            ["5m", "10m", "15m", "30m", "1h"],
            index=0
        )
    with col2:
        detect_frozen = st.checkbox("æª¢æ¸¬å‡çµè³‡æ–™", value=True)
    
    # Physics-based validation options
    st.subheader("ç‰©ç†é©—è­‰é¸é …")
    col1, col2, col3 = st.columns(3)
    with col1:
        apply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False,
            help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™")
    with col2:
        apply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
            help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚")
    with col3:
        apply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
            help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚")
    
    # Filter options
    filter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
        help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—")
    
    if st.button("ğŸ§¹ é–‹å§‹æ¸…æ´—", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æ¸…æ´—è³‡æ–™..."):
                cleaner = DataCleaner(resample_interval=resample_interval)
                df_clean = cleaner.clean_data(
                    merged_df,
                    apply_steady_state=apply_steady_state,
                    apply_heat_balance=apply_heat_balance,
                    apply_affinity_laws=apply_affinity,
                    filter_invalid=filter_invalid
                )
            
            st.success(f"âœ… æ¸…æ´—å®Œæˆï¼")
            
            # Show metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("åŸå§‹åˆ—æ•¸", f"{len(merged_df):,}")
            with col2:
                st.metric("æ¸…æ´—å¾Œåˆ—æ•¸", f"{len(df_clean):,}")
            with col3:
                retention = len(df_clean) / len(merged_df) * 100 if len(merged_df) > 0 else 0
                st.metric("ä¿ç•™ç‡", f"{retention:.1f}%")
            
            # Validation results
            validation_results = []
            if apply_steady_state and "is_steady_state" in df_clean.columns:
                steady_count = df_clean["is_steady_state"].sum()
                validation_results.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
            if apply_heat_balance and "heat_balance_invalid" in df_clean.columns:
                invalid_count = df_clean["heat_balance_invalid"].sum()
                validation_results.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
            if apply_affinity and "affinity_law_invalid" in df_clean.columns:
                invalid_count = df_clean["affinity_law_invalid"].sum()
                validation_results.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
            
            if validation_results:
                st.info(" | ".join(validation_results))
            
            # Preview
            st.subheader("æ¸…æ´—å¾Œè³‡æ–™é è¦½")
            st.dataframe(
                df_clean.head(100).to_pandas(),
                use_container_width=True,
                height=400
            )
            
            # Frozen data detection
            frozen_cols = [col for col in df_clean.columns if '_frozen' in col]
            if frozen_cols:
                st.subheader("âš ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
                for col in frozen_cols:
                    frozen_count = df_clean[col].sum()
                    if frozen_count > 0:
                        st.warning(f"{col.replace('_frozen', '')}: {frozen_count} ç­†å‡çµè³‡æ–™")
            
            st.session_state['batch_df_clean'] = df_clean
            
        except Exception as e:
            st.error(f"âŒ æ¸…æ´—éŒ¯èª¤: {str(e)}")
            st.exception(e)


def _render_stats_tab():
    """æ¸²æŸ“çµ±è¨ˆè³‡è¨Šæ¨™ç±¤é """
    st.header("çµ±è¨ˆè³‡è¨Š")
    
    df = _get_current_df()
    if df is None:
        st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
        return
    
    # Data status indicator
    if 'batch_df_clean' in st.session_state:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
    else:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
    
    numeric_cols = get_analysis_numeric_cols(df)
    
    if numeric_cols:
        selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols)
        
        if selected_col:
            _show_column_stats(df, selected_col)
    else:
        st.info("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")


def _render_timeseries_tab():
    """æ¸²æŸ“æ™‚é–“åºåˆ—æ¨™ç±¤é """
    st.header("æ™‚é–“åºåˆ—åˆ†æ")
    
    df = _get_current_df()
    if df is None:
        st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
        return
    
    # Data status indicator
    if 'batch_df_clean' in st.session_state:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
    else:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
    
    if 'timestamp' not in df.columns:
        st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
        return
    
    numeric_cols = get_analysis_numeric_cols(df)
    
    if not numeric_cols:
        st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        return
    
    st.subheader("é¸æ“‡æ¬„ä½é€²è¡Œæ™‚é–“åºåˆ—åˆ†æ")
    
    selected_cols = st.multiselect(
        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
        numeric_cols,
        default=[numeric_cols[0]] if numeric_cols else [],
        max_selections=3
    )
    
    if selected_cols:
        pandas_df = df.select(['timestamp'] + selected_cols).to_pandas()
        pandas_df = pandas_df.set_index('timestamp')
        
        st.line_chart(pandas_df)
        
        st.caption(f"æ™‚é–“ç¯„åœ: {df['timestamp'].min()} è‡³ {df['timestamp'].max()}")
        st.caption(f"è³‡æ–™é»æ•¸: {len(df):,}")
    else:
        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")


def _render_correlation_tab():
    """æ¸²æŸ“é—œè¯çŸ©é™£æ¨™ç±¤é """
    st.header("ğŸ”— é—œè¯çŸ©é™£ç†±åœ–")
    
    df = _get_current_df()
    if df is None:
        st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
        return
    
    # Data status indicator
    if 'batch_df_clean' in st.session_state:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
    else:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
    
    from .components import show_correlation_heatmap
    show_correlation_heatmap(df)


def _render_quality_tab():
    """æ¸²æŸ“è³‡æ–™å“è³ªæ¨™ç±¤é """
    st.header("ğŸ¯ è³‡æ–™å“è³ªå„€è¡¨æ¿")
    
    df = _get_current_df()
    if df is None:
        st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
        return
    
    # Data status indicator
    if 'batch_df_clean' in st.session_state:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
    else:
        st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
    
    from .components import show_quality_dashboard, show_physics_validation_status, show_frozen_data_detection
    from .components import calculate_quality_score, show_quality_score
    
    # Overall quality metrics
    show_quality_dashboard(df)
    
    # Physics validation
    st.markdown("---")
    show_physics_validation_status(df)
    
    # Frozen data detection
    if 'batch_df_clean' in st.session_state:
        st.markdown("---")
        show_frozen_data_detection(df)
    
    # Quality score
    st.markdown("---")
    quality_score = calculate_quality_score(df)
    show_quality_score(quality_score)


def _render_export_tab():
    """æ¸²æŸ“åŒ¯å‡ºæ¨™ç±¤é """
    st.header("åŒ¯å‡ºè³‡æ–™")
    
    has_parsed = 'batch_merged_df' in st.session_state
    has_clean = 'batch_df_clean' in st.session_state
    
    if not has_parsed and not has_clean:
        st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
        return
    
    export_type = st.radio(
        "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
        ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"]
    )
    
    df_to_export = None
    if export_type == "è§£æå¾Œè³‡æ–™" and has_parsed:
        df_to_export = st.session_state['batch_merged_df']
    elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and has_clean:
        df_to_export = st.session_state['batch_df_clean']
    
    if df_to_export is not None:
        show_export_buttons(df_to_export, "hvac_batch")
    else:
        st.warning("è«‹å…ˆæ¸…æ´—è³‡æ–™æˆ–é¸æ“‡è§£æå¾Œè³‡æ–™åŒ¯å‡º")


def _get_current_df():
    """ç²å–ç•¶å‰ä½¿ç”¨çš„ DataFrameï¼ˆå„ªå…ˆä½¿ç”¨æ¸…æ´—å¾Œçš„ï¼‰"""
    if 'batch_df_clean' in st.session_state:
        return st.session_state['batch_df_clean']
    elif 'batch_merged_df' in st.session_state:
        return st.session_state['batch_merged_df']
    return None


def _show_column_stats(df: pl.DataFrame, selected_col: str):
    """é¡¯ç¤ºå–®ä¸€æ¬„ä½çµ±è¨ˆè³‡è¨Š"""
    import numpy as np
    
    col_data = df[selected_col]
    col_data_clean = col_data.drop_nulls()
    
    # Metrics
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("å¹³å‡å€¼", f"{col_data_clean.mean():.2f}")
    with col2:
        st.metric("ä¸­ä½æ•¸", f"{col_data_clean.median():.2f}")
    with col3:
        st.metric("æœ€å°å€¼", f"{col_data_clean.min():.2f}")
    with col4:
        st.metric("æœ€å¤§å€¼", f"{col_data_clean.max():.2f}")
    with col5:
        st.metric("æ¨™æº–å·®", f"{col_data_clean.std():.2f}")
    
    # Distribution
    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
    
    pandas_data = col_data_clean.to_pandas()
    
    if len(pandas_data) > 0:
        counts, bin_edges = np.histogram(pandas_data, bins=30)
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        
        import pandas as pd
        hist_df = pd.DataFrame({
            'value': bin_centers,
            'count': counts
        }).set_index('value')
        
        if hist_df['count'].sum() > 0:
            st.bar_chart(hist_df)
        else:
            st.info("è³‡æ–™ç¯„åœå¤ªå°ï¼Œç„¡æ³•ç”¢ç”Ÿåˆ†å¸ƒåœ–")
        
        data_range = col_data_clean.max() - col_data_clean.min()
        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
    else:
        st.warning("æ­¤æ¬„ä½æ²’æœ‰æœ‰æ•ˆæ•¸å€¼")
