import streamlit as st
import polars as pl
import sys
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from etl.parser import ReportParser
from etl.cleaner import DataCleaner

# Try to import ML modules (may not be available if dependencies missing)
try:
    from models.energy_model import ChillerEnergyModel
    from optimization.optimizer import ChillerOptimizer, OptimizationContext
    from optimization.history_tracker import OptimizationHistoryTracker, create_record_from_result
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# Helper function to get numeric columns for analysis (excluding Date/Time)
def get_analysis_numeric_cols(df):
    """Get numeric columns suitable for statistical analysis, excluding Date/Time/timestamp."""
    # Columns to exclude from analysis
    exclude_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
    
    numeric_cols = [
        col for col in df.columns 
        if df[col].dtype in [pl.Float32, pl.Float64, pl.Int64, pl.Int32]
        and col not in exclude_cols
    ]
    return numeric_cols

st.set_page_config(
    page_title="HVAC ETL æ¸¬è©¦å·¥å…·",
    page_icon="ğŸ­",
    layout="wide"
)

st.title("ğŸ­ HVAC å†°æ°´ç³»çµ± - ETL æ¸¬è©¦ä»‹é¢")
st.markdown("**è³‡æ–™è§£æèˆ‡æ¸…æ´—å·¥å…·** | Chiller Plant Optimization")

# Sidebar
st.sidebar.header("âš™ï¸ è¨­å®š")

# Processing mode selection
mode_options = ["å–®ä¸€æª”æ¡ˆ", "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰"]
if ML_AVAILABLE:
    mode_options.append("âš¡ æœ€ä½³åŒ–æ¨¡æ“¬")

processing_mode = st.sidebar.radio(
    "è™•ç†æ¨¡å¼",
    mode_options,
    help="é¸æ“‡å–®ä¸€æª”æ¡ˆã€æ‰¹æ¬¡è™•ç†æˆ–æœ€ä½³åŒ–æ¨¡æ“¬æ¨¡å¼"
)

# File selection based on mode
if processing_mode == "å–®ä¸€æª”æ¡ˆ":
    # File upload
    uploaded_file = st.sidebar.file_uploader(
        "ä¸Šå‚³ CSV å ±è¡¨æª”æ¡ˆ",
        type=['csv'],
        help="é¸æ“‡ TI_ANDY_SCHEDULER_USE_REPORT_*.csv æª”æ¡ˆ"
    )
    
    # Or select from data directory
    st.sidebar.markdown("---")
    st.sidebar.subheader("æˆ–å¾ç¾æœ‰è³‡æ–™é¸æ“‡")
    
    data_dir = Path("data/CGMH-TY")
    if data_dir.exists():
        csv_files = sorted([f.name for f in data_dir.glob("*.csv")])
        selected_file = st.sidebar.selectbox(
            "é¸æ“‡æª”æ¡ˆ",
            [""] + csv_files
        )
    else:
        selected_file = None
        st.sidebar.warning("æ‰¾ä¸åˆ°è³‡æ–™ç›®éŒ„")
    selected_files = []

elif processing_mode == "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰":
    # Batch mode
    uploaded_file = None
    selected_file = None
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("æ‰¹æ¬¡è™•ç†è¨­å®š")
    
    data_dir = Path("data/CGMH-TY")
    if data_dir.exists():
        csv_files = sorted([f.name for f in data_dir.glob("*.csv")])
        st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹æª”æ¡ˆ")
        
        # File range selection
        batch_mode_type = st.sidebar.radio(
            "é¸æ“‡ç¯„åœ",
            ["å…¨éƒ¨æª”æ¡ˆ", "é¸æ“‡æ—¥æœŸç¯„åœ"]
        )
        
        if batch_mode_type == "é¸æ“‡æ—¥æœŸç¯„åœ":
            col1, col2 = st.sidebar.columns(2)
            with col1:
                start_idx = st.number_input("èµ·å§‹æª”æ¡ˆ", 0, len(csv_files)-1, 0)
            with col2:
                end_idx = st.number_input("çµæŸæª”æ¡ˆ", 0, len(csv_files)-1, min(9, len(csv_files)-1))
            
            selected_files = csv_files[start_idx:end_idx+1]
            st.sidebar.caption(f"é¸æ“‡äº† {len(selected_files)} å€‹æª”æ¡ˆ")
        else:
            selected_files = csv_files
        
        # Clear batch data button
        if st.session_state.get('batch_processing_complete', False):
            st.sidebar.markdown("---")
            if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤æ‰¹æ¬¡è™•ç†è³‡æ–™", type="secondary"):
                st.session_state['batch_processing_complete'] = False
                st.session_state.pop('batch_file_count', None)
                st.session_state.pop('batch_auto_clean', None)
                st.rerun()
    else:
        st.sidebar.error("æ‰¾ä¸åˆ°è³‡æ–™ç›®éŒ„")
        selected_files = []

elif processing_mode == "âš¡ æœ€ä½³åŒ–æ¨¡æ“¬":
    # Optimization mode
    uploaded_file = None
    selected_file = None
    selected_files = []
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("æ¨¡å‹è¨­å®š")
    
    # Model file selection
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)
    
    model_files = list(model_dir.glob("*.joblib"))
    if model_files:
        model_file_names = [f.name for f in model_files]
        selected_model = st.sidebar.selectbox(
            "é¸æ“‡å·²è¨“ç·´æ¨¡å‹",
            model_file_names
        )
    else:
        selected_model = None
        st.sidebar.warning("å°šæœªè¨“ç·´æ¨¡å‹")
        st.sidebar.caption("è«‹å…ˆä½¿ç”¨æ‰¹æ¬¡è™•ç†æ¨¡å¼è¨“ç·´æ¨¡å‹")

else:
    uploaded_file = None
    selected_file = None
    selected_files = []

# Main content
# Main content
if processing_mode == "å–®ä¸€æª”æ¡ˆ" and (uploaded_file or selected_file):
    file_path = None
    
    if uploaded_file:
        # Save uploaded file temporarily
        temp_path = Path(f"/tmp/{uploaded_file.name}")
        with open(temp_path, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        file_path = str(temp_path)
        st.success(f"âœ… å·²ä¸Šå‚³: {uploaded_file.name}")
    elif selected_file:
        file_path = str(data_dir / selected_file)
        st.success(f"âœ… å·²é¸æ“‡: {selected_file}")
    
    # Tabs for different views
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“‹ è§£æè³‡æ–™", 
        "ğŸ§¹ æ¸…æ´—è³‡æ–™", 
        "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
        "ğŸ“ˆ æ™‚é–“åºåˆ—",
        "ğŸ”— é—œè¯çŸ©é™£",
        "ğŸ¯ è³‡æ–™å“è³ª",
        "ğŸ’¾ åŒ¯å‡º"
    ])
    
    with tab1:
        st.header("åŸå§‹è³‡æ–™è§£æ")
        
        try:
            with st.spinner("æ­£åœ¨è§£æå ±è¡¨..."):
                parser = ReportParser()
                df_parsed = parser.parse_file(file_path)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{len(df_parsed):,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{len(df_parsed.columns):,}")
            with col3:
                if 'timestamp' in df_parsed.columns:
                    time_range = df_parsed['timestamp'].max() - df_parsed['timestamp'].min()
                    st.metric("æ™‚é–“ç¯„åœ", f"{time_range}")
            
            st.subheader("è³‡æ–™é è¦½ï¼ˆå‰ 100 ç­†ï¼‰")
            st.dataframe(
                df_parsed.head(100).to_pandas(),
                use_container_width=True,
                height=400
            )
            
            st.subheader("æ¬„ä½æ¸…å–®")
            col_list = st.columns(4)
            for i, col in enumerate(df_parsed.columns):
                with col_list[i % 4]:
                    st.text(f"â€¢ {col}")
            
            # Store in session state for other tabs
            st.session_state['df_parsed'] = df_parsed
            
        except Exception as e:
            st.error(f"âŒ è§£æéŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    with tab2:
        st.header("è³‡æ–™æ¸…æ´—")
        
        if 'df_parsed' in st.session_state:
            df_parsed = st.session_state['df_parsed']
            
            # Cleaning options
            st.subheader("æ¸…æ´—é¸é …")
            
            # Basic options
            col1, col2 = st.columns(2)
            with col1:
                resample_interval = st.selectbox(
                    "é‡æ¡æ¨£é–“éš”",
                    ["5m", "10m", "15m", "30m", "1h"],
                    index=0
                )
            with col2:
                detect_frozen = st.checkbox("æª¢æ¸¬å‡çµè³‡æ–™", value=True)
            
            # Physics-based validation options
            st.subheader("ç‰©ç†é©—è­‰é¸é …")
            col1, col2, col3 = st.columns(3)
            with col1:
                apply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False, 
                    help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™")
            with col2:
                apply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
                    help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚")
            with col3:
                apply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
                    help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚")
            
            # Filter options
            filter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
                help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—")
            
            if st.button("ğŸ§¹ é–‹å§‹æ¸…æ´—", type="primary"):
                try:
                    with st.spinner("æ­£åœ¨æ¸…æ´—è³‡æ–™..."):
                        cleaner = DataCleaner(resample_interval=resample_interval)
                        df_clean = cleaner.clean_data(
                            df_parsed,
                            apply_steady_state=apply_steady_state,
                            apply_heat_balance=apply_heat_balance,
                            apply_affinity_laws=apply_affinity,
                            filter_invalid=filter_invalid
                        )
                    
                    st.success(f"âœ… æ¸…æ´—å®Œæˆï¼")
                    
                    # Show metrics
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("åŸå§‹åˆ—æ•¸", f"{len(df_parsed):,}")
                    with col2:
                        st.metric("æ¸…æ´—å¾Œåˆ—æ•¸", f"{len(df_clean):,}")
                    with col3:
                        retention = len(df_clean) / len(df_parsed) * 100 if len(df_parsed) > 0 else 0
                        st.metric("ä¿ç•™ç‡", f"{retention:.1f}%")
                    
                    # Show validation results
                    validation_results = []
                    if apply_steady_state and "is_steady_state" in df_clean.columns:
                        steady_count = df_clean["is_steady_state"].sum()
                        validation_results.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
                    if apply_heat_balance and "heat_balance_invalid" in df_clean.columns:
                        invalid_count = df_clean["heat_balance_invalid"].sum()
                        validation_results.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
                    if apply_affinity and "affinity_law_invalid" in df_clean.columns:
                        invalid_count = df_clean["affinity_law_invalid"].sum()
                        validation_results.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
                    
                    if validation_results:
                        st.info(" | ".join(validation_results))
                    
                    st.subheader("æ¸…æ´—å¾Œè³‡æ–™é è¦½")
                    st.dataframe(
                        df_clean.head(100).to_pandas(),
                        use_container_width=True,
                        height=400
                    )
                    
                    # Check for frozen data flags
                    frozen_cols = [col for col in df_clean.columns if '_frozen' in col]
                    if frozen_cols:
                        st.subheader("âš ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
                        for col in frozen_cols:
                            frozen_count = df_clean[col].sum()
                            if frozen_count > 0:
                                st.warning(f"{col.replace('_frozen', '')}: {frozen_count} ç­†å‡çµè³‡æ–™")
                    
                    st.session_state['df_clean'] = df_clean
                    
                except Exception as e:
                    st.error(f"âŒ æ¸…æ´—éŒ¯èª¤: {str(e)}")
                    st.exception(e)
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab3:
        st.header("çµ±è¨ˆè³‡è¨Š")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
            
            # Select numeric columns for stats (excluding Date/Time)
            numeric_cols = get_analysis_numeric_cols(df)
            
            if numeric_cols:
                selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols)
                
                if selected_col:
                    col_data = df[selected_col]
                    
                    # Filter out nulls for statistics
                    col_data_clean = col_data.drop_nulls()
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        st.metric("å¹³å‡å€¼", f"{col_data_clean.mean():.2f}")
                    with col2:
                        st.metric("ä¸­ä½æ•¸", f"{col_data_clean.median():.2f}")
                    with col3:
                        st.metric("æœ€å°å€¼", f"{col_data_clean.min():.2f}")
                    with col4:
                        st.metric("æœ€å¤§å€¼", f"{col_data_clean.max():.2f}")
                    with col5:
                        st.metric("æ¨™æº–å·®", f"{col_data_clean.std():.2f}")
                    
                    # Distribution visualization
                    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
                    
                    # Convert to pandas for histogram
                    pandas_data = col_data_clean.to_pandas()
                    
                    if len(pandas_data) > 0:
                        # Create histogram using numpy
                        import numpy as np
                        
                        # Calculate histogram bins
                        counts, bin_edges = np.histogram(pandas_data, bins=30)
                        
                        # Create bin labels (using bin centers)
                        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                        
                        # Create dataframe for plotting
                        import pandas as pd
                        hist_df = pd.DataFrame({
                            'value': bin_centers,
                            'count': counts
                        }).set_index('value')
                        
                        if hist_df['count'].sum() > 0:
                            st.bar_chart(hist_df)
                        else:
                            st.info("è³‡æ–™ç¯„åœå¤ªå°ï¼Œç„¡æ³•ç”¢ç”Ÿåˆ†å¸ƒåœ–")
                        
                        # Show data range info
                        data_range = col_data_clean.max() - col_data_clean.min()
                        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
                    else:
                        st.warning("æ­¤æ¬„ä½æ²’æœ‰æœ‰æ•ˆæ•¸å€¼")
            else:
                st.info("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab4:
        st.header("æ™‚é–“åºåˆ—åˆ†æ")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            # Check if timestamp exists
            if 'timestamp' in df.columns:
                # Select numeric columns (excluding Date/Time)
                numeric_cols = get_analysis_numeric_cols(df)
                
                if numeric_cols:
                    st.subheader("é¸æ“‡æ¬„ä½é€²è¡Œæ™‚é–“åºåˆ—åˆ†æ")
                    
                    # Multi-select for comparison
                    selected_cols = st.multiselect(
                        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
                        numeric_cols,
                        default=[numeric_cols[0]] if numeric_cols else [],
                        max_selections=3
                    )
                    
                    if selected_cols:
                        # Create time series chart
                        pandas_df = df.select(['timestamp'] + selected_cols).to_pandas()
                        pandas_df = pandas_df.set_index('timestamp')
                        
                        st.line_chart(pandas_df)
                        
                        # Show data summary
                        st.caption(f"æ™‚é–“ç¯„åœ: {df['timestamp'].min()} è‡³ {df['timestamp'].max()}")
                        st.caption(f"è³‡æ–™é»æ•¸: {len(df):,}")
                    else:
                        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")
                else:
                    st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            else:
                st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
# å–®ä¸€æª”æ¡ˆ tab5 å’Œ tab6 çš„å…§å®¹

    with tab5:
        st.header("ğŸ”— é—œè¯çŸ©é™£ç†±åœ–")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            numeric_cols = get_analysis_numeric_cols(df)
            
            if numeric_cols:
                st.subheader("é¸æ“‡è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
                
                # Let user select variables (max 15 for readability)
                max_vars = min(15, len(numeric_cols))
                selected_vars = st.multiselect(
                    f"é¸æ“‡è¦åˆ†æçš„è®Šæ•¸ï¼ˆæœ€å¤š {max_vars} å€‹ï¼Œå»ºè­° 5-10 å€‹ï¼‰",
                    numeric_cols,
                    default=numeric_cols[:min(8, len(numeric_cols))],
                    max_selections=max_vars
                )
                
                if len(selected_vars) >= 2:
                    try:
                        # Calculate correlation matrix
                        import plotly.figure_factory as ff
                        import numpy as np
                        
                        # Extract data and convert to pandas
                        corr_df = df.select(selected_vars).to_pandas()
                        
                        # Calculate correlation matrix
                        corr_matrix = corr_df.corr()
                        
                        # Create heatmap using plotly
                        fig = ff.create_annotated_heatmap(
                            z=corr_matrix.values,
                            x=list(corr_matrix.columns),
                            y=list(corr_matrix.index),
                            annotation_text=np.around(corr_matrix.values, decimals=2),
                            colorscale='RdBu',
                            zmid=0,
                            showscale=True
                        )
                        
                        fig.update_layout(
                            title="è®Šæ•¸ç›¸é—œæ€§çŸ©é™£",
                            xaxis_title="",
                            yaxis_title="",
                            height=600,
                            xaxis={'side': 'bottom'}
                        )
                        
                        # Rotate x-axis labels
                        fig.update_xaxes(tickangle=45)
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Show interpretation guide
                        st.markdown("---")
                        st.subheader("ğŸ“– ç›¸é—œä¿‚æ•¸è§£è®€")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.markdown("**ğŸ”´ å¼·è² ç›¸é—œ**: -1.0 ~ -0.7")
                            st.caption("ä¸€å€‹è®Šæ•¸å¢åŠ æ™‚ï¼Œå¦ä¸€å€‹æ˜é¡¯æ¸›å°‘")
                        with col2:
                            st.markdown("**âšª ç„¡ç›¸é—œ**: -0.3 ~ 0.3")
                            st.caption("å…©è®Šæ•¸ä¹‹é–“ç„¡æ˜é¡¯ç·šæ€§é—œä¿‚")
                        with col3:
                            st.markdown("**ğŸ”µ å¼·æ­£ç›¸é—œ**: 0.7 ~ 1.0")
                            st.caption("ä¸€å€‹è®Šæ•¸å¢åŠ æ™‚ï¼Œå¦ä¸€å€‹ä¹Ÿå¢åŠ ")
                        
                        # Highlight strong correlations
                        st.markdown("---")
                        st.subheader("ğŸ¯ é¡¯è‘—ç›¸é—œæ€§ï¼ˆ|r| > 0.7ï¼‰")
                        
                        strong_corr = []
                        for i in range(len(corr_matrix)):
                            for j in range(i+1, len(corr_matrix)):
                                corr_val = corr_matrix.iloc[i, j]
                                if abs(corr_val) > 0.7:
                                    var1 = corr_matrix.index[i]
                                    var2 = corr_matrix.columns[j]
                                    strong_corr.append({
                                        'è®Šæ•¸ 1': var1,
                                        'è®Šæ•¸ 2': var2,
                                        'ç›¸é—œä¿‚æ•¸': f"{corr_val:.3f}",
                                        'é¡å‹': 'æ­£ç›¸é—œ ğŸ”µ' if corr_val > 0 else 'è² ç›¸é—œ ğŸ”´'
                                    })
                        
                        if strong_corr:
                            import pandas as pd
                            st.dataframe(pd.DataFrame(strong_corr), use_container_width=True)
                        else:
                            st.info("æ²’æœ‰ç™¼ç¾å¼·ç›¸é—œæ€§ï¼ˆ|r| > 0.7ï¼‰çš„è®Šæ•¸å°")
                    
                    except Exception as e:
                        st.error(f"è¨ˆç®—ç›¸é—œæ€§å¤±æ•—: {str(e)}")
                        st.exception(e)
                else:
                    st.warning("è«‹è‡³å°‘é¸æ“‡ 2 å€‹è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab6:
        st.header("ğŸ¯ è³‡æ–™å“è³ªå„€è¡¨æ¿")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            # Overall quality metrics
            st.subheader("ğŸ“ˆ æ•´é«”è³‡æ–™å“è³ª")
            
            total_rows = len(df)
            total_cols = len(df.columns)
            numeric_cols = get_analysis_numeric_cols(df)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{total_rows:,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{total_cols}")
            with col3:
                st.metric("æ•¸å€¼æ¬„ä½", f"{len(numeric_cols)}")
            with col4:
                if 'timestamp' in df.columns:
                    time_span = df['timestamp'].max() - df['timestamp'].min()
                    st.metric("æ™‚é–“è·¨åº¦", str(time_span))
            
            # Missing data analysis
            st.markdown("---")
            st.subheader("ğŸ” ç¼ºå¤±å€¼åˆ†æ")
            
            # Columns to exclude from missing value analysis (Date/Time related)
            exclude_missing_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
            
            missing_data = []
            for col in df.columns:
                # Skip Date/Time columns
                if col in exclude_missing_cols:
                    continue
                null_count = df[col].null_count()
                if null_count > 0:
                    null_pct = (null_count / total_rows) * 100
                    missing_data.append({
                        'æ¬„ä½åç¨±': col,
                        'ç¼ºå¤±æ•¸é‡': null_count,
                        'ç¼ºå¤±æ¯”ä¾‹': f"{null_pct:.2f}%",
                        'åš´é‡ç¨‹åº¦': 'ğŸ”´ é«˜' if null_pct > 30 else ('ğŸŸ¡ ä¸­' if null_pct > 10 else 'ğŸŸ¢ ä½')
                    })
            
            if missing_data:
                import pandas as pd
                missing_df = pd.DataFrame(missing_data).sort_values('ç¼ºå¤±æ•¸é‡', ascending=False)
                st.dataframe(missing_df, use_container_width=True)
                
                # Visualize missing data
                import plotly.express as px
                fig = px.bar(
                    missing_df.head(10),
                    x='æ¬„ä½åç¨±',
                    y='ç¼ºå¤±æ•¸é‡',
                    title='å‰ 10 å€‹ç¼ºå¤±å€¼æœ€å¤šçš„æ¬„ä½',
                    labels={'ç¼ºå¤±æ•¸é‡': 'ç¼ºå¤±æ•¸é‡', 'æ¬„ä½åç¨±': 'æ¬„ä½'}
                )
                fig.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.success("âœ… æ²’æœ‰ç¼ºå¤±å€¼ï¼")
            
            # Frozen data detection (only if cleaned)
            if 'df_clean' in st.session_state:
                st.markdown("---")
                st.subheader("â„ï¸ å‡çµè³‡æ–™åµæ¸¬")
                
                frozen_cols = [col for col in df.columns if '_frozen' in col]
                
                if frozen_cols:
                    frozen_summary = []
                    for col in frozen_cols:
                        original_col = col.replace('_frozen', '')
                        frozen_count = df[col].sum()
                        if frozen_count > 0:
                            frozen_pct = (frozen_count / total_rows) * 100
                            frozen_summary.append({
                                'æ„Ÿæ¸¬å™¨': original_col,
                                'å‡çµé»æ•¸': frozen_count,
                                'å‡çµæ¯”ä¾‹': f"{frozen_pct:.2f}%",
                                'ç‹€æ…‹': 'ğŸ”´ è­¦å‘Š' if frozen_pct > 5 else 'ğŸŸ¡ æ³¨æ„'
                            })
                    
                    if frozen_summary:
                        import pandas as pd
                        frozen_df = pd.DataFrame(frozen_summary).sort_values('å‡çµé»æ•¸', ascending=False)
                        st.dataframe(frozen_df, use_container_width=True)
                        
                        st.warning("âš ï¸ å‡çµè³‡æ–™å¯èƒ½è¡¨ç¤ºæ„Ÿæ¸¬å™¨æ•…éšœæˆ–æ•¸æ“šå‚³è¼¸å•é¡Œ")
                    else:
                        st.success("âœ… æ²’æœ‰åµæ¸¬åˆ°å‡çµè³‡æ–™")
                else:
                    st.info("è³‡æ–™ä¸­ç„¡å‡çµæ¨™è¨˜æ¬„ä½")
            else:
                st.info("å°šæœªåŸ·è¡Œå‡çµè³‡æ–™åµæ¸¬ï¼ˆéœ€å…ˆæ¸…æ´—è³‡æ–™ï¼‰")
            
            # Physics Validation Status Section
            st.markdown("---")
            st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰ç‹€æ…‹")
            
            validation_cols = st.columns(3)
            
            with validation_cols[0]:
                st.markdown("**ğŸ“Š ç©©æ…‹æª¢æ¸¬**")
                if 'is_steady_state' in df.columns:
                    steady_count = df['is_steady_state'].sum()
                    total_count = len(df)
                    steady_pct = (steady_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç©©æ…‹è³‡æ–™", f"{steady_count:,} ({steady_pct:.1f}%)")
                    
                    # Small bar chart
                    steady_data = {'ç‹€æ…‹': ['ç©©æ…‹', 'éç©©æ…‹'], 'æ•¸é‡': [steady_count, total_count - steady_count]}
                    import pandas as pd
                    st.bar_chart(pd.DataFrame(steady_data).set_index('ç‹€æ…‹'))
                else:
                    st.caption("æœªåŸ·è¡Œç©©æ…‹æª¢æ¸¬")
            
            with validation_cols[1]:
                st.markdown("**ğŸŒ¡ï¸ ç†±å¹³è¡¡é©—è­‰**")
                if 'heat_balance_invalid' in df.columns:
                    invalid_count = df['heat_balance_invalid'].sum()
                    total_count = len(df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    if invalid_pct > 20:
                        st.error("ğŸ”´ ç•°å¸¸æ¯”ä¾‹éé«˜")
                    elif invalid_pct > 10:
                        st.warning("ğŸŸ¡ ç•°å¸¸æ¯”ä¾‹ä¸­ç­‰")
                    else:
                        st.success("ğŸŸ¢ ç•°å¸¸æ¯”ä¾‹æ­£å¸¸")
                else:
                    st.caption("æœªåŸ·è¡Œç†±å¹³è¡¡é©—è­‰")
            
            with validation_cols[2]:
                st.markdown("**âš¡ è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥**")
                if 'affinity_law_invalid' in df.columns:
                    invalid_count = df['affinity_law_invalid'].sum()
                    total_count = len(df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    # Show affinity ratio distribution if available
                    if 'affinity_ratio' in df.columns:
                        ratio_data = df['affinity_ratio'].drop_nulls()
                        if len(ratio_data) > 0:
                            st.caption(f"æ¯”ç‡ç¯„åœ: {ratio_data.min():.4f} ~ {ratio_data.max():.4f}")
                else:
                    st.caption("æœªåŸ·è¡Œè¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥")
            
            # Data completeness timeline
            if 'timestamp' in df.columns and numeric_cols:
                st.markdown("---")
                st.subheader("ğŸ“… è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸")
                
                # Select a representative column to check completeness
                sample_col = st.selectbox(
                    "é¸æ“‡æ¬„ä½æª¢è¦–å®Œæ•´æ€§",
                    numeric_cols
                )
                
                if sample_col:
                    # Create a binary completeness indicator
                    timeline_df = df.select(['timestamp', sample_col]).to_pandas()
                    timeline_df['å®Œæ•´æ€§'] = (~timeline_df[sample_col].isna()).astype(int)
                    timeline_df = timeline_df.set_index('timestamp')
                    
                    import plotly.graph_objects as go
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=timeline_df.index,
                        y=timeline_df['å®Œæ•´æ€§'],
                        mode='lines',
                        fill='tozeroy',
                        name='è³‡æ–™å­˜åœ¨',
                        line=dict(color='green')
                    ))
                    
                    fig.update_layout(
                        title=f"{sample_col} è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸",
                        xaxis_title="æ™‚é–“",
                        yaxis_title="è³‡æ–™å­˜åœ¨ (1=æœ‰, 0=ç„¡)",
                        height=300,
                        yaxis=dict(tickvals=[0, 1], ticktext=['ç¼ºå¤±', 'å­˜åœ¨'])
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
            
            # Data quality score
            st.markdown("---")
            st.subheader("â­ æ•´é«”å“è³ªè©•åˆ†")
            
            # Calculate quality score (0-100)
            quality_score = 100
            
            # Deduct points for missing data
            if missing_data:
                avg_missing_pct = sum([float(d['ç¼ºå¤±æ¯”ä¾‹'].strip('%')) for d in missing_data]) / len(df.columns)
                quality_score -= min(avg_missing_pct, 30)
            
            # Deduct points for frozen data (only if cleaned)
            if 'df_clean' in st.session_state:
                frozen_cols = [col for col in df.columns if '_frozen' in col]
                if frozen_cols:
                    frozen_count = sum([df[col].sum() for col in frozen_cols])
                    frozen_pct = (frozen_count / (total_rows * len(frozen_cols))) * 100 if frozen_cols else 0
                    quality_score -= min(frozen_pct, 20)
            
            quality_score = max(0, quality_score)
            
            # Display score with color coding
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                st.metric("è³‡æ–™å“è³ªè©•åˆ†", f"{quality_score:.1f}/100")
            
            with col2:
                if quality_score >= 90:
                    st.success("ğŸŸ¢ å„ªç§€")
                elif quality_score >= 75:
                    st.info("ğŸ”µ è‰¯å¥½")
                elif quality_score >= 60:
                    st.warning("ğŸŸ¡ å°šå¯")
                else:
                    st.error("ğŸ”´ éœ€æ”¹å–„")
            
            with col3:
                # Progress bar
                st.progress(quality_score / 100)
            
            # Recommendations
            if quality_score < 90:
                st.markdown("---")
                st.subheader("ğŸ’¡ æ”¹å–„å»ºè­°")
                
                if missing_data and len(missing_data) > 0:
                    st.markdown("- æª¢æŸ¥ç¼ºå¤±æ¯”ä¾‹ > 10% çš„æ¬„ä½ï¼Œè€ƒæ…®è£œå€¼æˆ–ç§»é™¤")
                
                if 'df_clean' in st.session_state:
                    frozen_cols = [col for col in df.columns if '_frozen' in col]
                    if frozen_cols:
                        frozen_count = sum([df[col].sum() for col in frozen_cols])
                        if frozen_count > 0:
                            st.markdown("- æª¢æŸ¥å‡çµè³‡æ–™çš„æ„Ÿæ¸¬å™¨ï¼Œå¯èƒ½éœ€è¦ç¶­è­·")
                
                st.markdown("- ç¢ºèªè³‡æ–™æ”¶é›†é »ç‡èˆ‡é æœŸä¸€è‡´")
                st.markdown("- è€ƒæ…®é€²è¡Œç•°å¸¸å€¼åµæ¸¬èˆ‡è™•ç†")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")

    with tab7:
        st.header("åŒ¯å‡ºè³‡æ–™")
        
        if 'df_parsed' in st.session_state or 'df_clean' in st.session_state:
            
            export_type = st.radio(
                "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
                ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"]
            )
            
            df_to_export = None
            if export_type == "è§£æå¾Œè³‡æ–™" and 'df_parsed' in st.session_state:
                df_to_export = st.session_state['df_parsed']
            elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and 'df_clean' in st.session_state:
                df_to_export = st.session_state['df_clean']
            
            if df_to_export is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    # CSV export
                    csv_data = df_to_export.write_csv()
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ CSV",
                        data=csv_data,
                        file_name=f"hvac_etl_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Parquet export (more efficient)
                    # Use BytesIO buffer since write_parquet needs a file
                    from io import BytesIO
                    buffer = BytesIO()
                    df_to_export.write_parquet(buffer)
                    parquet_data = buffer.getvalue()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ Parquet",
                        data=parquet_data,
                        file_name=f"hvac_etl_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
                        mime="application/octet-stream"
                    )
                
                st.info("ğŸ’¡ Parquet æ ¼å¼è¼ƒå°ä¸”æ•ˆèƒ½æ›´å¥½ï¼Œé©åˆå¤§å‹è³‡æ–™é›†")
            else:
                st.warning("è«‹å…ˆæ¸…æ´—è³‡æ–™æˆ–é¸æ“‡è§£æå¾Œè³‡æ–™åŒ¯å‡º")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")

elif processing_mode == "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰" and selected_files:
    st.header("ğŸ“¦ æ‰¹æ¬¡è™•ç†æ¨¡å¼")
    
    st.info(f"æº–å‚™è™•ç† {len(selected_files)} å€‹æª”æ¡ˆ")
    
    # Show file list preview
    with st.expander("æŸ¥çœ‹æª”æ¡ˆæ¸…å–®"):
        if len(selected_files) <= 10:
            for f in selected_files:
                st.text(f"â€¢ {f}")
        else:
            st.text("å‰ 5 å€‹æª”æ¡ˆ:")
            for f in selected_files[:5]:
                st.text(f"  â€¢ {f}")
            st.text(f"  ... ({len(selected_files) - 10} å€‹æª”æ¡ˆ)")
            st.text("å¾Œ 5 å€‹æª”æ¡ˆ:")
            for f in selected_files[-5:]:
                st.text(f"  â€¢ {f}")
    
    # Processing options
    st.subheader("âš™ï¸ è™•ç†é¸é …")
    
    col1, col2 = st.columns(2)
    with col1:
        batch_resample = st.selectbox("é‡æ¡æ¨£é–“éš”", ["5m", "10m", "15m", "30m", "1h"], index=0)
    with col2:
        auto_clean = st.checkbox("è‡ªå‹•æ¸…æ´—è³‡æ–™", value=True)
    
    # Physics-based validation options (only show if auto_clean is enabled)
    if auto_clean:
        st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰é¸é …")
        col1, col2, col3 = st.columns(3)
        with col1:
            batch_apply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False, 
                help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™")
        with col2:
            batch_apply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
                help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚")
        with col3:
            batch_apply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
                help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚")
        
        batch_filter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
            help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—")
    
    # Start batch processing
    if st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡è™•ç†", type="primary"):
        try:
            from etl.batch_processor import BatchProcessor
            from etl.cleaner import DataCleaner
            
            # Prepare file paths
            file_paths = [str(data_dir / f) for f in selected_files]
            
            # Create processor
            processor = BatchProcessor(resample_interval=batch_resample)
            
            # Progress bar
            status_text = st.empty()
            status_text.text("æ­£åœ¨è™•ç†æª”æ¡ˆ...")
            
            if auto_clean:
                # Process files without cleaning first
                with st.spinner("æ­£åœ¨è§£ææª”æ¡ˆ..."):
                    merged_df = processor.process_files(file_paths, clean=False)
                
                # Apply advanced cleaning with physics validation
                status_text.text("æ­£åœ¨åŸ·è¡Œè³‡æ–™æ¸…æ´—èˆ‡é©—è­‰...")
                with st.spinner("æ¸…æ´—èˆ‡é©—è­‰ä¸­..."):
                    cleaner = DataCleaner(resample_interval=batch_resample)
                    merged_df = cleaner.clean_data(
                        merged_df,
                        apply_steady_state=batch_apply_steady_state if auto_clean else False,
                        apply_heat_balance=batch_apply_heat_balance if auto_clean else False,
                        apply_affinity_laws=batch_apply_affinity if auto_clean else False,
                        filter_invalid=batch_filter_invalid if auto_clean else False
                    )
                
                # Store validation results
                st.session_state['batch_validation_results'] = {
                    'steady_state': batch_apply_steady_state if auto_clean else False,
                    'heat_balance': batch_apply_heat_balance if auto_clean else False,
                    'affinity_laws': batch_apply_affinity if auto_clean else False,
                    'filter_invalid': batch_filter_invalid if auto_clean else False
                }
            else:
                with st.spinner("è™•ç†ä¸­..."):
                    merged_df = processor.process_files(file_paths, clean=False)
                
                st.session_state['batch_validation_results'] = None
            
            status_text.text("è™•ç†å®Œæˆ!")
            
            # Store in session state
            if auto_clean:
                st.session_state['df_clean'] = merged_df
                st.session_state['df_parsed'] = merged_df
            else:
                st.session_state['df_parsed'] = merged_df
            
            # Mark batch processing as complete
            st.session_state['batch_processing_complete'] = True
            st.session_state['batch_file_count'] = len(selected_files)
            st.session_state['batch_auto_clean'] = auto_clean
            
        except Exception as e:
            st.error(f"âŒ æ‰¹æ¬¡è™•ç†éŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    # Show analysis tabs if batch processing is complete (persists across re-renders)
    if st.session_state.get('batch_processing_complete', False):
        # Get data from session state
        if 'df_clean' in st.session_state:
            merged_df = st.session_state['df_clean']
        elif 'df_parsed' in st.session_state:
            merged_df = st.session_state['df_parsed']
        else:
            st.error("è³‡æ–™éºå¤±ï¼Œè«‹é‡æ–°åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
            if st.button("é‡ç½®"):
                st.session_state['batch_processing_complete'] = False
                st.rerun()
            st.stop()
        
        batch_file_count = st.session_state.get('batch_file_count', 0)
        auto_clean = st.session_state.get('batch_auto_clean', True)
            
        # Show summary
        st.success(f"âœ… æˆåŠŸè™•ç† {batch_file_count} å€‹æª”æ¡ˆ")
        
        # Show validation results if any were applied
        validation_results = st.session_state.get('batch_validation_results')
        if validation_results:
            result_cols = []
            if validation_results.get('steady_state'):
                steady_count = merged_df['is_steady_state'].sum() if 'is_steady_state' in merged_df.columns else 0
                result_cols.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
            if validation_results.get('heat_balance'):
                invalid_count = merged_df['heat_balance_invalid'].sum() if 'heat_balance_invalid' in merged_df.columns else 0
                result_cols.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
            if validation_results.get('affinity_laws'):
                invalid_count = merged_df['affinity_law_invalid'].sum() if 'affinity_law_invalid' in merged_df.columns else 0
                result_cols.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
            
            if result_cols:
                st.info(" | ".join(result_cols))
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç¸½åˆ—æ•¸", f"{len(merged_df):,}")
        with col2:
            st.metric("ç¸½æ¬„ä½æ•¸", f"{len(merged_df.columns):,}")
        with col3:
            if 'timestamp' in merged_df.columns:
                time_range = merged_df['timestamp'].max() - merged_df['timestamp'].min()
                st.metric("æ™‚é–“ç¯„åœ", str(time_range))
        
        st.markdown("---")
        st.info("ğŸ“Š **è³‡æ–™å·²è¼‰å…¥ï¼** è«‹ä½¿ç”¨ä¸‹æ–¹æ¨™ç±¤é åˆ†æåˆä½µå¾Œçš„è³‡æ–™")
        
        # Analysis tabs
        batch_tab1, batch_tab2, batch_tab3, batch_tab4, batch_tab5, batch_tab6, batch_tab7 = st.tabs([
            "ğŸ“‹ è³‡æ–™é è¦½",
            "ğŸ§¹ æ¸…æ´—è³‡æ–™",
            "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
            "ğŸ“ˆ æ™‚é–“åºåˆ—",
            "ğŸ”— é—œè¯çŸ©é™£",
            "ğŸ¯ è³‡æ–™å“è³ª",
            "ğŸ’¾ åŒ¯å‡º"
        ])
            
        with batch_tab1:
            st.subheader("åˆä½µå¾Œè³‡æ–™é è¦½")
            st.dataframe(merged_df.head(100).to_pandas(), use_container_width=True)
            st.caption(f"é¡¯ç¤ºå‰ 100 ç­†ï¼Œå…± {len(merged_df):,} ç­†è³‡æ–™")
        with batch_tab2:
            st.header("ğŸ§¹ è³‡æ–™æ¸…æ´—")
            
            # Check if we have parsed data to clean
            if 'df_parsed' in st.session_state:
                df_to_clean = st.session_state['df_parsed']
                
                # Cleaning options
                st.subheader("æ¸…æ´—é¸é …")
                
                # Basic options
                col1, col2 = st.columns(2)
                with col1:
                    batch_clean_resample = st.selectbox(
                        "é‡æ¡æ¨£é–“éš”",
                        ["5m", "10m", "15m", "30m", "1h"],
                        index=0,
                        key="batch_clean_resample"
                    )
                with col2:
                    batch_detect_frozen = st.checkbox("æª¢æ¸¬å‡çµè³‡æ–™", value=True, key="batch_detect_frozen")
                
                # Physics-based validation options
                st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰é¸é …")
                col1, col2, col3 = st.columns(3)
                with col1:
                    batch_reapply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False, 
                        help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™",
                        key="batch_reapply_steady")
                with col2:
                    batch_reapply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
                        help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚",
                        key="batch_reapply_heat")
                with col3:
                    batch_reapply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
                        help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚",
                        key="batch_reapply_affinity")
                
                # Filter options
                batch_refilter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
                    help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—",
                    key="batch_refilter_invalid")
                
                if st.button("ğŸ§¹ é–‹å§‹æ¸…æ´—", type="primary", key="batch_clean_button"):
                    try:
                        with st.spinner("æ­£åœ¨æ¸…æ´—è³‡æ–™..."):
                            cleaner = DataCleaner(resample_interval=batch_clean_resample)
                            df_cleaned = cleaner.clean_data(
                                df_to_clean,
                                apply_steady_state=batch_reapply_steady_state,
                                apply_heat_balance=batch_reapply_heat_balance,
                                apply_affinity_laws=batch_reapply_affinity,
                                filter_invalid=batch_refilter_invalid
                            )
                        
                        st.success(f"âœ… æ¸…æ´—å®Œæˆï¼")
                        
                        # Show metrics
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("åŸå§‹åˆ—æ•¸", f"{len(df_to_clean):,}")
                        with col2:
                            st.metric("æ¸…æ´—å¾Œåˆ—æ•¸", f"{len(df_cleaned):,}")
                        with col3:
                            retention = len(df_cleaned) / len(df_to_clean) * 100 if len(df_to_clean) > 0 else 0
                            st.metric("ä¿ç•™ç‡", f"{retention:.1f}%")
                        
                        # Show validation results
                        validation_results = []
                        if batch_reapply_steady_state and "is_steady_state" in df_cleaned.columns:
                            steady_count = df_cleaned["is_steady_state"].sum()
                            validation_results.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
                        if batch_reapply_heat_balance and "heat_balance_invalid" in df_cleaned.columns:
                            invalid_count = df_cleaned["heat_balance_invalid"].sum()
                            validation_results.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
                        if batch_reapply_affinity and "affinity_law_invalid" in df_cleaned.columns:
                            invalid_count = df_cleaned["affinity_law_invalid"].sum()
                            validation_results.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
                        
                        if validation_results:
                            st.info(" | ".join(validation_results))
                        
                        # Show frozen data detection
                        frozen_cols = [col for col in df_cleaned.columns if '_frozen' in col]
                        if frozen_cols:
                            st.subheader("âš ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
                            for col in frozen_cols:
                                frozen_count = df_cleaned[col].sum()
                                if frozen_count > 0:
                                    st.warning(f"{col.replace('_frozen', '')}: {frozen_count} ç­†å‡çµè³‡æ–™")
                        
                        # Update session state
                        st.session_state['df_clean'] = df_cleaned
                        merged_df = df_cleaned  # Update local reference
                        
                        st.subheader("æ¸…æ´—å¾Œè³‡æ–™é è¦½")
                        st.dataframe(
                            df_cleaned.head(100).to_pandas(),
                            use_container_width=True,
                            height=400
                        )
                        
                    except Exception as e:
                        st.error(f"âŒ æ¸…æ´—éŒ¯èª¤: {str(e)}")
                        st.exception(e)
                
                # Show current cleaning status
                if auto_clean and not batch_reapply_steady_state and not batch_reapply_heat_balance and not batch_reapply_affinity:
                    st.markdown("---")
                    st.success("âœ… æ‰¹æ¬¡è™•ç†æ™‚å·²è‡ªå‹•åŸ·è¡ŒåŸºç¤æ¸…æ´—ï¼ˆé‡æ¡æ¨£ã€æ¿•çƒæº«åº¦ã€å‡çµæª¢æ¸¬ï¼‰")
                    
            else:
                st.error("âŒ æ²’æœ‰å¯æ¸…æ´—çš„è³‡æ–™")
                st.info("è«‹å…ˆåŸ·è¡Œæ‰¹æ¬¡è™•ç†æˆ–è¼‰å…¥è³‡æ–™")
            
        with batch_tab3:
            st.subheader("çµ±è¨ˆè³‡è¨Š")
            
            # Show data status
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
            
            # Select numeric columns (excluding Date/Time)
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            if numeric_cols:
                selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols, key="batch_stats_col")
                
                if selected_col:
                    col_data_clean = merged_df[selected_col].drop_nulls()
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        mean_val = col_data_clean.mean()
                        st.metric("å¹³å‡å€¼", f"{mean_val:.2f}" if mean_val is not None else "N/A")
                    with col2:
                        median_val = col_data_clean.median()
                        st.metric("ä¸­ä½æ•¸", f"{median_val:.2f}" if median_val is not None else "N/A")
                    with col3:
                        min_val = col_data_clean.min()
                        st.metric("æœ€å°å€¼", f"{min_val:.2f}" if min_val is not None else "N/A")
                    with col4:
                        max_val = col_data_clean.max()
                        st.metric("æœ€å¤§å€¼", f"{max_val:.2f}" if max_val is not None else "N/A")
                    with col5:
                        std_val = col_data_clean.std()
                        st.metric("æ¨™æº–å·®", f"{std_val:.2f}" if std_val is not None else "N/A")
                    
                    # Distribution
                    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
                    pandas_data = col_data_clean.to_pandas()
                    
                    if len(pandas_data) > 0:
                        import numpy as np
                        import pandas as pd
                        
                        counts, bin_edges = np.histogram(pandas_data, bins=30)
                        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                        hist_df = pd.DataFrame({'value': bin_centers, 'count': counts}).set_index('value')
                        
                        if hist_df['count'].sum() > 0:
                            st.bar_chart(hist_df)
                        
                        data_range = col_data_clean.max() - col_data_clean.min()
                        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            
        with batch_tab4:
            st.subheader("æ™‚é–“åºåˆ—åˆ†æ")
            
            if 'timestamp' in merged_df.columns:
                numeric_cols = get_analysis_numeric_cols(merged_df)
                
                if numeric_cols:
                    selected_cols = st.multiselect(
                        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
                        numeric_cols,
                        default=[numeric_cols[0]] if numeric_cols else [],
                        max_selections=3,
                        key="batch_timeseries_cols"
                    )
                    
                    if selected_cols:
                        pandas_df = merged_df.select(['timestamp'] + selected_cols).to_pandas()
                        pandas_df = pandas_df.set_index('timestamp')
                        st.line_chart(pandas_df)
                        
                        st.caption(f"æ™‚é–“ç¯„åœ: {merged_df['timestamp'].min()} è‡³ {merged_df['timestamp'].max()}")
                        st.caption(f"è³‡æ–™é»æ•¸: {len(merged_df):,}")
                    else:
                        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")
                else:
                    st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            else:
                st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
            

        with batch_tab5:
            st.header("ğŸ”— é—œè¯çŸ©é™£ç†±åœ–")
            
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            if numeric_cols:
                st.subheader("é¸æ“‡è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
                
                max_vars = min(15, len(numeric_cols))
                selected_vars = st.multiselect(
                    f"é¸æ“‡è¦åˆ†æçš„è®Šæ•¸ï¼ˆæœ€å¤š {max_vars} å€‹ï¼Œå»ºè­° 5-10 å€‹ï¼‰",
                    numeric_cols,
                    default=numeric_cols[:min(8, len(numeric_cols))],
                    max_selections=max_vars,
                    key="batch_corr_vars"
                )
                
                if len(selected_vars) >= 2:
                    try:
                        import plotly.figure_factory as ff
                        import numpy as np
                        
                        corr_df = merged_df.select(selected_vars).to_pandas()
                        corr_matrix = corr_df.corr()
                        
                        fig = ff.create_annotated_heatmap(
                            z=corr_matrix.values,
                            x=list(corr_matrix.columns),
                            y=list(corr_matrix.index),
                            annotation_text=np.around(corr_matrix.values, decimals=2),
                            colorscale='RdBu',
                            zmid=0,
                            showscale=True
                        )
                        
                        fig.update_layout(
                            title="è®Šæ•¸ç›¸é—œæ€§çŸ©é™£",
                            height=600,
                            xaxis={'side': 'bottom'}
                        )
                        fig.update_xaxes(tickangle=45)
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        st.markdown("---")
                        st.subheader("ğŸ“– ç›¸é—œä¿‚æ•¸è§£è®€")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.markdown("**ğŸ”´ å¼·è² ç›¸é—œ**: -1.0 ~ -0.7")
                        with col2:
                            st.markdown("**âšª ç„¡ç›¸é—œ**: -0.3 ~ 0.3")
                        with col3:
                            st.markdown("**ğŸ”µ å¼·æ­£ç›¸é—œ**: 0.7 ~ 1.0")
                        
                    except Exception as e:
                        st.error(f"è¨ˆç®—ç›¸é—œæ€§å¤±æ•—: {str(e)}")
                else:
                    st.warning("è«‹è‡³å°‘é¸æ“‡ 2 å€‹è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        
        with batch_tab6:
            st.header("ğŸ¯ è³‡æ–™å“è³ªå„€è¡¨æ¿")
            
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            st.subheader("ğŸ“ˆ æ•´é«”è³‡æ–™å“è³ª")
            
            total_rows = len(merged_df)
            total_cols = len(merged_df.columns)
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{total_rows:,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{total_cols}")
            with col3:
                st.metric("æ•¸å€¼æ¬„ä½", f"{len(numeric_cols)}")
            with col4:
                if 'timestamp' in merged_df.columns:
                    time_span = merged_df['timestamp'].max() - merged_df['timestamp'].min()
                    st.metric("æ™‚é–“è·¨åº¦", str(time_span))
            
            st.markdown("---")
            st.subheader("ğŸ” ç¼ºå¤±å€¼åˆ†æ")
            
            # Columns to exclude from missing value analysis (Date/Time related)
            exclude_missing_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
            
            missing_data = []
            for col in merged_df.columns:
                # Skip Date/Time columns
                if col in exclude_missing_cols:
                    continue
                null_count = merged_df[col].null_count()
                if null_count > 0:
                    null_pct = (null_count / total_rows) * 100
                    missing_data.append({
                        'æ¬„ä½åç¨±': col,
                        'ç¼ºå¤±æ•¸é‡': null_count,
                        'ç¼ºå¤±æ¯”ä¾‹': f"{null_pct:.2f}%",
                        'åš´é‡ç¨‹åº¦': 'ğŸ”´ é«˜' if null_pct > 30 else ('ğŸŸ¡ ä¸­' if null_pct > 10 else 'ğŸŸ¢ ä½')
                    })
            
            if missing_data:
                import pandas as pd
                missing_df = pd.DataFrame(missing_data).sort_values('ç¼ºå¤±æ•¸é‡', ascending=False)
                st.dataframe(missing_df, use_container_width=True)
                
                # Visualize missing data
                import plotly.express as px
                fig = px.bar(
                    missing_df.head(10),
                    x='æ¬„ä½åç¨±',
                    y='ç¼ºå¤±æ•¸é‡',
                    title='å‰ 10 å€‹ç¼ºå¤±å€¼æœ€å¤šçš„æ¬„ä½',
                    labels={'ç¼ºå¤±æ•¸é‡': 'ç¼ºå¤±æ•¸é‡', 'æ¬„ä½åç¨±': 'æ¬„ä½'}
                )
                fig.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.success("âœ… æ²’æœ‰ç¼ºå¤±å€¼ï¼")
            
            # Physics Validation Status Section
            st.markdown("---")
            st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰ç‹€æ…‹")
            
            validation_cols = st.columns(3)
            
            with validation_cols[0]:
                st.markdown("**ğŸ“Š ç©©æ…‹æª¢æ¸¬**")
                if 'is_steady_state' in merged_df.columns:
                    steady_count = merged_df['is_steady_state'].sum()
                    total_count = len(merged_df)
                    steady_pct = (steady_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç©©æ…‹è³‡æ–™", f"{steady_count:,} ({steady_pct:.1f}%)")
                    
                    # Small bar chart
                    steady_data = {'ç‹€æ…‹': ['ç©©æ…‹', 'éç©©æ…‹'], 'æ•¸é‡': [steady_count, total_count - steady_count]}
                    import pandas as pd
                    st.bar_chart(pd.DataFrame(steady_data).set_index('ç‹€æ…‹'))
                else:
                    st.caption("æœªåŸ·è¡Œç©©æ…‹æª¢æ¸¬")
            
            with validation_cols[1]:
                st.markdown("**ğŸŒ¡ï¸ ç†±å¹³è¡¡é©—è­‰**")
                if 'heat_balance_invalid' in merged_df.columns:
                    invalid_count = merged_df['heat_balance_invalid'].sum()
                    total_count = len(merged_df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    if invalid_pct > 20:
                        st.error("ğŸ”´ ç•°å¸¸æ¯”ä¾‹éé«˜")
                    elif invalid_pct > 10:
                        st.warning("ğŸŸ¡ ç•°å¸¸æ¯”ä¾‹ä¸­ç­‰")
                    else:
                        st.success("ğŸŸ¢ ç•°å¸¸æ¯”ä¾‹æ­£å¸¸")
                else:
                    st.caption("æœªåŸ·è¡Œç†±å¹³è¡¡é©—è­‰")
            
            with validation_cols[2]:
                st.markdown("**âš¡ è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥**")
                if 'affinity_law_invalid' in merged_df.columns:
                    invalid_count = merged_df['affinity_law_invalid'].sum()
                    total_count = len(merged_df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    # Show affinity ratio distribution if available
                    if 'affinity_ratio' in merged_df.columns:
                        ratio_data = merged_df['affinity_ratio'].drop_nulls()
                        if len(ratio_data) > 0:
                            st.caption(f"æ¯”ç‡ç¯„åœ: {ratio_data.min():.4f} ~ {ratio_data.max():.4f}")
                else:
                    st.caption("æœªåŸ·è¡Œè¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥")
            
            # Frozen data detection summary
            st.markdown("---")
            st.subheader("â„ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
            
            if auto_clean:
                frozen_cols = [col for col in merged_df.columns if '_frozen' in col]
                
                if frozen_cols:
                    frozen_summary = []
                    for col in frozen_cols:
                        original_col = col.replace('_frozen', '')
                        frozen_count = merged_df[col].sum()
                        if frozen_count > 0:
                            frozen_pct = (frozen_count / total_rows) * 100
                            frozen_summary.append({
                                'æ„Ÿæ¸¬å™¨': original_col,
                                'å‡çµé»æ•¸': frozen_count,
                                'å‡çµæ¯”ä¾‹': f"{frozen_pct:.2f}%",
                                'ç‹€æ…‹': 'ğŸ”´ è­¦å‘Š' if frozen_pct > 5 else 'ğŸŸ¡ æ³¨æ„'
                            })
                    
                    if frozen_summary:
                        import pandas as pd
                        frozen_df = pd.DataFrame(frozen_summary).sort_values('å‡çµé»æ•¸', ascending=False)
                        st.dataframe(frozen_df, use_container_width=True)
                        
                        st.warning("âš ï¸ å‡çµè³‡æ–™å¯èƒ½è¡¨ç¤ºæ„Ÿæ¸¬å™¨æ•…éšœæˆ–æ•¸æ“šå‚³è¼¸å•é¡Œ")
                    else:
                        st.success("âœ… æ²’æœ‰åµæ¸¬åˆ°å‡çµè³‡æ–™")
                else:
                    st.info("è³‡æ–™ä¸­ç„¡å‡çµæ¨™è¨˜æ¬„ä½")
            else:
                st.info("å°šæœªåŸ·è¡Œå‡çµè³‡æ–™åµæ¸¬ï¼ˆéœ€å…ˆæ¸…æ´—è³‡æ–™ï¼‰")
            
            # Data completeness timeline
            if 'timestamp' in merged_df.columns and numeric_cols:
                st.markdown("---")
                st.subheader("ğŸ“… è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸")
                
                # Select a representative column to check completeness
                sample_col = st.selectbox(
                    "é¸æ“‡æ¬„ä½æª¢è¦–å®Œæ•´æ€§",
                    numeric_cols,
                    key="batch_completeness_col"
                )
                
                if sample_col:
                    # Create a binary completeness indicator
                    timeline_df = merged_df.select(['timestamp', sample_col]).to_pandas()
                    timeline_df['å®Œæ•´æ€§'] = (~timeline_df[sample_col].isna()).astype(int)
                    timeline_df = timeline_df.set_index('timestamp')
                    
                    import plotly.graph_objects as go
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=timeline_df.index,
                        y=timeline_df['å®Œæ•´æ€§'],
                        mode='lines',
                        fill='tozeroy',
                        name='è³‡æ–™å­˜åœ¨',
                        line=dict(color='green')
                    ))
                    
                    fig.update_layout(
                        title=f"{sample_col} è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸",
                        xaxis_title="æ™‚é–“",
                        yaxis_title="è³‡æ–™å­˜åœ¨ (1=æœ‰, 0=ç„¡)",
                        height=300,
                        yaxis=dict(tickvals=[0, 1], ticktext=['ç¼ºå¤±', 'å­˜åœ¨'])
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
            
            # Data quality score
            st.markdown("---")
            st.subheader("â­ æ•´é«”å“è³ªè©•åˆ†")
            
            # Calculate quality score (0-100)
            quality_score = 100
            
            # Deduct points for missing data
            if missing_data:
                avg_missing_pct = sum([float(d['ç¼ºå¤±æ¯”ä¾‹'].strip('%')) for d in missing_data]) / len(merged_df.columns)
                quality_score -= min(avg_missing_pct, 30)
            
            # Deduct points for frozen data (only if cleaned)
            if auto_clean:
                frozen_cols = [col for col in merged_df.columns if '_frozen' in col]
                if frozen_cols:
                    frozen_count = sum([merged_df[col].sum() for col in frozen_cols])
                    frozen_pct = (frozen_count / (total_rows * len(frozen_cols))) * 100 if frozen_cols else 0
                    quality_score -= min(frozen_pct, 20)
            
            quality_score = max(0, quality_score)
            
            # Display score with color coding
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                st.metric("è³‡æ–™å“è³ªè©•åˆ†", f"{quality_score:.1f}/100")
            
            with col2:
                if quality_score >= 90:
                    st.success("ğŸŸ¢ å„ªç§€")
                elif quality_score >= 75:
                    st.info("ğŸ”µ è‰¯å¥½")
                elif quality_score >= 60:
                    st.warning("ğŸŸ¡ å°šå¯")
                else:
                    st.error("ğŸ”´ éœ€æ”¹å–„")
            
            with col3:
                # Progress bar
                st.progress(quality_score / 100)
            
            # Recommendations
            if quality_score < 90:
                st.markdown("---")
                st.subheader("ğŸ’¡ æ”¹å–„å»ºè­°")
                
                if missing_data and len(missing_data) > 0:
                    st.markdown("- æª¢æŸ¥ç¼ºå¤±æ¯”ä¾‹ > 10% çš„æ¬„ä½ï¼Œè€ƒæ…®è£œå€¼æˆ–ç§»é™¤")
                
                if auto_clean:
                    frozen_cols = [col for col in merged_df.columns if '_frozen' in col]
                    if frozen_cols:
                        frozen_count = sum([merged_df[col].sum() for col in frozen_cols])
                        if frozen_count > 0:
                            st.markdown("- æª¢æŸ¥å‡çµè³‡æ–™çš„æ„Ÿæ¸¬å™¨ï¼Œå¯èƒ½éœ€è¦ç¶­è­·")
                
                st.markdown("- ç¢ºèªè³‡æ–™æ”¶é›†é »ç‡èˆ‡é æœŸä¸€è‡´")
                st.markdown("- è€ƒæ…®é€²è¡Œç•°å¸¸å€¼åµæ¸¬èˆ‡è™•ç†")

        with batch_tab7:
            st.header("åŒ¯å‡ºè³‡æ–™")
            
            # Data selection radio (matching single file mode)
            export_type = st.radio(
                "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
                ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"],
                key="batch_export_type"
            )
            
            df_to_export = None
            if export_type == "è§£æå¾Œè³‡æ–™" and 'df_parsed' in st.session_state:
                df_to_export = st.session_state['df_parsed']
                st.info("ğŸ“Š **åŒ¯å‡ºï¼šè§£æå¾Œè³‡æ–™**ï¼ˆåŸå§‹åˆä½µè³‡æ–™ï¼‰")
            elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and 'df_clean' in st.session_state:
                df_to_export = st.session_state['df_clean']
                st.info("ğŸ“Š **åŒ¯å‡ºï¼šæ¸…æ´—å¾Œè³‡æ–™**ï¼ˆå·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼ï¼‰")
            
            if df_to_export is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    # CSV export
                    csv_data = df_to_export.write_csv()
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ CSV",
                        data=csv_data,
                        file_name=f"hvac_batch_{batch_file_count}files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Parquet export
                    from io import BytesIO
                    buffer = BytesIO()
                    df_to_export.write_parquet(buffer)
                    parquet_data = buffer.getvalue()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ Parquet",
                        data=parquet_data,
                        file_name=f"hvac_batch_{batch_file_count}files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
                        mime="application/octet-stream"
                    )
                
                st.info("ğŸ’¡ Parquet æ ¼å¼è¼ƒå°ä¸”æ•ˆèƒ½æ›´å¥½ï¼Œé©åˆå¤§å‹è³‡æ–™é›†")
            else:
                if export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰":
                    st.warning("è«‹å…ˆåŸ·è¡Œè³‡æ–™æ¸…æ´—æˆ–é¸æ“‡ã€Œè§£æå¾Œè³‡æ–™ã€")
                else:
                    st.warning("æ²’æœ‰å¯åŒ¯å‡ºçš„è³‡æ–™")

elif processing_mode == "âš¡ æœ€ä½³åŒ–æ¨¡æ“¬" and ML_AVAILABLE:
    # Optimization Simulation Mode
    st.header("âš¡ èƒ½è€—æœ€ä½³åŒ–æ¨¡æ“¬")
    st.markdown("**ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œæ‰¾å‡ºæœ€çœé›»çš„è®Šé »å™¨è¨­å®š**")
    
    # Check if model is selected
    if 'selected_model' in dir() and selected_model:
        model_path = Path("models") / selected_model
        
        # Load model
        @st.cache_resource
        def load_model(path):
            return ChillerEnergyModel.load_model(str(path))
        
        try:
            model = load_model(model_path)
            
            # Show model info
            col1, col2, col3 = st.columns(3)
            with col1:
                if model.training_metrics:
                    st.metric("æ¨¡å‹ MAPE", f"{model.training_metrics.get('mape', 0):.2f}%")
            with col2:
                if model.training_metrics:
                    st.metric("æ¨¡å‹ RÂ²", f"{model.training_metrics.get('r2', 0):.4f}")
            with col3:
                st.metric("ç‰¹å¾µæ•¸é‡", f"{len(model.feature_names)}")
            
            st.success(f"âœ… å·²è¼‰å…¥æ¨¡å‹: {selected_model}")
            
            # Create tabs for different functions
            opt_tab1, opt_tab2, opt_tab3, opt_tab4 = st.tabs([
                "ğŸ¯ å³æ™‚æœ€ä½³åŒ–",
                "ğŸ“Š ç‰¹å¾µé‡è¦æ€§",
                "ğŸ“ˆ æ­·å²è¿½è¹¤",
                "ğŸ”§ æ¨¡å‹è¨“ç·´"
            ])
            
            with opt_tab1:
                st.subheader("è¨­å®šç•¶å‰é‹è½‰æ¢ä»¶")
                
                # Input parameters
                st.markdown("#### ğŸ­ è² è¼‰æ¢ä»¶")
                col1, col2 = st.columns(2)
                with col1:
                    load_rt = st.slider(
                        "å†·å‡å™¸è² è¼‰ (RT)",
                        min_value=100,
                        max_value=2000,
                        value=500,
                        step=50,
                        help="ç•¶å‰çš„å†·å»è² è¼‰"
                    )
                with col2:
                    temp_db_out = st.slider(
                        "å®¤å¤–ä¹¾çƒæº«åº¦ (Â°C)",
                        min_value=15.0,
                        max_value=40.0,
                        value=30.0,
                        step=0.5,
                        help="ç•¶å‰å®¤å¤–æº«åº¦"
                    )
                
                st.markdown("#### âš™ï¸ ç•¶å‰è®Šé »å™¨è¨­å®š")
                col1, col2, col3 = st.columns(3)
                with col1:
                    current_chw_pump_hz = st.slider(
                        "å†°æ°´æ³µé »ç‡ (Hz)",
                        min_value=30.0,
                        max_value=60.0,
                        value=50.0,
                        step=1.0,
                        help="CHP è®Šé »å™¨è¼¸å‡º"
                    )
                with col2:
                    current_cw_pump_hz = st.slider(
                        "å†·å»æ°´æ³µé »ç‡ (Hz)",
                        min_value=30.0,
                        max_value=60.0,
                        value=50.0,
                        step=1.0,
                        help="CWP è®Šé »å™¨è¼¸å‡º"
                    )
                with col3:
                    current_ct_fan_hz = st.slider(
                        "å†·å»å¡”é¢¨æ‰‡é »ç‡ (Hz)",
                        min_value=30.0,
                        max_value=60.0,
                        value=50.0,
                        step=1.0,
                        help="CT è®Šé »å™¨è¼¸å‡º"
                    )
                
                st.markdown("---")
                
                # Optimization options
                col1, col2 = st.columns(2)
                with col1:
                    opt_method = st.radio(
                        "æœ€ä½³åŒ–æ–¹æ³•",
                        ["SLSQP (å¿«é€Ÿ)", "Differential Evolution (å…¨åŸŸ)"],
                        help="SLSQP é©åˆå¿«é€Ÿæ±‚è§£ï¼ŒDE é©åˆå°‹æ‰¾å…¨åŸŸæœ€ä½³è§£"
                    )
                
                # Run optimization button
                if st.button("ğŸš€ åŸ·è¡Œæœ€ä½³åŒ–", type="primary", use_container_width=True):
                    with st.spinner("æ­£åœ¨è¨ˆç®—æœ€ä½³è¨­å®š..."):
                        # Create context
                        context = OptimizationContext(
                            load_rt=load_rt,
                            temp_db_out=temp_db_out,
                            current_chw_pump_hz=current_chw_pump_hz,
                            current_cw_pump_hz=current_cw_pump_hz,
                            current_ct_fan_hz=current_ct_fan_hz
                        )
                        
                        # Create optimizer
                        optimizer = ChillerOptimizer(model)
                        
                        # Run optimization
                        if "SLSQP" in opt_method:
                            result = optimizer.optimize_slsqp(context)
                        else:
                            result = optimizer.optimize_global(context, maxiter=50)
                        
                        # Store result and context in session state for persistence
                        st.session_state['last_optimization_result'] = result
                        st.session_state['last_optimization_context'] = {
                            'load_rt': load_rt,
                            'temp_db_out': temp_db_out,
                            'current_chw_pump_hz': current_chw_pump_hz,
                            'current_cw_pump_hz': current_cw_pump_hz,
                            'current_ct_fan_hz': current_ct_fan_hz,
                            'opt_method': opt_method,
                            'model_name': selected_model
                        }
                        st.session_state['optimization_saved'] = False
                
                # Display results if available in session state
                if 'last_optimization_result' in st.session_state and st.session_state['last_optimization_result'] is not None:
                    result = st.session_state['last_optimization_result']
                    ctx = st.session_state.get('last_optimization_context', {})
                    
                    # Display results
                    st.markdown("---")
                    st.subheader("ğŸ“Š æœ€ä½³åŒ–çµæœ")
                    
                    if result.success:
                        st.success("âœ… æœ€ä½³åŒ–æˆåŠŸå®Œæˆï¼")
                    else:
                        st.warning(f"âš ï¸ {result.message}")
                    
                    # Comparison table
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.markdown("##### ğŸ”§ è®Šé »å™¨è¨­å®š")
                        import pandas as pd
                        settings_df = pd.DataFrame({
                            'é …ç›®': ['å†°æ°´æ³µ (Hz)', 'å†·å»æ°´æ³µ (Hz)', 'å†·å»å¡”é¢¨æ‰‡ (Hz)'],
                            'ç›®å‰è¨­å®š': [
                                ctx.get('current_chw_pump_hz', '-'),
                                ctx.get('current_cw_pump_hz', '-'),
                                ctx.get('current_ct_fan_hz', '-')
                            ],
                            'å»ºè­°è¨­å®š': [
                                f"{result.optimal_chw_pump_hz:.1f}",
                                f"{result.optimal_cw_pump_hz:.1f}",
                                f"{result.optimal_ct_fan_hz:.1f}"
                            ]
                        })
                        st.dataframe(settings_df, hide_index=True, use_container_width=True)
                    
                    with col2:
                        st.markdown("##### âš¡ èƒ½è€—æ¯”è¼ƒ")
                        st.metric(
                            "ç›®å‰é ä¼°èƒ½è€—",
                            f"{result.baseline_power_kw:.1f} kW"
                        )
                        st.metric(
                            "æœ€ä½³åŒ–å¾Œèƒ½è€—",
                            f"{result.predicted_power_kw:.1f} kW",
                            delta=f"-{result.savings_kw:.1f} kW" if result.savings_kw > 0 else f"+{-result.savings_kw:.1f} kW",
                            delta_color="inverse"
                        )
                    
                    with col3:
                        st.markdown("##### ğŸ’° ç¯€èƒ½æ•ˆç›Š")
                        st.metric(
                            "ç¯€èƒ½æ¯”ä¾‹",
                            f"{result.savings_percent:.1f}%"
                        )
                        # Estimate annual savings (assuming 8760 hours/year, $0.1/kWh)
                        annual_savings = result.savings_kw * 8760 * 3.5  # TWD per kWh
                        if result.savings_kw > 0:
                            st.metric(
                                "é ä¼°å¹´ç¯€çœ",
                                f"NT$ {annual_savings:,.0f}"
                            )
                    
                    # Constraint violations
                    if result.constraint_violations:
                        st.markdown("---")
                        st.warning("âš ï¸ é™åˆ¶æ¢ä»¶è­¦å‘Š")
                        for v in result.constraint_violations:
                            st.caption(f"â€¢ {v}")
                    
                    # Save result button - only show if not already saved
                    st.markdown("---")
                    if not st.session_state.get('optimization_saved', False):
                        if st.button("ğŸ’¾ å„²å­˜æ­¤æ¬¡çµæœ", key="save_optimization_result"):
                            try:
                                # Initialize history tracker
                                history_tracker = OptimizationHistoryTracker()
                                
                                # Create current and optimal settings dicts
                                current_settings = {
                                    'chw_pump_hz': ctx.get('current_chw_pump_hz', 0),
                                    'cw_pump_hz': ctx.get('current_cw_pump_hz', 0),
                                    'tower_fan_hz': ctx.get('current_ct_fan_hz', 0)
                                }
                                optimal_settings = {
                                    'chw_pump_hz': result.optimal_chw_pump_hz,
                                    'cw_pump_hz': result.optimal_cw_pump_hz,
                                    'tower_fan_hz': result.optimal_ct_fan_hz
                                }
                                
                                # Create record
                                record = create_record_from_result(
                                    model_name=ctx.get('model_name', 'unknown'),
                                    load_rt=ctx.get('load_rt', 0),
                                    outdoor_temp=ctx.get('temp_db_out', 0),
                                    current_settings=current_settings,
                                    optimal_settings=optimal_settings,
                                    current_power=result.baseline_power_kw,
                                    optimal_power=result.predicted_power_kw,
                                    method="SLSQP" if "SLSQP" in ctx.get('opt_method', '') else "Differential Evolution"
                                )
                                
                                # Save record
                                history_tracker.add_record(record)
                                st.session_state['optimization_saved'] = True
                                st.success("âœ… çµæœå·²å„²å­˜ï¼å¯åœ¨ã€ŒğŸ“ˆ æ­·å²è¿½è¹¤ã€åˆ†é æŸ¥çœ‹ã€‚")
                            except Exception as e:
                                st.error(f"å„²å­˜å¤±æ•—: {e}")
                    else:
                        st.info("âœ… æ­¤æ¬¡çµæœå·²å„²å­˜ã€‚åŸ·è¡Œæ–°çš„æœ€ä½³åŒ–å¾Œå¯å†æ¬¡å„²å­˜ã€‚")
            
            with opt_tab2:
                st.subheader("ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ")
                
                importance = model.get_feature_importance()
                
                if importance:
                    import pandas as pd
                    import plotly.express as px
                    
                    # Create dataframe
                    importance_df = pd.DataFrame([
                        {'ç‰¹å¾µ': k, 'é‡è¦æ€§': v}
                        for k, v in list(importance.items())[:15]
                    ])
                    
                    # Bar chart
                    fig = px.bar(
                        importance_df,
                        x='é‡è¦æ€§',
                        y='ç‰¹å¾µ',
                        orientation='h',
                        title='Top 15 ç‰¹å¾µé‡è¦æ€§',
                        labels={'é‡è¦æ€§': 'é‡è¦æ€§åˆ†æ•¸', 'ç‰¹å¾µ': 'ç‰¹å¾µåç¨±'}
                    )
                    fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Table
                    st.markdown("##### å®Œæ•´ç‰¹å¾µé‡è¦æ€§åˆ—è¡¨")
                    full_importance_df = pd.DataFrame([
                        {'æ’å': i+1, 'ç‰¹å¾µ': k, 'é‡è¦æ€§': f"{v:.4f}"}
                        for i, (k, v) in enumerate(importance.items())
                    ])
                    st.dataframe(full_importance_df, hide_index=True, use_container_width=True)
                else:
                    st.info("ç„¡æ³•å–å¾—ç‰¹å¾µé‡è¦æ€§")
            
            with opt_tab3:
                st.subheader("ğŸ“ˆ æœ€ä½³åŒ–æ­·å²è¿½è¹¤")
                st.markdown("è¿½è¹¤éå»çš„æœ€ä½³åŒ–çµæœä¸¦åˆ†æç¯€èƒ½è¶¨å‹¢")
                
                try:
                    # Load history
                    history_tracker = OptimizationHistoryTracker()
                    records = history_tracker.get_all_records()
                    stats = history_tracker.get_total_savings()
                    
                    if records:
                        # Summary metrics
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("ç¸½åŸ·è¡Œæ¬¡æ•¸", f"{stats['total_runs']} æ¬¡")
                        with col2:
                            st.metric("ç´¯è¨ˆç¯€çœ", f"{stats['total_savings_kw']:.1f} kW")
                        with col3:
                            st.metric("å¹³å‡ç¯€èƒ½ç‡", f"{stats['avg_savings_percent']:.1f}%")
                        with col4:
                            st.metric("æœ€é«˜ç¯€èƒ½ç‡", f"{stats['max_savings_percent']:.1f}%")
                        
                        st.markdown("---")
                        
                        # Trend chart
                        import pandas as pd
                        import plotly.express as px
                        import plotly.graph_objects as go
                        
                        # Prepare data for chart
                        history_df = pd.DataFrame([{
                            'æ™‚é–“': r.timestamp[:16].replace('T', ' '),
                            'ç¯€èƒ½ç‡ (%)': r.savings_percent,
                            'ç¯€çœé›»åŠ› (kW)': r.savings_kw,
                            'è² è¼‰ (RT)': r.load_rt,
                            'ç›®å‰èƒ½è€— (kW)': r.current_power_kw,
                            'æœ€ä½³èƒ½è€— (kW)': r.optimal_power_kw
                        } for r in records])
                        
                        # Savings trend chart
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            x=history_df['æ™‚é–“'],
                            y=history_df['ç¯€èƒ½ç‡ (%)'],
                            mode='lines+markers',
                            name='ç¯€èƒ½ç‡ (%)',
                            line=dict(color='#00CC96', width=2),
                            marker=dict(size=8)
                        ))
                        fig.update_layout(
                            title='ç¯€èƒ½ç‡è¶¨å‹¢',
                            xaxis_title='æ™‚é–“',
                            yaxis_title='ç¯€èƒ½ç‡ (%)',
                            height=350
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Power comparison chart
                        fig2 = go.Figure()
                        fig2.add_trace(go.Bar(
                            x=history_df['æ™‚é–“'],
                            y=history_df['ç›®å‰èƒ½è€— (kW)'],
                            name='ç›®å‰èƒ½è€—',
                            marker_color='#EF553B'
                        ))
                        fig2.add_trace(go.Bar(
                            x=history_df['æ™‚é–“'],
                            y=history_df['æœ€ä½³èƒ½è€— (kW)'],
                            name='æœ€ä½³èƒ½è€—',
                            marker_color='#00CC96'
                        ))
                        fig2.update_layout(
                            title='èƒ½è€—æ¯”è¼ƒ',
                            xaxis_title='æ™‚é–“',
                            yaxis_title='èƒ½è€— (kW)',
                            barmode='group',
                            height=350
                        )
                        st.plotly_chart(fig2, use_container_width=True)
                        
                        # History table
                        st.markdown("##### è©³ç´°ç´€éŒ„")
                        st.dataframe(
                            history_df[['æ™‚é–“', 'è² è¼‰ (RT)', 'ç›®å‰èƒ½è€— (kW)', 'æœ€ä½³èƒ½è€— (kW)', 'ç¯€çœé›»åŠ› (kW)', 'ç¯€èƒ½ç‡ (%)']],
                            hide_index=True,
                            use_container_width=True
                        )
                        
                        # Clear history button
                        st.markdown("---")
                        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ­·å²ç´€éŒ„", type="secondary"):
                            history_tracker.clear_history()
                            st.success("å·²æ¸…é™¤æ‰€æœ‰ç´€éŒ„")
                            st.rerun()
                    else:
                        st.info("ğŸ“­ å°šç„¡æ­·å²ç´€éŒ„ã€‚è«‹å…ˆåœ¨ã€ŒğŸ¯ å³æ™‚æœ€ä½³åŒ–ã€åˆ†é åŸ·è¡Œå„ªåŒ–ä¸¦å„²å­˜çµæœã€‚")
                except Exception as e:
                    st.error(f"è¼‰å…¥æ­·å²ç´€éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            with opt_tab4:
                st.subheader("ğŸ”§ è¨“ç·´æ–°æ¨¡å‹")
                st.markdown("ä½¿ç”¨æ‰¹æ¬¡è™•ç†å¾Œçš„è³‡æ–™è¨“ç·´èƒ½è€—é æ¸¬æ¨¡å‹")
                
                # Check if batch data is available
                if 'df_clean' in st.session_state or 'df_parsed' in st.session_state:
                    df_for_training = st.session_state.get('df_clean', st.session_state.get('df_parsed'))
                    
                    st.info(f"ğŸ“Š å¯ç”¨è³‡æ–™: {len(df_for_training):,} ç­†")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        new_model_name = st.text_input(
                            "æ¨¡å‹åç¨±",
                            value=f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        )
                    
                    if st.button("ğŸ“ é–‹å§‹è¨“ç·´", type="primary"):
                        with st.spinner("æ­£åœ¨è¨“ç·´æ¨¡å‹..."):
                            try:
                                new_model = ChillerEnergyModel()
                                metrics = new_model.train(df_for_training)
                                
                                # Save model
                                model_path = f"models/{new_model_name}.joblib"
                                new_model.save_model(model_path)
                                
                                st.success(f"âœ… è¨“ç·´å®Œæˆï¼")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("MAPE", f"{metrics['mape']:.2f}%")
                                with col2:
                                    st.metric("RÂ²", f"{metrics['r2']:.4f}")
                                with col3:
                                    st.metric("RMSE", f"{metrics['rmse']:.2f}")
                                
                                st.info(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")
                                st.caption("é‡æ–°æ•´ç†é é¢å³å¯é¸æ“‡æ–°æ¨¡å‹")
                                
                            except Exception as e:
                                st.error(f"âŒ è¨“ç·´å¤±æ•—: {str(e)}")
                else:
                    st.warning("è«‹å…ˆä½¿ç”¨ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼è¼‰å…¥ä¸¦æ¸…æ´—è³‡æ–™")
                    st.caption("1. åˆ‡æ›åˆ°ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼")
                    st.caption("2. é¸æ“‡æª”æ¡ˆä¸¦åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
                    st.caption("3. å›åˆ°æ­¤é é¢é€²è¡Œæ¨¡å‹è¨“ç·´")
        
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
    else:
        st.warning("ğŸ‘ˆ è«‹å¾å·¦å´é¸æ“‡å·²è¨“ç·´çš„æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨ä¸‹æ–¹ã€Œæ¨¡å‹è¨“ç·´ã€åˆ†é è¨“ç·´æ–°æ¨¡å‹")
        
        # Still show tabs so user can train a model
        opt_tab1, opt_tab2, opt_tab3, opt_tab4 = st.tabs([
            "ğŸ¯ å³æ™‚æœ€ä½³åŒ–",
            "ğŸ“Š ç‰¹å¾µé‡è¦æ€§",
            "ğŸ“ˆ æ­·å²è¿½è¹¤",
            "ğŸ”§ æ¨¡å‹è¨“ç·´"
        ])
        
        with opt_tab1:
            st.info("è«‹å…ˆé¸æ“‡æˆ–è¨“ç·´æ¨¡å‹å¾Œæ‰èƒ½ä½¿ç”¨å³æ™‚æœ€ä½³åŒ–åŠŸèƒ½")
            st.markdown("""
            ### å¦‚ä½•é–‹å§‹ï¼Ÿ
            
            #### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ç¾æœ‰æ¨¡å‹
            å¦‚æœå·²ç¶“æœ‰è¨“ç·´å¥½çš„æ¨¡å‹ (`.joblib` æª”æ¡ˆ)ï¼Œè«‹å°‡å®ƒæ”¾åœ¨ `models/` è³‡æ–™å¤¾ä¸­ã€‚
            
            #### æ–¹æ³•äºŒï¼šè¨“ç·´æ–°æ¨¡å‹
            1. é»é¸ä¸Šæ–¹ã€ŒğŸ”§ æ¨¡å‹è¨“ç·´ã€åˆ†é 
            2. è‹¥å°šç„¡è³‡æ–™ï¼Œè«‹å…ˆåˆ‡æ›åˆ°ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼è¼‰å…¥è³‡æ–™
            3. å›åˆ°æ­¤æ¨¡å¼å¾Œå¯ç›´æ¥è¨“ç·´æ¨¡å‹
            """)
        
        with opt_tab2:
            st.info("è«‹å…ˆé¸æ“‡æ¨¡å‹æ‰èƒ½æŸ¥çœ‹ç‰¹å¾µé‡è¦æ€§")
        
        with opt_tab3:
            st.subheader("ğŸ“ˆ æœ€ä½³åŒ–æ­·å²è¿½è¹¤")
            st.markdown("è¿½è¹¤éå»çš„æœ€ä½³åŒ–çµæœä¸¦åˆ†æç¯€èƒ½è¶¨å‹¢")
            
            try:
                # Load history
                history_tracker = OptimizationHistoryTracker()
                records = history_tracker.get_all_records()
                stats = history_tracker.get_total_savings()
                
                if records:
                    # Summary metrics
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ç¸½åŸ·è¡Œæ¬¡æ•¸", f"{stats['total_runs']} æ¬¡")
                    with col2:
                        st.metric("ç´¯è¨ˆç¯€çœ", f"{stats['total_savings_kw']:.1f} kW")
                    with col3:
                        st.metric("å¹³å‡ç¯€èƒ½ç‡", f"{stats['avg_savings_percent']:.1f}%")
                    with col4:
                        st.metric("æœ€é«˜ç¯€èƒ½ç‡", f"{stats['max_savings_percent']:.1f}%")
                    
                    st.markdown("---")
                    
                    # Prepare data for chart
                    import pandas as pd
                    import plotly.graph_objects as go
                    
                    history_df = pd.DataFrame([{
                        'æ™‚é–“': r.timestamp[:16].replace('T', ' '),
                        'ç¯€èƒ½ç‡ (%)': r.savings_percent,
                        'ç¯€çœé›»åŠ› (kW)': r.savings_kw,
                        'è² è¼‰ (RT)': r.load_rt,
                        'ç›®å‰èƒ½è€— (kW)': r.current_power_kw,
                        'æœ€ä½³èƒ½è€— (kW)': r.optimal_power_kw
                    } for r in records])
                    
                    # Savings trend chart
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=history_df['æ™‚é–“'],
                        y=history_df['ç¯€èƒ½ç‡ (%)'],
                        mode='lines+markers',
                        name='ç¯€èƒ½ç‡ (%)',
                        line=dict(color='#00CC96', width=2),
                        marker=dict(size=8)
                    ))
                    fig.update_layout(
                        title='ç¯€èƒ½ç‡è¶¨å‹¢',
                        xaxis_title='æ™‚é–“',
                        yaxis_title='ç¯€èƒ½ç‡ (%)',
                        height=350
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # History table
                    st.markdown("##### è©³ç´°ç´€éŒ„")
                    st.dataframe(
                        history_df[['æ™‚é–“', 'è² è¼‰ (RT)', 'ç›®å‰èƒ½è€— (kW)', 'æœ€ä½³èƒ½è€— (kW)', 'ç¯€çœé›»åŠ› (kW)', 'ç¯€èƒ½ç‡ (%)']],
                        hide_index=True,
                        use_container_width=True
                    )
                    
                    # Clear history button
                    st.markdown("---")
                    if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ­·å²ç´€éŒ„", type="secondary", key="clear_history_no_model"):
                        history_tracker.clear_history()
                        st.success("å·²æ¸…é™¤æ‰€æœ‰ç´€éŒ„")
                        st.rerun()
                else:
                    st.info("ğŸ“­ å°šç„¡æ­·å²ç´€éŒ„ã€‚è«‹å…ˆè¨“ç·´æ¨¡å‹ä¸¦åŸ·è¡Œå„ªåŒ–ã€‚")
            except Exception as e:
                st.error(f"è¼‰å…¥æ­·å²ç´€éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        with opt_tab4:
            st.subheader("ğŸ”§ è¨“ç·´æ–°æ¨¡å‹")
            st.markdown("ä½¿ç”¨æ‰¹æ¬¡è™•ç†å¾Œçš„è³‡æ–™è¨“ç·´èƒ½è€—é æ¸¬æ¨¡å‹")
            
            # Check if batch data is available
            if 'df_clean' in st.session_state or 'df_parsed' in st.session_state:
                df_for_training = st.session_state.get('df_clean', st.session_state.get('df_parsed'))
                
                st.info(f"ğŸ“Š å¯ç”¨è³‡æ–™: {len(df_for_training):,} ç­†")
                
                col1, col2 = st.columns(2)
                with col1:
                    new_model_name = st.text_input(
                        "æ¨¡å‹åç¨±",
                        value=f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        key="new_model_name_no_model"
                    )
                
                if st.button("ğŸ“ é–‹å§‹è¨“ç·´", type="primary", key="train_no_model"):
                    with st.spinner("æ­£åœ¨è¨“ç·´æ¨¡å‹..."):
                        try:
                            new_model = ChillerEnergyModel()
                            metrics = new_model.train(df_for_training)
                            
                            # Save model
                            model_path = f"models/{new_model_name}.joblib"
                            new_model.save_model(model_path)
                            
                            st.success(f"âœ… è¨“ç·´å®Œæˆï¼")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("MAPE", f"{metrics['mape']:.2f}%")
                            with col2:
                                st.metric("RÂ²", f"{metrics['r2']:.4f}")
                            with col3:
                                st.metric("RMSE", f"{metrics['rmse']:.2f}")
                            
                            st.info(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")
                            st.caption("é‡æ–°æ•´ç†é é¢å³å¯é¸æ“‡æ–°æ¨¡å‹")
                            
                        except Exception as e:
                            st.error(f"âŒ è¨“ç·´å¤±æ•—: {str(e)}")
            else:
                st.warning("è«‹å…ˆä½¿ç”¨ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼è¼‰å…¥ä¸¦æ¸…æ´—è³‡æ–™")
                st.caption("1. åˆ‡æ›åˆ°ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼")
                st.caption("2. é¸æ“‡æª”æ¡ˆä¸¦åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
                st.caption("3. å›åˆ°æ­¤é é¢é€²è¡Œæ¨¡å‹è¨“ç·´")

else:
    # Welcome screen
    st.info("ğŸ‘ˆ è«‹å¾å·¦å´ä¸Šå‚³æª”æ¡ˆæˆ–é¸æ“‡ç¾æœ‰è³‡æ–™é–‹å§‹")
    
    st.markdown("""
    ### åŠŸèƒ½ä»‹ç´¹
    
    #### ğŸ“‹ è§£æè³‡æ–™
    - è‡ªå‹•è§£æå ±è¡¨æ ¼å¼çš„ CSV æª”æ¡ˆ
    - æå– Point å°ç…§è¡¨
    - è½‰æ›æ™‚é–“æˆ³è¨˜
    
    #### ğŸ§¹ æ¸…æ´—è³‡æ–™
    - é‡æ¡æ¨£è‡³å›ºå®šæ™‚é–“é–“éš”ï¼ˆ5åˆ†é˜/15åˆ†é˜ç­‰ï¼‰
    - è¨ˆç®—æ¿•çƒæº«åº¦
    - åµæ¸¬å‡çµè³‡æ–™
    
    #### ğŸ“Š çµ±è¨ˆè³‡è¨Š
    - æŸ¥çœ‹æ¬„ä½çµ±è¨ˆæ•¸æ“š
    - æ•¸å€¼åˆ†å¸ƒè¦–è¦ºåŒ–
    
    #### ğŸ“ˆ æ™‚é–“åºåˆ—
    - å¤šè®Šæ•¸è¶¨å‹¢æ¯”è¼ƒ
    - æ™‚é–“ç¯„åœåˆ†æ
    
    #### ğŸ’¾ åŒ¯å‡º
    - CSV æ ¼å¼
    - Parquet æ ¼å¼ï¼ˆæ¨è–¦ï¼‰
    
    #### ğŸ“¦ æ‰¹æ¬¡è™•ç†
    - ä¸€æ¬¡è™•ç†å¤šå€‹æª”æ¡ˆ
    - è‡ªå‹•åˆä½µè³‡æ–™
    - é€²åº¦è¿½è¹¤
    
    #### âš¡ æœ€ä½³åŒ–æ¨¡æ“¬ (æ–°åŠŸèƒ½!)
    - è¼‰å…¥è¨“ç·´å¥½çš„èƒ½è€—é æ¸¬æ¨¡å‹
    - èª¿æ•´è®Šé »å™¨åƒæ•¸æŸ¥çœ‹é ä¼°èƒ½è€—
    - è‡ªå‹•æ‰¾å‡ºæœ€çœé›»çš„è¨­å®šçµ„åˆ
    - åˆ†æç‰¹å¾µé‡è¦æ€§
    """)

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**HVAC Analytics** | Spec-Kit Implementation")
st.sidebar.caption(f"ETL Pipeline v1.0")
