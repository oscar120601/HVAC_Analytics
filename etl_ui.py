import streamlit as st
import polars as pl
import sys
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from etl.parser import ReportParser
from etl.cleaner import DataCleaner

# Helper function to get numeric columns for analysis (excluding Date/Time)
def get_analysis_numeric_cols(df):
    """Get numeric columns suitable for statistical analysis, excluding Date/Time/timestamp."""
    # Columns to exclude from analysis
    exclude_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
    
    numeric_cols = [
        col for col in df.columns 
        if df[col].dtype in [pl.Float32, pl.Float64, pl.Int64, pl.Int32]
        and col not in exclude_cols
    ]
    return numeric_cols

st.set_page_config(
    page_title="HVAC ETL æ¸¬è©¦å·¥å…·",
    page_icon="ğŸ­",
    layout="wide"
)

st.title("ğŸ­ HVAC å†°æ°´ç³»çµ± - ETL æ¸¬è©¦ä»‹é¢")
st.markdown("**è³‡æ–™è§£æèˆ‡æ¸…æ´—å·¥å…·** | Chiller Plant Optimization")

# Sidebar
st.sidebar.header("âš™ï¸ è¨­å®š")

# Processing mode selection
processing_mode = st.sidebar.radio(
    "è™•ç†æ¨¡å¼",
    ["å–®ä¸€æª”æ¡ˆ", "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰"],
    help="é¸æ“‡å–®ä¸€æª”æ¡ˆæˆ–æ‰¹æ¬¡è™•ç†æ¨¡å¼"
)

# File selection based on mode
if processing_mode == "å–®ä¸€æª”æ¡ˆ":
    # File upload
    uploaded_file = st.sidebar.file_uploader(
        "ä¸Šå‚³ CSV å ±è¡¨æª”æ¡ˆ",
        type=['csv'],
        help="é¸æ“‡ TI_ANDY_SCHEDULER_USE_REPORT_*.csv æª”æ¡ˆ"
    )
    
    # Or select from data directory
    st.sidebar.markdown("---")
    st.sidebar.subheader("æˆ–å¾ç¾æœ‰è³‡æ–™é¸æ“‡")
    
    data_dir = Path("data/CGMH-TY")
    if data_dir.exists():
        csv_files = sorted([f.name for f in data_dir.glob("*.csv")])
        selected_file = st.sidebar.selectbox(
            "é¸æ“‡æª”æ¡ˆ",
            [""] + csv_files
        )
    else:
        selected_file = None
        st.sidebar.warning("æ‰¾ä¸åˆ°è³‡æ–™ç›®éŒ„")
else:
    # Batch mode
    uploaded_file = None
    selected_file = None
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("æ‰¹æ¬¡è™•ç†è¨­å®š")
    
    data_dir = Path("data/CGMH-TY")
    if data_dir.exists():
        csv_files = sorted([f.name for f in data_dir.glob("*.csv")])
        st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹æª”æ¡ˆ")
        
        # File range selection
        batch_mode_type = st.sidebar.radio(
            "é¸æ“‡ç¯„åœ",
            ["å…¨éƒ¨æª”æ¡ˆ", "é¸æ“‡æ—¥æœŸç¯„åœ"]
        )
        
        if batch_mode_type == "é¸æ“‡æ—¥æœŸç¯„åœ":
            col1, col2 = st.sidebar.columns(2)
            with col1:
                start_idx = st.number_input("èµ·å§‹æª”æ¡ˆ", 0, len(csv_files)-1, 0)
            with col2:
                end_idx = st.number_input("çµæŸæª”æ¡ˆ", 0, len(csv_files)-1, min(9, len(csv_files)-1))
            
            selected_files = csv_files[start_idx:end_idx+1]
            st.sidebar.caption(f"é¸æ“‡äº† {len(selected_files)} å€‹æª”æ¡ˆ")
        else:
            selected_files = csv_files
        
        # Clear batch data button
        if st.session_state.get('batch_processing_complete', False):
            st.sidebar.markdown("---")
            if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤æ‰¹æ¬¡è™•ç†è³‡æ–™", type="secondary"):
                st.session_state['batch_processing_complete'] = False
                st.session_state.pop('batch_file_count', None)
                st.session_state.pop('batch_auto_clean', None)
                st.rerun()
    else:
        st.sidebar.error("æ‰¾ä¸åˆ°è³‡æ–™ç›®éŒ„")
        selected_files = []

# Main content
# Main content
if processing_mode == "å–®ä¸€æª”æ¡ˆ" and (uploaded_file or selected_file):
    file_path = None
    
    if uploaded_file:
        # Save uploaded file temporarily
        temp_path = Path(f"/tmp/{uploaded_file.name}")
        with open(temp_path, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        file_path = str(temp_path)
        st.success(f"âœ… å·²ä¸Šå‚³: {uploaded_file.name}")
    elif selected_file:
        file_path = str(data_dir / selected_file)
        st.success(f"âœ… å·²é¸æ“‡: {selected_file}")
    
    # Tabs for different views
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“‹ è§£æè³‡æ–™", 
        "ğŸ§¹ æ¸…æ´—è³‡æ–™", 
        "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
        "ğŸ“ˆ æ™‚é–“åºåˆ—",
        "ğŸ’¾ åŒ¯å‡º"
    ])
    
    with tab1:
        st.header("åŸå§‹è³‡æ–™è§£æ")
        
        try:
            with st.spinner("æ­£åœ¨è§£æå ±è¡¨..."):
                parser = ReportParser()
                df_parsed = parser.parse_file(file_path)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{len(df_parsed):,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{len(df_parsed.columns):,}")
            with col3:
                if 'timestamp' in df_parsed.columns:
                    time_range = df_parsed['timestamp'].max() - df_parsed['timestamp'].min()
                    st.metric("æ™‚é–“ç¯„åœ", f"{time_range}")
            
            st.subheader("è³‡æ–™é è¦½ï¼ˆå‰ 100 ç­†ï¼‰")
            st.dataframe(
                df_parsed.head(100).to_pandas(),
                use_container_width=True,
                height=400
            )
            
            st.subheader("æ¬„ä½æ¸…å–®")
            col_list = st.columns(4)
            for i, col in enumerate(df_parsed.columns):
                with col_list[i % 4]:
                    st.text(f"â€¢ {col}")
            
            # Store in session state for other tabs
            st.session_state['df_parsed'] = df_parsed
            
        except Exception as e:
            st.error(f"âŒ è§£æéŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    with tab2:
        st.header("è³‡æ–™æ¸…æ´—")
        
        if 'df_parsed' in st.session_state:
            df_parsed = st.session_state['df_parsed']
            
            # Cleaning options
            st.subheader("æ¸…æ´—é¸é …")
            col1, col2 = st.columns(2)
            
            with col1:
                resample_interval = st.selectbox(
                    "é‡æ¡æ¨£é–“éš”",
                    ["5m", "10m", "15m", "30m", "1h"],
                    index=0
                )
            
            with col2:
                detect_frozen = st.checkbox("æª¢æ¸¬å‡çµè³‡æ–™", value=True)
            
            if st.button("ğŸ§¹ é–‹å§‹æ¸…æ´—", type="primary"):
                try:
                    with st.spinner("æ­£åœ¨æ¸…æ´—è³‡æ–™..."):
                        cleaner = DataCleaner(resample_interval=resample_interval)
                        df_clean = cleaner.clean_data(df_parsed)
                    
                    st.success(f"âœ… æ¸…æ´—å®Œæˆï¼")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("åŸå§‹åˆ—æ•¸", f"{len(df_parsed):,}")
                    with col2:
                        st.metric("æ¸…æ´—å¾Œåˆ—æ•¸", f"{len(df_clean):,}")
                    
                    st.subheader("æ¸…æ´—å¾Œè³‡æ–™é è¦½")
                    st.dataframe(
                        df_clean.head(100).to_pandas(),
                        use_container_width=True,
                        height=400
                    )
                    
                    # Check for frozen data flags
                    frozen_cols = [col for col in df_clean.columns if '_frozen' in col]
                    if frozen_cols:
                        st.subheader("âš ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
                        for col in frozen_cols:
                            frozen_count = df_clean[col].sum()
                            if frozen_count > 0:
                                st.warning(f"{col.replace('_frozen', '')}: {frozen_count} ç­†å‡çµè³‡æ–™")
                    
                    st.session_state['df_clean'] = df_clean
                    
                except Exception as e:
                    st.error(f"âŒ æ¸…æ´—éŒ¯èª¤: {str(e)}")
                    st.exception(e)
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab3:
        st.header("çµ±è¨ˆè³‡è¨Š")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
            
            # Select numeric columns for stats (excluding Date/Time)
            numeric_cols = get_analysis_numeric_cols(df)
            
            if numeric_cols:
                selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols)
                
                if selected_col:
                    col_data = df[selected_col]
                    
                    # Filter out nulls for statistics
                    col_data_clean = col_data.drop_nulls()
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        st.metric("å¹³å‡å€¼", f"{col_data_clean.mean():.2f}")
                    with col2:
                        st.metric("ä¸­ä½æ•¸", f"{col_data_clean.median():.2f}")
                    with col3:
                        st.metric("æœ€å°å€¼", f"{col_data_clean.min():.2f}")
                    with col4:
                        st.metric("æœ€å¤§å€¼", f"{col_data_clean.max():.2f}")
                    with col5:
                        st.metric("æ¨™æº–å·®", f"{col_data_clean.std():.2f}")
                    
                    # Distribution visualization
                    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
                    
                    # Convert to pandas for histogram
                    pandas_data = col_data_clean.to_pandas()
                    
                    if len(pandas_data) > 0:
                        # Create histogram using numpy
                        import numpy as np
                        
                        # Calculate histogram bins
                        counts, bin_edges = np.histogram(pandas_data, bins=30)
                        
                        # Create bin labels (using bin centers)
                        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                        
                        # Create dataframe for plotting
                        import pandas as pd
                        hist_df = pd.DataFrame({
                            'value': bin_centers,
                            'count': counts
                        }).set_index('value')
                        
                        if hist_df['count'].sum() > 0:
                            st.bar_chart(hist_df)
                        else:
                            st.info("è³‡æ–™ç¯„åœå¤ªå°ï¼Œç„¡æ³•ç”¢ç”Ÿåˆ†å¸ƒåœ–")
                        
                        # Show data range info
                        data_range = col_data_clean.max() - col_data_clean.min()
                        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
                    else:
                        st.warning("æ­¤æ¬„ä½æ²’æœ‰æœ‰æ•ˆæ•¸å€¼")
            else:
                st.info("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab4:
        st.header("æ™‚é–“åºåˆ—åˆ†æ")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            # Check if timestamp exists
            if 'timestamp' in df.columns:
                # Select numeric columns (excluding Date/Time)
                numeric_cols = get_analysis_numeric_cols(df)
                
                if numeric_cols:
                    st.subheader("é¸æ“‡æ¬„ä½é€²è¡Œæ™‚é–“åºåˆ—åˆ†æ")
                    
                    # Multi-select for comparison
                    selected_cols = st.multiselect(
                        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
                        numeric_cols,
                        default=[numeric_cols[0]] if numeric_cols else [],
                        max_selections=3
                    )
                    
                    if selected_cols:
                        # Create time series chart
                        pandas_df = df.select(['timestamp'] + selected_cols).to_pandas()
                        pandas_df = pandas_df.set_index('timestamp')
                        
                        st.line_chart(pandas_df)
                        
                        # Show data summary
                        st.caption(f"æ™‚é–“ç¯„åœ: {df['timestamp'].min()} è‡³ {df['timestamp'].max()}")
                        st.caption(f"è³‡æ–™é»æ•¸: {len(df):,}")
                    else:
                        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")
                else:
                    st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            else:
                st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab5:
        st.header("åŒ¯å‡ºè³‡æ–™")
        
        if 'df_parsed' in st.session_state or 'df_clean' in st.session_state:
            
            export_type = st.radio(
                "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
                ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"]
            )
            
            df_to_export = None
            if export_type == "è§£æå¾Œè³‡æ–™" and 'df_parsed' in st.session_state:
                df_to_export = st.session_state['df_parsed']
            elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and 'df_clean' in st.session_state:
                df_to_export = st.session_state['df_clean']
            
            if df_to_export is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    # CSV export
                    csv_data = df_to_export.write_csv()
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ CSV",
                        data=csv_data,
                        file_name=f"hvac_etl_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Parquet export (more efficient)
                    # Use BytesIO buffer since write_parquet needs a file
                    from io import BytesIO
                    buffer = BytesIO()
                    df_to_export.write_parquet(buffer)
                    parquet_data = buffer.getvalue()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ Parquet",
                        data=parquet_data,
                        file_name=f"hvac_etl_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
                        mime="application/octet-stream"
                    )
                
                st.info("ğŸ’¡ Parquet æ ¼å¼è¼ƒå°ä¸”æ•ˆèƒ½æ›´å¥½ï¼Œé©åˆå¤§å‹è³‡æ–™é›†")
            else:
                st.warning("è«‹å…ˆæ¸…æ´—è³‡æ–™æˆ–é¸æ“‡è§£æå¾Œè³‡æ–™åŒ¯å‡º")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")

elif processing_mode == "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰" and selected_files:
    st.header("ğŸ“¦ æ‰¹æ¬¡è™•ç†æ¨¡å¼")
    
    st.info(f"æº–å‚™è™•ç† {len(selected_files)} å€‹æª”æ¡ˆ")
    
    # Show file list preview
    with st.expander("æŸ¥çœ‹æª”æ¡ˆæ¸…å–®"):
        if len(selected_files) <= 10:
            for f in selected_files:
                st.text(f"â€¢ {f}")
        else:
            st.text("å‰ 5 å€‹æª”æ¡ˆ:")
            for f in selected_files[:5]:
                st.text(f"  â€¢ {f}")
            st.text(f"  ... ({len(selected_files) - 10} å€‹æª”æ¡ˆ)")
            st.text("å¾Œ 5 å€‹æª”æ¡ˆ:")
            for f in selected_files[-5:]:
                st.text(f"  â€¢ {f}")
    
    # Processing options
    col1, col2 = st.columns(2)
    with col1:
        batch_resample = st.selectbox("é‡æ¡æ¨£é–“éš”", ["5m", "10m", "15m", "30m", "1h"], index=0)
    with col2:
        auto_clean = st.checkbox("è‡ªå‹•æ¸…æ´—è³‡æ–™", value=True)
    
    # Start batch processing
    if st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡è™•ç†", type="primary"):
        try:
            from etl.batch_processor import BatchProcessor
            
            # Prepare file paths
            file_paths = [str(data_dir / f) for f in selected_files]
            
            # Create processor
            processor = BatchProcessor(resample_interval=batch_resample)
            
            # Progress bar
            status_text = st.empty()
            status_text.text("æ­£åœ¨è™•ç†æª”æ¡ˆ...")
            
            with st.spinner("è™•ç†ä¸­..."):
                merged_df = processor.process_files(file_paths, clean=auto_clean)
            
            status_text.text("è™•ç†å®Œæˆ!")
            
            # Store in session state
            if auto_clean:
                st.session_state['df_clean'] = merged_df
                st.session_state['df_parsed'] = merged_df
            else:
                st.session_state['df_parsed'] = merged_df
            
            # Mark batch processing as complete
            st.session_state['batch_processing_complete'] = True
            st.session_state['batch_file_count'] = len(selected_files)
            st.session_state['batch_auto_clean'] = auto_clean
            
        except Exception as e:
            st.error(f"âŒ æ‰¹æ¬¡è™•ç†éŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    # Show analysis tabs if batch processing is complete (persists across re-renders)
    if st.session_state.get('batch_processing_complete', False):
        # Get data from session state
        if 'df_clean' in st.session_state:
            merged_df = st.session_state['df_clean']
        elif 'df_parsed' in st.session_state:
            merged_df = st.session_state['df_parsed']
        else:
            st.error("è³‡æ–™éºå¤±ï¼Œè«‹é‡æ–°åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
            if st.button("é‡ç½®"):
                st.session_state['batch_processing_complete'] = False
                st.rerun()
            st.stop()
        
        batch_file_count = st.session_state.get('batch_file_count', 0)
        auto_clean = st.session_state.get('batch_auto_clean', True)
            
        # Show summary
        st.success(f"âœ… æˆåŠŸè™•ç† {batch_file_count} å€‹æª”æ¡ˆ")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç¸½åˆ—æ•¸", f"{len(merged_df):,}")
        with col2:
            st.metric("ç¸½æ¬„ä½æ•¸", f"{len(merged_df.columns):,}")
        with col3:
            if 'timestamp' in merged_df.columns:
                time_range = merged_df['timestamp'].max() - merged_df['timestamp'].min()
                st.metric("æ™‚é–“ç¯„åœ", str(time_range))
        
        st.markdown("---")
        st.info("ğŸ“Š **è³‡æ–™å·²è¼‰å…¥ï¼** è«‹ä½¿ç”¨ä¸‹æ–¹æ¨™ç±¤é åˆ†æåˆä½µå¾Œçš„è³‡æ–™")
        
        # Analysis tabs
        batch_tab1, batch_tab2, batch_tab3, batch_tab4 = st.tabs([
            "ğŸ“‹ è³‡æ–™é è¦½",
            "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
            "ğŸ“ˆ æ™‚é–“åºåˆ—",
            "ğŸ’¾ åŒ¯å‡º"
        ])
            
        with batch_tab1:
            st.subheader("åˆä½µå¾Œè³‡æ–™é è¦½")
            st.dataframe(merged_df.head(100).to_pandas(), use_container_width=True)
            st.caption(f"é¡¯ç¤ºå‰ 100 ç­†ï¼Œå…± {len(merged_df):,} ç­†è³‡æ–™")
            
        with batch_tab2:
            st.subheader("çµ±è¨ˆè³‡è¨Š")
            
            # Show data status
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
            
            # Select numeric columns (excluding Date/Time)
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            if numeric_cols:
                selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols, key="batch_stats_col")
                
                if selected_col:
                    col_data_clean = merged_df[selected_col].drop_nulls()
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        mean_val = col_data_clean.mean()
                        st.metric("å¹³å‡å€¼", f"{mean_val:.2f}" if mean_val is not None else "N/A")
                    with col2:
                        median_val = col_data_clean.median()
                        st.metric("ä¸­ä½æ•¸", f"{median_val:.2f}" if median_val is not None else "N/A")
                    with col3:
                        min_val = col_data_clean.min()
                        st.metric("æœ€å°å€¼", f"{min_val:.2f}" if min_val is not None else "N/A")
                    with col4:
                        max_val = col_data_clean.max()
                        st.metric("æœ€å¤§å€¼", f"{max_val:.2f}" if max_val is not None else "N/A")
                    with col5:
                        std_val = col_data_clean.std()
                        st.metric("æ¨™æº–å·®", f"{std_val:.2f}" if std_val is not None else "N/A")
                    
                    # Distribution
                    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
                    pandas_data = col_data_clean.to_pandas()
                    
                    if len(pandas_data) > 0:
                        import numpy as np
                        import pandas as pd
                        
                        counts, bin_edges = np.histogram(pandas_data, bins=30)
                        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                        hist_df = pd.DataFrame({'value': bin_centers, 'count': counts}).set_index('value')
                        
                        if hist_df['count'].sum() > 0:
                            st.bar_chart(hist_df)
                        
                        data_range = col_data_clean.max() - col_data_clean.min()
                        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            
        with batch_tab3:
            st.subheader("æ™‚é–“åºåˆ—åˆ†æ")
            
            if 'timestamp' in merged_df.columns:
                numeric_cols = get_analysis_numeric_cols(merged_df)
                
                if numeric_cols:
                    selected_cols = st.multiselect(
                        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
                        numeric_cols,
                        default=[numeric_cols[0]] if numeric_cols else [],
                        max_selections=3,
                        key="batch_timeseries_cols"
                    )
                    
                    if selected_cols:
                        pandas_df = merged_df.select(['timestamp'] + selected_cols).to_pandas()
                        pandas_df = pandas_df.set_index('timestamp')
                        st.line_chart(pandas_df)
                        
                        st.caption(f"æ™‚é–“ç¯„åœ: {merged_df['timestamp'].min()} è‡³ {merged_df['timestamp'].max()}")
                        st.caption(f"è³‡æ–™é»æ•¸: {len(merged_df):,}")
                    else:
                        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")
                else:
                    st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            else:
                st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
            
        with batch_tab4:
            st.header("åŒ¯å‡ºè³‡æ–™")
            
            # Data selection radio (matching single file mode)
            export_type = st.radio(
                "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
                ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"],
                key="batch_export_type"
            )
            
            df_to_export = None
            if export_type == "è§£æå¾Œè³‡æ–™" and 'df_parsed' in st.session_state:
                df_to_export = st.session_state['df_parsed']
                st.info("ğŸ“Š **åŒ¯å‡ºï¼šè§£æå¾Œè³‡æ–™**ï¼ˆåŸå§‹åˆä½µè³‡æ–™ï¼‰")
            elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and 'df_clean' in st.session_state:
                df_to_export = st.session_state['df_clean']
                st.info("ğŸ“Š **åŒ¯å‡ºï¼šæ¸…æ´—å¾Œè³‡æ–™**ï¼ˆå·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼ï¼‰")
            
            if df_to_export is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    # CSV export
                    csv_data = df_to_export.write_csv()
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ CSV",
                        data=csv_data,
                        file_name=f"hvac_batch_{batch_file_count}files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Parquet export
                    from io import BytesIO
                    buffer = BytesIO()
                    df_to_export.write_parquet(buffer)
                    parquet_data = buffer.getvalue()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ Parquet",
                        data=parquet_data,
                        file_name=f"hvac_batch_{batch_file_count}files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
                        mime="application/octet-stream"
                    )
                
                st.info("ğŸ’¡ Parquet æ ¼å¼è¼ƒå°ä¸”æ•ˆèƒ½æ›´å¥½ï¼Œé©åˆå¤§å‹è³‡æ–™é›†")
            else:
                if export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰":
                    st.warning("è«‹å…ˆåŸ·è¡Œè³‡æ–™æ¸…æ´—æˆ–é¸æ“‡ã€Œè§£æå¾Œè³‡æ–™ã€")
                else:
                    st.warning("æ²’æœ‰å¯åŒ¯å‡ºçš„è³‡æ–™")

else:
    # Welcome screen
    st.info("ğŸ‘ˆ è«‹å¾å·¦å´ä¸Šå‚³æª”æ¡ˆæˆ–é¸æ“‡ç¾æœ‰è³‡æ–™é–‹å§‹")
    
    st.markdown("""
    ### åŠŸèƒ½ä»‹ç´¹
    
    #### ğŸ“‹ è§£æè³‡æ–™
    - è‡ªå‹•è§£æå ±è¡¨æ ¼å¼çš„ CSV æª”æ¡ˆ
    - æå– Point å°ç…§è¡¨
    - è½‰æ›æ™‚é–“æˆ³è¨˜
    
    #### ğŸ§¹ æ¸…æ´—è³‡æ–™
    - é‡æ¡æ¨£è‡³å›ºå®šæ™‚é–“é–“éš”ï¼ˆ5åˆ†é˜/15åˆ†é˜ç­‰ï¼‰
    - è¨ˆç®—æ¿•çƒæº«åº¦
    - åµæ¸¬å‡çµè³‡æ–™
    
    #### ğŸ“Š çµ±è¨ˆè³‡è¨Š
    - æŸ¥çœ‹æ¬„ä½çµ±è¨ˆæ•¸æ“š
    - æ•¸å€¼åˆ†å¸ƒè¦–è¦ºåŒ–
    
    #### ğŸ“ˆ æ™‚é–“åºåˆ—
    - å¤šè®Šæ•¸è¶¨å‹¢æ¯”è¼ƒ
    - æ™‚é–“ç¯„åœåˆ†æ
    
    #### ğŸ’¾ åŒ¯å‡º
    - CSV æ ¼å¼
    - Parquet æ ¼å¼ï¼ˆæ¨è–¦ï¼‰
    
    #### ğŸ“¦ æ‰¹æ¬¡è™•ç†
    - ä¸€æ¬¡è™•ç†å¤šå€‹æª”æ¡ˆ
    - è‡ªå‹•åˆä½µè³‡æ–™
    - é€²åº¦è¿½è¹¤
    """)

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**HVAC Analytics** | Spec-Kit Implementation")
st.sidebar.caption(f"ETL Pipeline v1.0")
