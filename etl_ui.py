import streamlit as st
import polars as pl
import sys
import json
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from etl.parser import ReportParser
from etl.cleaner import DataCleaner

# Try to import ML modules (may not be available if dependencies missing)
try:
    from models.energy_model import ChillerEnergyModel
    from optimization.optimizer import ChillerOptimizer, OptimizationContext
    from optimization.history_tracker import OptimizationHistoryTracker, create_record_from_result
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# Import feature mapping (V2 with HVAC physical system hierarchy)
try:
    from config.feature_mapping_v2 import FeatureMapping, get_feature_mapping, PREDEFINED_MAPPINGS, STANDARD_CATEGORIES
    FEATURE_MAPPING_AVAILABLE = True
except ImportError:
    FEATURE_MAPPING_AVAILABLE = False
    FeatureMapping = None
    get_feature_mapping = None
    PREDEFINED_MAPPINGS = {}
    STANDARD_CATEGORIES = {}

# Helper function to get numeric columns for analysis (excluding Date/Time)
def get_analysis_numeric_cols(df):
    """Get numeric columns suitable for statistical analysis, excluding Date/Time/timestamp."""
    # Columns to exclude from analysis
    exclude_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
    
    numeric_cols = [
        col for col in df.columns 
        if df[col].dtype in [pl.Float32, pl.Float64, pl.Int64, pl.Int32]
        and col not in exclude_cols
    ]
    return numeric_cols

st.set_page_config(
    page_title="HVAC ETL æ¸¬è©¦å·¥å…·",
    page_icon="ğŸ­",
    layout="wide"
)

st.title("ğŸ­ HVAC å†°æ°´ç³»çµ± - ETL æ¸¬è©¦ä»‹é¢")
st.markdown("**è³‡æ–™è§£æèˆ‡æ¸…æ´—å·¥å…·** | Chiller Plant Optimization")

# Sidebar
st.sidebar.header("âš™ï¸ è¨­å®š")

# Processing mode selection
mode_options = ["å–®ä¸€æª”æ¡ˆ", "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰"]
if ML_AVAILABLE:
    mode_options.append("âš¡ æœ€ä½³åŒ–æ¨¡æ“¬")

processing_mode = st.sidebar.radio(
    "è™•ç†æ¨¡å¼",
    mode_options,
    help="é¸æ“‡å–®ä¸€æª”æ¡ˆã€æ‰¹æ¬¡è™•ç†æˆ–æœ€ä½³åŒ–æ¨¡æ“¬æ¨¡å¼"
)

# File selection based on mode
if processing_mode == "å–®ä¸€æª”æ¡ˆ":
    # File upload
    uploaded_file = st.sidebar.file_uploader(
        "ä¸Šå‚³ CSV å ±è¡¨æª”æ¡ˆ",
        type=['csv'],
        help="é¸æ“‡ TI_ANDY_SCHEDULER_USE_REPORT_*.csv æª”æ¡ˆ"
    )
    
    # Or select from data directory
    st.sidebar.markdown("---")
    st.sidebar.subheader("æˆ–å¾ç¾æœ‰è³‡æ–™é¸æ“‡")
    
    data_dir = Path("data/CGMH-TY")
    if data_dir.exists():
        csv_files = sorted([f.name for f in data_dir.glob("*.csv")])
        selected_file = st.sidebar.selectbox(
            "é¸æ“‡æª”æ¡ˆ",
            [""] + csv_files
        )
    else:
        selected_file = None
        st.sidebar.warning("æ‰¾ä¸åˆ°è³‡æ–™ç›®éŒ„")
    selected_files = []

elif processing_mode == "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰":
    # Batch mode
    uploaded_file = None
    selected_file = None
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("æ‰¹æ¬¡è™•ç†è¨­å®š")
    
    data_dir = Path("data/CGMH-TY")
    if data_dir.exists():
        csv_files = sorted([f.name for f in data_dir.glob("*.csv")])
        st.sidebar.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹æª”æ¡ˆ")
        
        # File range selection
        batch_mode_type = st.sidebar.radio(
            "é¸æ“‡ç¯„åœ",
            ["å…¨éƒ¨æª”æ¡ˆ", "é¸æ“‡æ—¥æœŸç¯„åœ"]
        )
        
        if batch_mode_type == "é¸æ“‡æ—¥æœŸç¯„åœ":
            col1, col2 = st.sidebar.columns(2)
            with col1:
                start_idx = st.number_input("èµ·å§‹æª”æ¡ˆ", 0, len(csv_files)-1, 0)
            with col2:
                end_idx = st.number_input("çµæŸæª”æ¡ˆ", 0, len(csv_files)-1, min(9, len(csv_files)-1))
            
            selected_files = csv_files[start_idx:end_idx+1]
            st.sidebar.caption(f"é¸æ“‡äº† {len(selected_files)} å€‹æª”æ¡ˆ")
        else:
            selected_files = csv_files
        
        # Clear batch data button
        if st.session_state.get('batch_processing_complete', False):
            st.sidebar.markdown("---")
            if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤æ‰¹æ¬¡è™•ç†è³‡æ–™", type="secondary"):
                st.session_state['batch_processing_complete'] = False
                st.session_state.pop('batch_file_count', None)
                st.session_state.pop('batch_auto_clean', None)
                st.rerun()
    else:
        st.sidebar.error("æ‰¾ä¸åˆ°è³‡æ–™ç›®éŒ„")
        selected_files = []

elif processing_mode == "âš¡ æœ€ä½³åŒ–æ¨¡æ“¬":
    # Optimization mode
    uploaded_file = None
    selected_file = None
    selected_files = []
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("æ¨¡å‹è¨­å®š")
    
    # Model file selection
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)
    
    model_files = list(model_dir.glob("*.joblib"))
    if model_files:
        model_file_names = [f.name for f in model_files]
        selected_model = st.sidebar.selectbox(
            "é¸æ“‡å·²è¨“ç·´æ¨¡å‹",
            model_file_names
        )
        
        # Delete model button
        st.sidebar.markdown("---")
        if st.sidebar.button("ğŸ—‘ï¸ åˆªé™¤é¸æ“‡çš„æ¨¡å‹", type="secondary"):
            delete_path = model_dir / selected_model
            try:
                delete_path.unlink()
                st.sidebar.success(f"âœ… å·²åˆªé™¤: {selected_model}")
                st.sidebar.caption("è«‹é‡æ–°æ•´ç†é é¢")
                selected_model = None
            except Exception as e:
                st.sidebar.error(f"âŒ åˆªé™¤å¤±æ•—: {e}")
    else:
        selected_model = None
        st.sidebar.warning("å°šæœªè¨“ç·´æ¨¡å‹")
        st.sidebar.caption("è«‹å…ˆä½¿ç”¨æ‰¹æ¬¡è™•ç†æ¨¡å¼è¨“ç·´æ¨¡å‹")

else:
    uploaded_file = None
    selected_file = None
    selected_files = []

# Main content
# Main content
if processing_mode == "å–®ä¸€æª”æ¡ˆ" and (uploaded_file or selected_file):
    file_path = None
    
    if uploaded_file:
        # Save uploaded file temporarily
        temp_path = Path(f"/tmp/{uploaded_file.name}")
        with open(temp_path, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        file_path = str(temp_path)
        st.success(f"âœ… å·²ä¸Šå‚³: {uploaded_file.name}")
    elif selected_file:
        file_path = str(data_dir / selected_file)
        st.success(f"âœ… å·²é¸æ“‡: {selected_file}")
    
    # Tabs for different views
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“‹ è§£æè³‡æ–™", 
        "ğŸ§¹ æ¸…æ´—è³‡æ–™", 
        "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
        "ğŸ“ˆ æ™‚é–“åºåˆ—",
        "ğŸ”— é—œè¯çŸ©é™£",
        "ğŸ¯ è³‡æ–™å“è³ª",
        "ğŸ’¾ åŒ¯å‡º"
    ])
    
    with tab1:
        st.header("åŸå§‹è³‡æ–™è§£æ")
        
        try:
            with st.spinner("æ­£åœ¨è§£æå ±è¡¨..."):
                parser = ReportParser()
                df_parsed = parser.parse_file(file_path)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{len(df_parsed):,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{len(df_parsed.columns):,}")
            with col3:
                if 'timestamp' in df_parsed.columns:
                    time_range = df_parsed['timestamp'].max() - df_parsed['timestamp'].min()
                    st.metric("æ™‚é–“ç¯„åœ", f"{time_range}")
            
            st.subheader("è³‡æ–™é è¦½ï¼ˆå‰ 100 ç­†ï¼‰")
            st.dataframe(
                df_parsed.head(100).to_pandas(),
                use_container_width=True,
                height=400
            )
            
            st.subheader("æ¬„ä½æ¸…å–®")
            col_list = st.columns(4)
            for i, col in enumerate(df_parsed.columns):
                with col_list[i % 4]:
                    st.text(f"â€¢ {col}")
            
            # Store in session state for other tabs
            st.session_state['df_parsed'] = df_parsed
            
        except Exception as e:
            st.error(f"âŒ è§£æéŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    with tab2:
        st.header("è³‡æ–™æ¸…æ´—")
        
        if 'df_parsed' in st.session_state:
            df_parsed = st.session_state['df_parsed']
            
            # Cleaning options
            st.subheader("æ¸…æ´—é¸é …")
            
            # Basic options
            col1, col2 = st.columns(2)
            with col1:
                resample_interval = st.selectbox(
                    "é‡æ¡æ¨£é–“éš”",
                    ["5m", "10m", "15m", "30m", "1h"],
                    index=0
                )
            with col2:
                detect_frozen = st.checkbox("æª¢æ¸¬å‡çµè³‡æ–™", value=True)
            
            # Physics-based validation options
            st.subheader("ç‰©ç†é©—è­‰é¸é …")
            col1, col2, col3 = st.columns(3)
            with col1:
                apply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False, 
                    help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™")
            with col2:
                apply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
                    help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚")
            with col3:
                apply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
                    help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚")
            
            # Filter options
            filter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
                help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—")
            
            if st.button("ğŸ§¹ é–‹å§‹æ¸…æ´—", type="primary"):
                try:
                    with st.spinner("æ­£åœ¨æ¸…æ´—è³‡æ–™..."):
                        cleaner = DataCleaner(resample_interval=resample_interval)
                        df_clean = cleaner.clean_data(
                            df_parsed,
                            apply_steady_state=apply_steady_state,
                            apply_heat_balance=apply_heat_balance,
                            apply_affinity_laws=apply_affinity,
                            filter_invalid=filter_invalid
                        )
                    
                    st.success(f"âœ… æ¸…æ´—å®Œæˆï¼")
                    
                    # Show metrics
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("åŸå§‹åˆ—æ•¸", f"{len(df_parsed):,}")
                    with col2:
                        st.metric("æ¸…æ´—å¾Œåˆ—æ•¸", f"{len(df_clean):,}")
                    with col3:
                        retention = len(df_clean) / len(df_parsed) * 100 if len(df_parsed) > 0 else 0
                        st.metric("ä¿ç•™ç‡", f"{retention:.1f}%")
                    
                    # Show validation results
                    validation_results = []
                    if apply_steady_state and "is_steady_state" in df_clean.columns:
                        steady_count = df_clean["is_steady_state"].sum()
                        validation_results.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
                    if apply_heat_balance and "heat_balance_invalid" in df_clean.columns:
                        invalid_count = df_clean["heat_balance_invalid"].sum()
                        validation_results.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
                    if apply_affinity and "affinity_law_invalid" in df_clean.columns:
                        invalid_count = df_clean["affinity_law_invalid"].sum()
                        validation_results.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
                    
                    if validation_results:
                        st.info(" | ".join(validation_results))
                    
                    st.subheader("æ¸…æ´—å¾Œè³‡æ–™é è¦½")
                    st.dataframe(
                        df_clean.head(100).to_pandas(),
                        use_container_width=True,
                        height=400
                    )
                    
                    # Check for frozen data flags
                    frozen_cols = [col for col in df_clean.columns if '_frozen' in col]
                    if frozen_cols:
                        st.subheader("âš ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
                        for col in frozen_cols:
                            frozen_count = df_clean[col].sum()
                            if frozen_count > 0:
                                st.warning(f"{col.replace('_frozen', '')}: {frozen_count} ç­†å‡çµè³‡æ–™")
                    
                    st.session_state['df_clean'] = df_clean
                    
                except Exception as e:
                    st.error(f"âŒ æ¸…æ´—éŒ¯èª¤: {str(e)}")
                    st.exception(e)
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab3:
        st.header("çµ±è¨ˆè³‡è¨Š")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
            
            # Select numeric columns for stats (excluding Date/Time)
            numeric_cols = get_analysis_numeric_cols(df)
            
            if numeric_cols:
                selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols)
                
                if selected_col:
                    col_data = df[selected_col]
                    
                    # Filter out nulls for statistics
                    col_data_clean = col_data.drop_nulls()
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        st.metric("å¹³å‡å€¼", f"{col_data_clean.mean():.2f}")
                    with col2:
                        st.metric("ä¸­ä½æ•¸", f"{col_data_clean.median():.2f}")
                    with col3:
                        st.metric("æœ€å°å€¼", f"{col_data_clean.min():.2f}")
                    with col4:
                        st.metric("æœ€å¤§å€¼", f"{col_data_clean.max():.2f}")
                    with col5:
                        st.metric("æ¨™æº–å·®", f"{col_data_clean.std():.2f}")
                    
                    # Distribution visualization
                    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
                    
                    # Convert to pandas for histogram
                    pandas_data = col_data_clean.to_pandas()
                    
                    if len(pandas_data) > 0:
                        # Create histogram using numpy
                        import numpy as np
                        
                        # Calculate histogram bins
                        counts, bin_edges = np.histogram(pandas_data, bins=30)
                        
                        # Create bin labels (using bin centers)
                        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                        
                        # Create dataframe for plotting
                        import pandas as pd
                        hist_df = pd.DataFrame({
                            'value': bin_centers,
                            'count': counts
                        }).set_index('value')
                        
                        if hist_df['count'].sum() > 0:
                            st.bar_chart(hist_df)
                        else:
                            st.info("è³‡æ–™ç¯„åœå¤ªå°ï¼Œç„¡æ³•ç”¢ç”Ÿåˆ†å¸ƒåœ–")
                        
                        # Show data range info
                        data_range = col_data_clean.max() - col_data_clean.min()
                        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
                    else:
                        st.warning("æ­¤æ¬„ä½æ²’æœ‰æœ‰æ•ˆæ•¸å€¼")
            else:
                st.info("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab4:
        st.header("æ™‚é–“åºåˆ—åˆ†æ")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            # Check if timestamp exists
            if 'timestamp' in df.columns:
                # Select numeric columns (excluding Date/Time)
                numeric_cols = get_analysis_numeric_cols(df)
                
                if numeric_cols:
                    st.subheader("é¸æ“‡æ¬„ä½é€²è¡Œæ™‚é–“åºåˆ—åˆ†æ")
                    
                    # Multi-select for comparison
                    selected_cols = st.multiselect(
                        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
                        numeric_cols,
                        default=[numeric_cols[0]] if numeric_cols else [],
                        max_selections=3
                    )
                    
                    if selected_cols:
                        # Create time series chart
                        pandas_df = df.select(['timestamp'] + selected_cols).to_pandas()
                        pandas_df = pandas_df.set_index('timestamp')
                        
                        st.line_chart(pandas_df)
                        
                        # Show data summary
                        st.caption(f"æ™‚é–“ç¯„åœ: {df['timestamp'].min()} è‡³ {df['timestamp'].max()}")
                        st.caption(f"è³‡æ–™é»æ•¸: {len(df):,}")
                    else:
                        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")
                else:
                    st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            else:
                st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
# å–®ä¸€æª”æ¡ˆ tab5 å’Œ tab6 çš„å…§å®¹

    with tab5:
        st.header("ğŸ”— é—œè¯çŸ©é™£ç†±åœ–")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            numeric_cols = get_analysis_numeric_cols(df)
            
            if numeric_cols:
                st.subheader("é¸æ“‡è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
                
                # Let user select variables (max 15 for readability)
                max_vars = min(15, len(numeric_cols))
                selected_vars = st.multiselect(
                    f"é¸æ“‡è¦åˆ†æçš„è®Šæ•¸ï¼ˆæœ€å¤š {max_vars} å€‹ï¼Œå»ºè­° 5-10 å€‹ï¼‰",
                    numeric_cols,
                    default=numeric_cols[:min(8, len(numeric_cols))],
                    max_selections=max_vars
                )
                
                if len(selected_vars) >= 2:
                    try:
                        # Calculate correlation matrix
                        import plotly.figure_factory as ff
                        import numpy as np
                        
                        # Extract data and convert to pandas
                        corr_df = df.select(selected_vars).to_pandas()
                        
                        # Calculate correlation matrix
                        corr_matrix = corr_df.corr()
                        
                        # Create heatmap using plotly
                        fig = ff.create_annotated_heatmap(
                            z=corr_matrix.values,
                            x=list(corr_matrix.columns),
                            y=list(corr_matrix.index),
                            annotation_text=np.around(corr_matrix.values, decimals=2),
                            colorscale='RdBu',
                            zmid=0,
                            showscale=True
                        )
                        
                        fig.update_layout(
                            title="è®Šæ•¸ç›¸é—œæ€§çŸ©é™£",
                            xaxis_title="",
                            yaxis_title="",
                            height=600,
                            xaxis={'side': 'bottom'}
                        )
                        
                        # Rotate x-axis labels
                        fig.update_xaxes(tickangle=45)
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Show interpretation guide
                        st.markdown("---")
                        st.subheader("ğŸ“– ç›¸é—œä¿‚æ•¸è§£è®€")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.markdown("**ğŸ”´ å¼·è² ç›¸é—œ**: -1.0 ~ -0.7")
                            st.caption("ä¸€å€‹è®Šæ•¸å¢åŠ æ™‚ï¼Œå¦ä¸€å€‹æ˜é¡¯æ¸›å°‘")
                        with col2:
                            st.markdown("**âšª ç„¡ç›¸é—œ**: -0.3 ~ 0.3")
                            st.caption("å…©è®Šæ•¸ä¹‹é–“ç„¡æ˜é¡¯ç·šæ€§é—œä¿‚")
                        with col3:
                            st.markdown("**ğŸ”µ å¼·æ­£ç›¸é—œ**: 0.7 ~ 1.0")
                            st.caption("ä¸€å€‹è®Šæ•¸å¢åŠ æ™‚ï¼Œå¦ä¸€å€‹ä¹Ÿå¢åŠ ")
                        
                        # Highlight strong correlations
                        st.markdown("---")
                        st.subheader("ğŸ¯ é¡¯è‘—ç›¸é—œæ€§ï¼ˆ|r| > 0.7ï¼‰")
                        
                        strong_corr = []
                        for i in range(len(corr_matrix)):
                            for j in range(i+1, len(corr_matrix)):
                                corr_val = corr_matrix.iloc[i, j]
                                if abs(corr_val) > 0.7:
                                    var1 = corr_matrix.index[i]
                                    var2 = corr_matrix.columns[j]
                                    strong_corr.append({
                                        'è®Šæ•¸ 1': var1,
                                        'è®Šæ•¸ 2': var2,
                                        'ç›¸é—œä¿‚æ•¸': f"{corr_val:.3f}",
                                        'é¡å‹': 'æ­£ç›¸é—œ ğŸ”µ' if corr_val > 0 else 'è² ç›¸é—œ ğŸ”´'
                                    })
                        
                        if strong_corr:
                            import pandas as pd
                            st.dataframe(pd.DataFrame(strong_corr), use_container_width=True)
                        else:
                            st.info("æ²’æœ‰ç™¼ç¾å¼·ç›¸é—œæ€§ï¼ˆ|r| > 0.7ï¼‰çš„è®Šæ•¸å°")
                    
                    except Exception as e:
                        st.error(f"è¨ˆç®—ç›¸é—œæ€§å¤±æ•—: {str(e)}")
                        st.exception(e)
                else:
                    st.warning("è«‹è‡³å°‘é¸æ“‡ 2 å€‹è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")
    
    with tab6:
        st.header("ğŸ¯ è³‡æ–™å“è³ªå„€è¡¨æ¿")
        
        if 'df_parsed' in st.session_state:
            df = st.session_state.get('df_clean', st.session_state['df_parsed'])
            
            # Show data status indicator
            if 'df_clean' in st.session_state:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            # Overall quality metrics
            st.subheader("ğŸ“ˆ æ•´é«”è³‡æ–™å“è³ª")
            
            total_rows = len(df)
            total_cols = len(df.columns)
            numeric_cols = get_analysis_numeric_cols(df)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{total_rows:,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{total_cols}")
            with col3:
                st.metric("æ•¸å€¼æ¬„ä½", f"{len(numeric_cols)}")
            with col4:
                if 'timestamp' in df.columns:
                    time_span = df['timestamp'].max() - df['timestamp'].min()
                    st.metric("æ™‚é–“è·¨åº¦", str(time_span))
            
            # Missing data analysis
            st.markdown("---")
            st.subheader("ğŸ” ç¼ºå¤±å€¼åˆ†æ")
            
            # Columns to exclude from missing value analysis (Date/Time related)
            exclude_missing_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
            
            missing_data = []
            for col in df.columns:
                # Skip Date/Time columns
                if col in exclude_missing_cols:
                    continue
                null_count = df[col].null_count()
                if null_count > 0:
                    null_pct = (null_count / total_rows) * 100
                    missing_data.append({
                        'æ¬„ä½åç¨±': col,
                        'ç¼ºå¤±æ•¸é‡': null_count,
                        'ç¼ºå¤±æ¯”ä¾‹': f"{null_pct:.2f}%",
                        'åš´é‡ç¨‹åº¦': 'ğŸ”´ é«˜' if null_pct > 30 else ('ğŸŸ¡ ä¸­' if null_pct > 10 else 'ğŸŸ¢ ä½')
                    })
            
            if missing_data:
                import pandas as pd
                missing_df = pd.DataFrame(missing_data).sort_values('ç¼ºå¤±æ•¸é‡', ascending=False)
                st.dataframe(missing_df, use_container_width=True)
                
                # Visualize missing data
                import plotly.express as px
                fig = px.bar(
                    missing_df.head(10),
                    x='æ¬„ä½åç¨±',
                    y='ç¼ºå¤±æ•¸é‡',
                    title='å‰ 10 å€‹ç¼ºå¤±å€¼æœ€å¤šçš„æ¬„ä½',
                    labels={'ç¼ºå¤±æ•¸é‡': 'ç¼ºå¤±æ•¸é‡', 'æ¬„ä½åç¨±': 'æ¬„ä½'}
                )
                fig.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.success("âœ… æ²’æœ‰ç¼ºå¤±å€¼ï¼")
            
            # Frozen data detection (only if cleaned)
            if 'df_clean' in st.session_state:
                st.markdown("---")
                st.subheader("â„ï¸ å‡çµè³‡æ–™åµæ¸¬")
                
                frozen_cols = [col for col in df.columns if '_frozen' in col]
                
                if frozen_cols:
                    frozen_summary = []
                    for col in frozen_cols:
                        original_col = col.replace('_frozen', '')
                        frozen_count = df[col].sum()
                        if frozen_count > 0:
                            frozen_pct = (frozen_count / total_rows) * 100
                            frozen_summary.append({
                                'æ„Ÿæ¸¬å™¨': original_col,
                                'å‡çµé»æ•¸': frozen_count,
                                'å‡çµæ¯”ä¾‹': f"{frozen_pct:.2f}%",
                                'ç‹€æ…‹': 'ğŸ”´ è­¦å‘Š' if frozen_pct > 5 else 'ğŸŸ¡ æ³¨æ„'
                            })
                    
                    if frozen_summary:
                        import pandas as pd
                        frozen_df = pd.DataFrame(frozen_summary).sort_values('å‡çµé»æ•¸', ascending=False)
                        st.dataframe(frozen_df, use_container_width=True)
                        
                        st.warning("âš ï¸ å‡çµè³‡æ–™å¯èƒ½è¡¨ç¤ºæ„Ÿæ¸¬å™¨æ•…éšœæˆ–æ•¸æ“šå‚³è¼¸å•é¡Œ")
                    else:
                        st.success("âœ… æ²’æœ‰åµæ¸¬åˆ°å‡çµè³‡æ–™")
                else:
                    st.info("è³‡æ–™ä¸­ç„¡å‡çµæ¨™è¨˜æ¬„ä½")
            else:
                st.info("å°šæœªåŸ·è¡Œå‡çµè³‡æ–™åµæ¸¬ï¼ˆéœ€å…ˆæ¸…æ´—è³‡æ–™ï¼‰")
            
            # Physics Validation Status Section
            st.markdown("---")
            st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰ç‹€æ…‹")
            
            validation_cols = st.columns(3)
            
            with validation_cols[0]:
                st.markdown("**ğŸ“Š ç©©æ…‹æª¢æ¸¬**")
                if 'is_steady_state' in df.columns:
                    steady_count = df['is_steady_state'].sum()
                    total_count = len(df)
                    steady_pct = (steady_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç©©æ…‹è³‡æ–™", f"{steady_count:,} ({steady_pct:.1f}%)")
                    
                    # Small bar chart
                    steady_data = {'ç‹€æ…‹': ['ç©©æ…‹', 'éç©©æ…‹'], 'æ•¸é‡': [steady_count, total_count - steady_count]}
                    import pandas as pd
                    st.bar_chart(pd.DataFrame(steady_data).set_index('ç‹€æ…‹'))
                else:
                    st.caption("æœªåŸ·è¡Œç©©æ…‹æª¢æ¸¬")
            
            with validation_cols[1]:
                st.markdown("**ğŸŒ¡ï¸ ç†±å¹³è¡¡é©—è­‰**")
                if 'heat_balance_invalid' in df.columns:
                    invalid_count = df['heat_balance_invalid'].sum()
                    total_count = len(df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    if invalid_pct > 20:
                        st.error("ğŸ”´ ç•°å¸¸æ¯”ä¾‹éé«˜")
                    elif invalid_pct > 10:
                        st.warning("ğŸŸ¡ ç•°å¸¸æ¯”ä¾‹ä¸­ç­‰")
                    else:
                        st.success("ğŸŸ¢ ç•°å¸¸æ¯”ä¾‹æ­£å¸¸")
                else:
                    st.caption("æœªåŸ·è¡Œç†±å¹³è¡¡é©—è­‰")
            
            with validation_cols[2]:
                st.markdown("**âš¡ è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥**")
                if 'affinity_law_invalid' in df.columns:
                    invalid_count = df['affinity_law_invalid'].sum()
                    total_count = len(df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    # Show affinity ratio distribution if available
                    if 'affinity_ratio' in df.columns:
                        ratio_data = df['affinity_ratio'].drop_nulls()
                        if len(ratio_data) > 0:
                            st.caption(f"æ¯”ç‡ç¯„åœ: {ratio_data.min():.4f} ~ {ratio_data.max():.4f}")
                else:
                    st.caption("æœªåŸ·è¡Œè¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥")
            
            # Data completeness timeline
            if 'timestamp' in df.columns and numeric_cols:
                st.markdown("---")
                st.subheader("ğŸ“… è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸")
                
                # Select a representative column to check completeness
                sample_col = st.selectbox(
                    "é¸æ“‡æ¬„ä½æª¢è¦–å®Œæ•´æ€§",
                    numeric_cols
                )
                
                if sample_col:
                    # Create a binary completeness indicator
                    timeline_df = df.select(['timestamp', sample_col]).to_pandas()
                    timeline_df['å®Œæ•´æ€§'] = (~timeline_df[sample_col].isna()).astype(int)
                    timeline_df = timeline_df.set_index('timestamp')
                    
                    import plotly.graph_objects as go
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=timeline_df.index,
                        y=timeline_df['å®Œæ•´æ€§'],
                        mode='lines',
                        fill='tozeroy',
                        name='è³‡æ–™å­˜åœ¨',
                        line=dict(color='green')
                    ))
                    
                    fig.update_layout(
                        title=f"{sample_col} è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸",
                        xaxis_title="æ™‚é–“",
                        yaxis_title="è³‡æ–™å­˜åœ¨ (1=æœ‰, 0=ç„¡)",
                        height=300,
                        yaxis=dict(tickvals=[0, 1], ticktext=['ç¼ºå¤±', 'å­˜åœ¨'])
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
            
            # Data quality score
            st.markdown("---")
            st.subheader("â­ æ•´é«”å“è³ªè©•åˆ†")
            
            # Calculate quality score (0-100)
            quality_score = 100
            
            # Deduct points for missing data
            if missing_data:
                avg_missing_pct = sum([float(d['ç¼ºå¤±æ¯”ä¾‹'].strip('%')) for d in missing_data]) / len(df.columns)
                quality_score -= min(avg_missing_pct, 30)
            
            # Deduct points for frozen data (only if cleaned)
            if 'df_clean' in st.session_state:
                frozen_cols = [col for col in df.columns if '_frozen' in col]
                if frozen_cols:
                    frozen_count = sum([df[col].sum() for col in frozen_cols])
                    frozen_pct = (frozen_count / (total_rows * len(frozen_cols))) * 100 if frozen_cols else 0
                    quality_score -= min(frozen_pct, 20)
            
            quality_score = max(0, quality_score)
            
            # Display score with color coding
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                st.metric("è³‡æ–™å“è³ªè©•åˆ†", f"{quality_score:.1f}/100")
            
            with col2:
                if quality_score >= 90:
                    st.success("ğŸŸ¢ å„ªç§€")
                elif quality_score >= 75:
                    st.info("ğŸ”µ è‰¯å¥½")
                elif quality_score >= 60:
                    st.warning("ğŸŸ¡ å°šå¯")
                else:
                    st.error("ğŸ”´ éœ€æ”¹å–„")
            
            with col3:
                # Progress bar
                st.progress(quality_score / 100)
            
            # Recommendations
            if quality_score < 90:
                st.markdown("---")
                st.subheader("ğŸ’¡ æ”¹å–„å»ºè­°")
                
                if missing_data and len(missing_data) > 0:
                    st.markdown("- æª¢æŸ¥ç¼ºå¤±æ¯”ä¾‹ > 10% çš„æ¬„ä½ï¼Œè€ƒæ…®è£œå€¼æˆ–ç§»é™¤")
                
                if 'df_clean' in st.session_state:
                    frozen_cols = [col for col in df.columns if '_frozen' in col]
                    if frozen_cols:
                        frozen_count = sum([df[col].sum() for col in frozen_cols])
                        if frozen_count > 0:
                            st.markdown("- æª¢æŸ¥å‡çµè³‡æ–™çš„æ„Ÿæ¸¬å™¨ï¼Œå¯èƒ½éœ€è¦ç¶­è­·")
                
                st.markdown("- ç¢ºèªè³‡æ–™æ”¶é›†é »ç‡èˆ‡é æœŸä¸€è‡´")
                st.markdown("- è€ƒæ…®é€²è¡Œç•°å¸¸å€¼åµæ¸¬èˆ‡è™•ç†")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")

    with tab7:
        st.header("åŒ¯å‡ºè³‡æ–™")
        
        if 'df_parsed' in st.session_state or 'df_clean' in st.session_state:
            
            export_type = st.radio(
                "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
                ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"]
            )
            
            df_to_export = None
            if export_type == "è§£æå¾Œè³‡æ–™" and 'df_parsed' in st.session_state:
                df_to_export = st.session_state['df_parsed']
            elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and 'df_clean' in st.session_state:
                df_to_export = st.session_state['df_clean']
            
            if df_to_export is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    # CSV export
                    csv_data = df_to_export.write_csv()
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ CSV",
                        data=csv_data,
                        file_name=f"hvac_etl_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Parquet export (more efficient)
                    # Use BytesIO buffer since write_parquet needs a file
                    from io import BytesIO
                    buffer = BytesIO()
                    df_to_export.write_parquet(buffer)
                    parquet_data = buffer.getvalue()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ Parquet",
                        data=parquet_data,
                        file_name=f"hvac_etl_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
                        mime="application/octet-stream"
                    )
                
                st.info("ğŸ’¡ Parquet æ ¼å¼è¼ƒå°ä¸”æ•ˆèƒ½æ›´å¥½ï¼Œé©åˆå¤§å‹è³‡æ–™é›†")
            else:
                st.warning("è«‹å…ˆæ¸…æ´—è³‡æ–™æˆ–é¸æ“‡è§£æå¾Œè³‡æ–™åŒ¯å‡º")
        else:
            st.info("è«‹å…ˆåœ¨ã€Œè§£æè³‡æ–™ã€åˆ†é è§£ææª”æ¡ˆ")

elif processing_mode == "æ‰¹æ¬¡è™•ç†ï¼ˆæ•´å€‹è³‡æ–™å¤¾ï¼‰" and selected_files:
    st.header("ğŸ“¦ æ‰¹æ¬¡è™•ç†æ¨¡å¼")
    
    st.info(f"æº–å‚™è™•ç† {len(selected_files)} å€‹æª”æ¡ˆ")
    
    # Show file list preview
    with st.expander("æŸ¥çœ‹æª”æ¡ˆæ¸…å–®"):
        if len(selected_files) <= 10:
            for f in selected_files:
                st.text(f"â€¢ {f}")
        else:
            st.text("å‰ 5 å€‹æª”æ¡ˆ:")
            for f in selected_files[:5]:
                st.text(f"  â€¢ {f}")
            st.text(f"  ... ({len(selected_files) - 10} å€‹æª”æ¡ˆ)")
            st.text("å¾Œ 5 å€‹æª”æ¡ˆ:")
            for f in selected_files[-5:]:
                st.text(f"  â€¢ {f}")
    
    # Processing options
    st.subheader("âš™ï¸ è™•ç†é¸é …")
    
    col1, col2 = st.columns(2)
    with col1:
        batch_resample = st.selectbox("é‡æ¡æ¨£é–“éš”", ["5m", "10m", "15m", "30m", "1h"], index=0)
    with col2:
        auto_clean = st.checkbox("è‡ªå‹•æ¸…æ´—è³‡æ–™", value=True)
    
    # Physics-based validation options (only show if auto_clean is enabled)
    if auto_clean:
        st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰é¸é …")
        col1, col2, col3 = st.columns(3)
        with col1:
            batch_apply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False, 
                help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™")
        with col2:
            batch_apply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
                help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚")
        with col3:
            batch_apply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
                help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚")
        
        batch_filter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
            help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—")
    
    # Start batch processing
    if st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡è™•ç†", type="primary"):
        try:
            from etl.batch_processor import BatchProcessor
            from etl.cleaner import DataCleaner
            
            # Prepare file paths
            file_paths = [str(data_dir / f) for f in selected_files]
            
            # Create processor
            processor = BatchProcessor(resample_interval=batch_resample)
            
            # Progress bar
            status_text = st.empty()
            status_text.text("æ­£åœ¨è™•ç†æª”æ¡ˆ...")
            
            if auto_clean:
                # Process files without cleaning first
                with st.spinner("æ­£åœ¨è§£ææª”æ¡ˆ..."):
                    merged_df = processor.process_files(file_paths, clean=False)
                
                # Apply advanced cleaning with physics validation
                status_text.text("æ­£åœ¨åŸ·è¡Œè³‡æ–™æ¸…æ´—èˆ‡é©—è­‰...")
                with st.spinner("æ¸…æ´—èˆ‡é©—è­‰ä¸­..."):
                    cleaner = DataCleaner(resample_interval=batch_resample)
                    merged_df = cleaner.clean_data(
                        merged_df,
                        apply_steady_state=batch_apply_steady_state if auto_clean else False,
                        apply_heat_balance=batch_apply_heat_balance if auto_clean else False,
                        apply_affinity_laws=batch_apply_affinity if auto_clean else False,
                        filter_invalid=batch_filter_invalid if auto_clean else False
                    )
                
                # Store validation results
                st.session_state['batch_validation_results'] = {
                    'steady_state': batch_apply_steady_state if auto_clean else False,
                    'heat_balance': batch_apply_heat_balance if auto_clean else False,
                    'affinity_laws': batch_apply_affinity if auto_clean else False,
                    'filter_invalid': batch_filter_invalid if auto_clean else False
                }
            else:
                with st.spinner("è™•ç†ä¸­..."):
                    merged_df = processor.process_files(file_paths, clean=False)
                
                st.session_state['batch_validation_results'] = None
            
            status_text.text("è™•ç†å®Œæˆ!")
            
            # Store in session state
            if auto_clean:
                st.session_state['df_clean'] = merged_df
                st.session_state['df_parsed'] = merged_df
            else:
                st.session_state['df_parsed'] = merged_df
            
            # Mark batch processing as complete
            st.session_state['batch_processing_complete'] = True
            st.session_state['batch_file_count'] = len(selected_files)
            st.session_state['batch_auto_clean'] = auto_clean
            
        except Exception as e:
            st.error(f"âŒ æ‰¹æ¬¡è™•ç†éŒ¯èª¤: {str(e)}")
            st.exception(e)
    
    # Show analysis tabs if batch processing is complete (persists across re-renders)
    if st.session_state.get('batch_processing_complete', False):
        # Get data from session state
        if 'df_clean' in st.session_state:
            merged_df = st.session_state['df_clean']
        elif 'df_parsed' in st.session_state:
            merged_df = st.session_state['df_parsed']
        else:
            st.error("è³‡æ–™éºå¤±ï¼Œè«‹é‡æ–°åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
            if st.button("é‡ç½®"):
                st.session_state['batch_processing_complete'] = False
                st.rerun()
            st.stop()
        
        batch_file_count = st.session_state.get('batch_file_count', 0)
        auto_clean = st.session_state.get('batch_auto_clean', True)
            
        # Show summary
        st.success(f"âœ… æˆåŠŸè™•ç† {batch_file_count} å€‹æª”æ¡ˆ")
        
        # Show validation results if any were applied
        validation_results = st.session_state.get('batch_validation_results')
        if validation_results:
            result_cols = []
            if validation_results.get('steady_state'):
                steady_count = merged_df['is_steady_state'].sum() if 'is_steady_state' in merged_df.columns else 0
                result_cols.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
            if validation_results.get('heat_balance'):
                invalid_count = merged_df['heat_balance_invalid'].sum() if 'heat_balance_invalid' in merged_df.columns else 0
                result_cols.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
            if validation_results.get('affinity_laws'):
                invalid_count = merged_df['affinity_law_invalid'].sum() if 'affinity_law_invalid' in merged_df.columns else 0
                result_cols.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
            
            if result_cols:
                st.info(" | ".join(result_cols))
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ç¸½åˆ—æ•¸", f"{len(merged_df):,}")
        with col2:
            st.metric("ç¸½æ¬„ä½æ•¸", f"{len(merged_df.columns):,}")
        with col3:
            if 'timestamp' in merged_df.columns:
                time_range = merged_df['timestamp'].max() - merged_df['timestamp'].min()
                st.metric("æ™‚é–“ç¯„åœ", str(time_range))
        
        st.markdown("---")
        
        # Feature Mapping Configuration Section (V2 - Dynamic Categories)
        st.header("ğŸ—ºï¸ ç‰¹å¾µæ˜ å°„é…ç½® (Feature Mapping)")
        st.caption("å°‡è³‡æ–™æ¬„ä½å°æ‡‰åˆ°æ¨¡å‹ç‰¹å¾µé¡åˆ¥ï¼Œæ”¯æ´10+ç¨®é¡å‹èˆ‡è‡ªå®šç¾©é¡åˆ¥")
        
        # Get available columns (exclude timestamp)
        available_cols = [c for c in merged_df.columns if c != 'timestamp']
        
        # Initialize session state for batch feature mapping if not exists
        if 'batch_feature_mapping' not in st.session_state:
            st.session_state.batch_feature_mapping = None
        
        # Configuration mode selection
        mapping_config_mode = st.radio(
            "é…ç½®æ–¹å¼",
            ["è‡ªå‹•è­˜åˆ¥ (Auto-detect)", "æ‰‹å‹•å°æ‡‰ (Manual Mapping)", "è¬ç”¨å­—å…ƒæ¨¡å¼ (Wildcard Pattern)"],
            horizontal=True,
            help="é¸æ“‡è‡ªå‹•æ ¹æ“šæ¬„ä½åç¨±è­˜åˆ¥ã€æ‰‹å‹•æŒ‡å®šæ¯å€‹æ¬„ä½çš„é¡åˆ¥ï¼Œæˆ–ä½¿ç”¨è¬ç”¨å­—å…ƒæ¨¡å¼å¿«é€ŸåŒ¹é…"
        )
        
        if mapping_config_mode == "è‡ªå‹•è­˜åˆ¥ (Auto-detect)":
            # Auto-create mapping from column names (V2 with all 10+ categories)
            if st.button("ğŸ¤– åŸ·è¡Œè‡ªå‹•è­˜åˆ¥", type="primary"):
                with st.spinner("æ­£åœ¨åˆ†ææ¬„ä½åç¨±..."):
                    auto_mapping = FeatureMapping.create_from_dataframe(available_cols)
                    st.session_state.batch_feature_mapping = auto_mapping
                    st.success(f"âœ… è‡ªå‹•è­˜åˆ¥å®Œæˆï¼è­˜åˆ¥åˆ° {len(auto_mapping.get_all_categories())} å€‹é¡åˆ¥")
        
        elif mapping_config_mode == "æ‰‹å‹•å°æ‡‰ (Manual Mapping)":
            # Manual mapping mode with HVAC physical system hierarchy
            st.info("è«‹åœ¨ä¸‹æ–¹ç‚ºæ¯å€‹æ¬„ä½é¸æ“‡é©ç•¶çš„ç‰¹å¾µé¡åˆ¥ã€‚é¡åˆ¥å·²æŒ‰ç‰©ç†ç³»çµ±åˆ†çµ„")
            
            # Group categories by parent system
            parent_systems = {
                "chilled_water_side": {"name": "å†°æ°´å´ç³»çµ±", "icon": "â„ï¸", "categories": []},
                "condenser_water_side": {"name": "å†·å»æ°´å´ç³»çµ±", "icon": "ğŸ”¥", "categories": []},
                "cooling_tower_system": {"name": "å†·å»æ°´å¡”ç³»çµ±", "icon": "ğŸ­", "categories": []},
                "environment": {"name": "ç’°å¢ƒåƒæ•¸", "icon": "ğŸŒ", "categories": []},
                "system_level": {"name": "ç³»çµ±å±¤ç´š", "icon": "âš¡", "categories": []},
            }
            
            # Sort categories into parent systems
            for cat_id, meta in STANDARD_CATEGORIES.items():
                parent = meta.get('parent_system', 'other')
                if parent in parent_systems:
                    parent_systems[parent]['categories'].append(cat_id)
            
            manual_selections = {}
            
            # Create expander for each parent system
            for system_id, system_info in parent_systems.items():
                if not system_info['categories']:
                    continue
                    
                with st.expander(f"{system_info['icon']} {system_info['name']} ({len(system_info['categories'])} é¡åˆ¥)", expanded=True):
                    # Create columns for this system
                    cols = st.columns(2)
                    col_idx = 0
                    
                    for cat_id in system_info['categories']:
                        if cat_id not in STANDARD_CATEGORIES:
                            continue
                            
                        meta = STANDARD_CATEGORIES[cat_id]
                        
                        # Auto-detect default columns based on patterns
                        defaults = []
                        patterns = meta.get('pattern', '').split(',')
                        for col in available_cols:
                            col_upper = col.upper()
                            # Check if any pattern matches
                            for pattern in patterns:
                                pattern = pattern.strip().replace('*', '')
                                if pattern and pattern in col_upper:
                                    if not any(exclude in col_upper for exclude in ['FROZEN', 'FLAG']):
                                        if col not in defaults:
                                            defaults.append(col)
                            # Also check for common patterns
                            if cat_id.upper() in col_upper.replace('_', ''):
                                if col not in defaults and not any(exclude in col_upper for exclude in ['FROZEN', 'FLAG']):
                                    defaults.append(col)
                        
                        with cols[col_idx % 2]:
                            st.markdown(f"**{meta['icon']} {meta['name']}**")
                            st.caption(f"{meta['description']} | å–®ä½: {meta['unit']}")
                            
                            manual_selections[cat_id] = st.multiselect(
                                f"é¸æ“‡{meta['name']}æ¬„ä½",
                                options=available_cols,
                                default=defaults,
                                key=f"manual_{cat_id}",
                                label_visibility="collapsed"
                            )
                        
                        col_idx += 1
            
            # Target variable section
            st.markdown("---")
            st.markdown("**ğŸ¯ ç›®æ¨™è®Šæ•¸ (Target Variable)**")
            
            col1, col2 = st.columns(2)
            with col1:
                # Target column selection
                target_candidates = []
                # 1. å„ªå…ˆæ‰¾ COP ç›¸é—œ
                target_candidates = [c for c in available_cols if 'COP' in c.upper()]
                # 2. å…¶æ¬¡æ‰¾ kW/RT æ•ˆç‡æŒ‡æ¨™
                if not target_candidates:
                    target_candidates = [c for c in available_cols if any(x in c.upper() for x in ['KW_RT', 'KW/RT', 'KW_PER_RT', 'EFFICIENCY'])]
                # 3. æœ€å¾Œæ‰¾ç¸½ç”¨é›»
                if not target_candidates:
                    target_candidates = [c for c in available_cols if 'TOTAL' in c.upper() and 'KW' in c.upper()]
                if not target_candidates:
                    target_candidates = [c for c in available_cols if c.upper().endswith('_KW')]
                if not target_candidates:
                    target_candidates = available_cols
                
                target_selection = st.selectbox(
                    "é¸æ“‡ç›®æ¨™æ¬„ä½",
                    options=available_cols,
                    index=available_cols.index(target_candidates[0]) if target_candidates else 0,
                    key="manual_target"
                )
            
            with col2:
                # Target metric type selection
                target_metric_type = st.selectbox(
                    "ç›®æ¨™é¡å‹",
                    options=["efficiency", "power"],
                    format_func=lambda x: "æ•ˆç‡æŒ‡æ¨™ (COP/kW/RT)" if x == "efficiency" else "åŠŸç‡ (kW)",
                    index=0 if any(x in target_selection.upper() for x in ['COP', 'EFFICIENCY', 'KW_RT', 'KW/RT']) else 1,
                    key="manual_target_metric",
                    help="é¸æ“‡ç›®æ¨™è®Šæ•¸æ˜¯æ•ˆç‡æŒ‡æ¨™(è¶Šå°è¶Šå¥½)é‚„æ˜¯åŠŸç‡(è¶Šå¤§è¶Šå·®)"
                )
            
            # Custom category addition
            st.markdown("---")
            with st.expander("â• æ–°å¢è‡ªå®šç¾©é¡åˆ¥ (Add Custom Category)"):
                st.caption("å¦‚æœéœ€è¦çš„é¡åˆ¥ä¸åœ¨ä¸Šæ–¹åˆ—è¡¨ä¸­ï¼Œå¯ä»¥åœ¨æ­¤æ–°å¢")
                
                custom_cat_id = st.text_input(
                    "é¡åˆ¥ä»£ç¢¼ (è‹±æ–‡ï¼Œå¦‚: custom_valve, backup_sensor)",
                    key="custom_cat_id"
                )
                custom_cat_name = st.text_input(
                    "é¡åˆ¥åç¨± (å¦‚: è‡ªå®šç¾©é–¥é–€, å‚™ç”¨æ„Ÿæ¸¬å™¨)",
                    key="custom_cat_name"
                )
                custom_cat_icon = st.selectbox(
                    "åœ–ç¤º",
                    options=["ğŸ“¦", "ğŸ”§", "ğŸ“¡", "âš™ï¸", "ğŸ”©", "ğŸ”—", "ğŸ“", "ğŸ·ï¸", "ğŸ”", "ğŸ“Š", "ğŸŒ¡ï¸", "ğŸ’§"],
                    key="custom_cat_icon"
                )
                custom_cat_unit = st.text_input(
                    "å–®ä½ (å¦‚: %, kPa, m/s, Â°C)",
                    key="custom_cat_unit"
                )
                custom_cat_description = st.text_input(
                    "æè¿°",
                    key="custom_cat_description"
                )
                custom_cat_cols = st.multiselect(
                    "é¸æ“‡æ¬„ä½",
                    options=available_cols,
                    key="custom_cat_cols"
                )
                
                if st.button("æ–°å¢è‡ªå®šç¾©é¡åˆ¥", type="secondary"):
                    if custom_cat_id and custom_cat_name and custom_cat_cols:
                        if 'custom_categories' not in st.session_state:
                            st.session_state.custom_categories = {}
                        
                        st.session_state.custom_categories[custom_cat_id] = {
                            'columns': custom_cat_cols,
                            'name': custom_cat_name,
                            'icon': custom_cat_icon,
                            'unit': custom_cat_unit,
                            'description': custom_cat_description
                        }
                        st.success(f"âœ… å·²æ–°å¢é¡åˆ¥: {custom_cat_name}")
                        st.rerun()
            
            # Save manual configuration
            if st.button("ğŸ’¾ å„²å­˜æ‰‹å‹•é…ç½®", type="primary"):
                manual_mapping = FeatureMapping(
                    # å†°æ°´å´ç³»çµ±
                    chiller_cols=manual_selections.get('chiller', []),
                    chw_pump_cols=manual_selections.get('chw_pump', []),
                    scp_pump_cols=manual_selections.get('scp_pump', []),
                    chw_temp_cols=manual_selections.get('chw_temp', []),
                    chw_pressure_cols=manual_selections.get('chw_pressure', []),
                    chw_flow_cols=manual_selections.get('chw_flow', []),
                    # å†·å»æ°´å´ç³»çµ±
                    cw_pump_cols=manual_selections.get('cw_pump', []),
                    cw_temp_cols=manual_selections.get('cw_temp', []),
                    cw_pressure_cols=manual_selections.get('cw_pressure', []),
                    cw_flow_cols=manual_selections.get('cw_flow', []),
                    # å†·å»æ°´å¡”
                    cooling_tower_cols=manual_selections.get('cooling_tower', []),
                    # ç’°å¢ƒ
                    environment_cols=manual_selections.get('environment', []),
                    # ç³»çµ±å±¤ç´š
                    system_level_cols=manual_selections.get('system_level', []),
                    # ç›®æ¨™
                    target_col=target_selection,
                    target_metric=target_metric_type
                )
                
                # Add custom categories if any
                if 'custom_categories' in st.session_state:
                    for cat_id, cat_data in st.session_state.custom_categories.items():
                        manual_mapping.add_custom_category(
                            category_id=cat_id,
                            columns=cat_data['columns'],
                            name=cat_data['name'],
                            icon=cat_data['icon'],
                            unit=cat_data['unit'],
                            description=cat_data.get('description', '')
                        )
                
                st.session_state.batch_feature_mapping = manual_mapping
                st.success(f"âœ… æ‰‹å‹•é…ç½®å·²å„²å­˜ï¼å…± {len(manual_mapping.get_all_feature_cols())} å€‹ç‰¹å¾µï¼Œç›®æ¨™: {target_selection} ({target_metric_type})")
        
        elif mapping_config_mode == "è¬ç”¨å­—å…ƒæ¨¡å¼ (Wildcard Pattern)":
            # Wildcard pattern mode for quick matching with HVAC physical system hierarchy
            st.info("ğŸ¯ ä½¿ç”¨è¬ç”¨å­—å…ƒæ¨¡å¼å¿«é€ŸåŒ¹é…æ¬„ä½ã€‚é¡åˆ¥å·²æŒ‰ç‰©ç†ç³»çµ±åˆ†çµ„")
            
            # Default wildcard patterns for 13 new categories
            default_patterns = {
                # å†°æ°´å´ç³»çµ±
                "chiller": "CH_*_RT|CHILLER*",
                "chw_pump": "CHP*VFD_OUT|CHWP*",
                "scp_pump": "SCP*VFD_OUT|SCP*",
                "chw_temp": "*CHW*TEMP*|*CHW*ST*|*CHW*RT*",
                "chw_pressure": "*CHW*PRESSURE*|*CHW*P*",
                "chw_flow": "*CHW*FLOW*|*CHW*LPM*",
                # å†·å»æ°´å´ç³»çµ±
                "cw_pump": "CWP*VFD_OUT|CWP*",
                "cw_temp": "*CW*TEMP*|*CW*ST*|*CW*RT*",
                "cw_pressure": "*CW*PRESSURE*|*CW*P*",
                "cw_flow": "*CW*FLOW*|*CW*LPM*",
                # å†·å»æ°´å¡”
                "cooling_tower": "CT_*_VFD_OUT|CT*",
                # ç’°å¢ƒ
                "environment": "*OAT*|*OAH*|*WBT*|*OUTDOOR*",
                # ç³»çµ±å±¤ç´š
                "system_level": "*TOTAL*|*COP*|*KW*RT*",
            }
            
            # Group categories by parent system
            parent_systems = {
                "chilled_water_side": {"name": "å†°æ°´å´ç³»çµ±", "icon": "â„ï¸", "categories": []},
                "condenser_water_side": {"name": "å†·å»æ°´å´ç³»çµ±", "icon": "ğŸ”¥", "categories": []},
                "cooling_tower_system": {"name": "å†·å»æ°´å¡”ç³»çµ±", "icon": "ğŸ­", "categories": []},
                "environment": {"name": "ç’°å¢ƒåƒæ•¸", "icon": "ğŸŒ", "categories": []},
                "system_level": {"name": "ç³»çµ±å±¤ç´š", "icon": "âš¡", "categories": []},
            }
            
            # Sort categories into parent systems
            for cat_id, meta in STANDARD_CATEGORIES.items():
                parent = meta.get('parent_system', 'other')
                if parent in parent_systems:
                    parent_systems[parent]['categories'].append(cat_id)
            
            st.caption("æ”¯æ´èªæ³•ï¼š`*` åŒ¹é…ä»»æ„å­—å…ƒï¼Œ`?` åŒ¹é…å–®ä¸€å­—å…ƒã€‚å¤šå€‹æ¨¡å¼å¯ç”¨ `|` åˆ†éš”")
            
            wildcard_patterns = {}
            
            # Create expander for each parent system
            for system_id, system_info in parent_systems.items():
                if not system_info['categories']:
                    continue
                    
                with st.expander(f"{system_info['icon']} {system_info['name']}", expanded=True):
                    cols = st.columns(2)
                    col_idx = 0
                    
                    for cat_id in system_info['categories']:
                        if cat_id not in STANDARD_CATEGORIES:
                            continue
                            
                        meta = STANDARD_CATEGORIES[cat_id]
                        
                        with cols[col_idx % 2]:
                            pattern = st.text_input(
                                f"{meta['icon']} {meta['name']}",
                                value=default_patterns.get(cat_id, "*"),
                                key=f"wildcard_{cat_id}",
                                help=f"{meta['description']} ({meta['unit']})"
                            )
                            wildcard_patterns[cat_id] = pattern
                        
                        col_idx += 1
            
            # Target section
            st.markdown("---")
            st.markdown("**ğŸ¯ ç›®æ¨™è®Šæ•¸è¨­å®š**")
            
            col1, col2 = st.columns(2)
            with col1:
                target_pattern = st.text_input(
                    "ç›®æ¨™æ¬„ä½æ¨¡å¼",
                    value="*TOTAL*KW|*SYS*_KW|*COP*",
                    help="åŒ¹é…ç›®æ¨™æ¬„ä½ï¼Œå¤šå€‹æ¨¡å¼å¯ç”¨ | åˆ†éš”"
                )
            
            with col2:
                wildcard_target_metric = st.selectbox(
                    "ç›®æ¨™é¡å‹",
                    options=["efficiency", "power"],
                    format_func=lambda x: "æ•ˆç‡æŒ‡æ¨™ (COP/kW/RT)" if x == "efficiency" else "åŠŸç‡ (kW)",
                    index=0,
                    key="wildcard_target_metric",
                    help="é¸æ“‡ç›®æ¨™è®Šæ•¸æ˜¯æ•ˆç‡æŒ‡æ¨™(è¶Šå°è¶Šå¥½)é‚„æ˜¯åŠŸç‡(è¶Šå¤§è¶Šå·®)"
                )
            
            # Preview matches
            if st.button("ğŸ” é è¦½åŒ¹é…çµæœ", type="secondary"):
                st.markdown("**ğŸ“‹ é è¦½åŒ¹é…çµæœï¼š**")
                
                preview_cols = st.columns(3)
                preview_col_idx = 0
                total_matched = 0
                
                # Group preview by parent system
                for system_id, system_info in parent_systems.items():
                    if not system_info['categories']:
                        continue
                    
                    system_matched = 0
                    system_details = []
                    
                    for cat_id in system_info['categories']:
                        if cat_id not in STANDARD_CATEGORIES:
                            continue
                            
                        pattern_str = wildcard_patterns.get(cat_id, "")
                        # Split by | to support multiple patterns
                        patterns = [p.strip() for p in pattern_str.split("|") if p.strip()]
                        
                        # Match columns
                        matched = []
                        for pattern in patterns:
                            matched.extend(FeatureMapping.match_columns_by_pattern(available_cols, pattern))
                        # Remove duplicates
                        matched = list(dict.fromkeys(matched))  # Preserves order
                        
                        if matched:
                            total_matched += len(matched)
                            system_matched += len(matched)
                            meta = STANDARD_CATEGORIES[cat_id]
                            system_details.append(f"{meta['icon']} {meta['name'].split('(')[0].strip()}: {len(matched)}")
                    
                    # Show system summary
                    if system_details and preview_col_idx < 6:
                        with preview_cols[preview_col_idx % 3]:
                            st.markdown(f"**{system_info['icon']} {system_info['name']}**: {system_matched} å€‹")
                            st.caption(" | ".join(system_details[:3]))
                        preview_col_idx += 1
                
                # Target preview
                target_patterns = [p.strip() for p in target_pattern.split("|") if p.strip()]
                target_matched = []
                for pattern in target_patterns:
                    target_matched.extend(FeatureMapping.match_columns_by_pattern(available_cols, pattern))
                target_matched = list(dict.fromkeys(target_matched))
                
                if preview_col_idx < 6:
                    with preview_cols[preview_col_idx % 3]:
                        st.markdown(f"**ğŸ¯ ç›®æ¨™è®Šæ•¸**: {len(target_matched)} å€‹")
                        st.caption(", ".join(target_matched[:3]) if target_matched else "ç„¡åŒ¹é…")
                
                st.success(f"âœ… å…±åŒ¹é…åˆ° {total_matched} å€‹ç‰¹å¾µæ¬„ä½ï¼Œç›®æ¨™: {target_matched[0] if target_matched else 'ç„¡'}")
            
            # Apply wildcard patterns
            if st.button("âœ… å¥—ç”¨è¬ç”¨å­—å…ƒæ¨¡å¼", type="primary"):
                with st.spinner("æ­£åœ¨å¥—ç”¨è¬ç”¨å­—å…ƒæ¨¡å¼..."):
                    # Filter out empty patterns
                    valid_patterns = {k: v for k, v in wildcard_patterns.items() if v.strip()}
                    
                    wildcard_mapping = FeatureMapping.create_from_wildcard_patterns(
                        df_columns=available_cols,
                        wildcard_patterns=valid_patterns,
                        target_pattern=target_pattern
                    )
                    
                    # Set target metric type
                    wildcard_mapping.target_metric = wildcard_target_metric
                    
                    st.session_state.batch_feature_mapping = wildcard_mapping
                    st.success(f"âœ… è¬ç”¨å­—å…ƒé…ç½®å®Œæˆï¼å…± {len(wildcard_mapping.get_all_feature_cols())} å€‹ç‰¹å¾µï¼Œç›®æ¨™é¡å‹: {wildcard_target_metric}")
        
        # Display current mapping (works for all modes)
        if st.session_state.batch_feature_mapping:
            with st.expander("ğŸ“‹ æŸ¥çœ‹/ç·¨è¼¯ç•¶å‰æ˜ å°„", expanded=True):
                mapping = st.session_state.batch_feature_mapping
                
                # Get all categories dynamically
                all_categories = mapping.get_all_categories()
                
                # Summary row with total features and target info
                total_features = len(mapping.get_all_feature_cols())
                target_info = f"{mapping.target_col} ({mapping.target_metric})" if hasattr(mapping, 'target_metric') else mapping.target_col
                
                summary_cols = st.columns(3)
                with summary_cols[0]:
                    st.metric("ç¸½ç‰¹å¾µæ•¸", total_features)
                with summary_cols[1]:
                    st.metric("é¡åˆ¥æ•¸", len([c for c in all_categories.values() if c]))
                with summary_cols[2]:
                    st.metric("ç›®æ¨™è®Šæ•¸", mapping.target_col.split('_')[-1] if '_' in mapping.target_col else mapping.target_col)
                
                # Show target metric type if available
                if hasattr(mapping, 'target_metric'):
                    target_type_label = "æ•ˆç‡æŒ‡æ¨™ ğŸ“ˆ" if mapping.target_metric == "efficiency" else "åŠŸç‡ âš¡"
                    st.caption(f"ğŸ¯ ç›®æ¨™é¡å‹: {target_type_label} | {mapping.target_col}")
                
                st.markdown("---")
                st.markdown("**è©³ç´°å°æ‡‰ï¼ˆæŒ‰ç‰©ç†ç³»çµ±åˆ†çµ„ï¼‰ï¼š**")
                
                # Group categories by parent system
                parent_systems_display = {
                    "chilled_water_side": {"name": "å†°æ°´å´ç³»çµ±", "icon": "â„ï¸"},
                    "condenser_water_side": {"name": "å†·å»æ°´å´ç³»çµ±", "icon": "ğŸ”¥"},
                    "cooling_tower_system": {"name": "å†·å»æ°´å¡”ç³»çµ±", "icon": "ğŸ­"},
                    "environment": {"name": "ç’°å¢ƒåƒæ•¸", "icon": "ğŸŒ"},
                    "system_level": {"name": "ç³»çµ±å±¤ç´š", "icon": "âš¡"},
                    "other": {"name": "å…¶ä»–é¡åˆ¥", "icon": "ğŸ“¦"},
                }
                
                # Organize categories by parent system
                categories_by_system = {k: [] for k in parent_systems_display.keys()}
                for cat_id, cols in all_categories.items():
                    if cols:
                        parent = STANDARD_CATEGORIES.get(cat_id, {}).get('parent_system', 'other')
                        if parent not in categories_by_system:
                            parent = 'other'
                        categories_by_system[parent].append((cat_id, cols))
                
                # Display by system groups
                for system_id, system_info in parent_systems_display.items():
                    cat_list = categories_by_system.get(system_id, [])
                    if not cat_list:
                        continue
                    
                    with st.expander(f"{system_info['icon']} {system_info['name']} ({len(cat_list)} é¡åˆ¥)", expanded=False):
                        # Two-column layout for categories
                        display_cols = st.columns(2)
                        for idx, (cat_id, cols) in enumerate(cat_list):
                            info = mapping.get_category_info(cat_id)
                            with display_cols[idx % 2]:
                                st.markdown(f"**{info['icon']} {info['name']}** ({len(cols)} å€‹)")
                                st.caption(f"â€¢ {', '.join(cols[:5])}{'...' if len(cols) > 5 else ''}")
                
                # Validation
                validation = mapping.validate_against_dataframe(merged_df.columns)
                if validation['missing_required']:
                    st.error(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {validation['missing_required']}")
                elif validation['missing_optional']:
                    st.warning(f"âš ï¸ ç¼ºå°‘å¯é¸æ¬„ä½: {validation['missing_optional']}")
                else:
                    st.success("âœ… æ‰€æœ‰æ˜ å°„æ¬„ä½éƒ½å­˜åœ¨æ–¼è³‡æ–™ä¸­")
                
                # Export option
                col1, col2 = st.columns([1, 3])
                with col1:
                    if st.button("ğŸ“¥ åŒ¯å‡º JSON"):
                        json_str = json.dumps(mapping.to_dict(), indent=2, ensure_ascii=False)
                        st.download_button(
                            label="ä¸‹è¼‰",
                            data=json_str,
                            file_name="feature_mapping.json",
                            mime="application/json"
                        )
        
        st.markdown("---")
        st.info("ğŸ“Š **è³‡æ–™å·²è¼‰å…¥ï¼** è«‹ä½¿ç”¨ä¸‹æ–¹æ¨™ç±¤é åˆ†æåˆä½µå¾Œçš„è³‡æ–™")
        
        # Analysis tabs
        batch_tab1, batch_tab2, batch_tab3, batch_tab4, batch_tab5, batch_tab6, batch_tab7 = st.tabs([
            "ğŸ“‹ è³‡æ–™é è¦½",
            "ğŸ§¹ æ¸…æ´—è³‡æ–™",
            "ğŸ“Š çµ±è¨ˆè³‡è¨Š", 
            "ğŸ“ˆ æ™‚é–“åºåˆ—",
            "ğŸ”— é—œè¯çŸ©é™£",
            "ğŸ¯ è³‡æ–™å“è³ª",
            "ğŸ’¾ åŒ¯å‡º"
        ])
            
        with batch_tab1:
            st.subheader("åˆä½µå¾Œè³‡æ–™é è¦½")
            st.dataframe(merged_df.head(100).to_pandas(), use_container_width=True)
            st.caption(f"é¡¯ç¤ºå‰ 100 ç­†ï¼Œå…± {len(merged_df):,} ç­†è³‡æ–™")
        with batch_tab2:
            st.header("ğŸ§¹ è³‡æ–™æ¸…æ´—")
            
            # Check if we have parsed data to clean
            if 'df_parsed' in st.session_state:
                df_to_clean = st.session_state['df_parsed']
                
                # Cleaning options
                st.subheader("æ¸…æ´—é¸é …")
                
                # Basic options
                col1, col2 = st.columns(2)
                with col1:
                    batch_clean_resample = st.selectbox(
                        "é‡æ¡æ¨£é–“éš”",
                        ["5m", "10m", "15m", "30m", "1h"],
                        index=0,
                        key="batch_clean_resample"
                    )
                with col2:
                    batch_detect_frozen = st.checkbox("æª¢æ¸¬å‡çµè³‡æ–™", value=True, key="batch_detect_frozen")
                
                # Physics-based validation options
                st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰é¸é …")
                col1, col2, col3 = st.columns(3)
                with col1:
                    batch_reapply_steady_state = st.checkbox("ç©©æ…‹æª¢æ¸¬", value=False, 
                        help="åªä¿ç•™è² è¼‰è®ŠåŒ–å°æ–¼ 5% çš„ç©©æ…‹è³‡æ–™",
                        key="batch_reapply_steady")
                with col2:
                    batch_reapply_heat_balance = st.checkbox("ç†±å¹³è¡¡é©—è­‰", value=False,
                        help="é©—è­‰ Q = Flow Ã— Î”T é—œä¿‚",
                        key="batch_reapply_heat")
                with col3:
                    batch_reapply_affinity = st.checkbox("è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥", value=False,
                        help="é©—è­‰æ³µæµ¦ Power âˆ FrequencyÂ³ é—œä¿‚",
                        key="batch_reapply_affinity")
                
                # Filter options
                batch_refilter_invalid = st.checkbox("ç§»é™¤ç„¡æ•ˆè³‡æ–™", value=False,
                    help="ç§»é™¤æœªé€šéä¸Šè¿°é©—è­‰çš„è³‡æ–™åˆ—",
                    key="batch_refilter_invalid")
                
                if st.button("ğŸ§¹ é–‹å§‹æ¸…æ´—", type="primary", key="batch_clean_button"):
                    try:
                        with st.spinner("æ­£åœ¨æ¸…æ´—è³‡æ–™..."):
                            cleaner = DataCleaner(resample_interval=batch_clean_resample)
                            df_cleaned = cleaner.clean_data(
                                df_to_clean,
                                apply_steady_state=batch_reapply_steady_state,
                                apply_heat_balance=batch_reapply_heat_balance,
                                apply_affinity_laws=batch_reapply_affinity,
                                filter_invalid=batch_refilter_invalid
                            )
                        
                        st.success(f"âœ… æ¸…æ´—å®Œæˆï¼")
                        
                        # Show metrics
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("åŸå§‹åˆ—æ•¸", f"{len(df_to_clean):,}")
                        with col2:
                            st.metric("æ¸…æ´—å¾Œåˆ—æ•¸", f"{len(df_cleaned):,}")
                        with col3:
                            retention = len(df_cleaned) / len(df_to_clean) * 100 if len(df_to_clean) > 0 else 0
                            st.metric("ä¿ç•™ç‡", f"{retention:.1f}%")
                        
                        # Show validation results
                        validation_results = []
                        if batch_reapply_steady_state and "is_steady_state" in df_cleaned.columns:
                            steady_count = df_cleaned["is_steady_state"].sum()
                            validation_results.append(f"ç©©æ…‹è³‡æ–™: {steady_count} ç­†")
                        if batch_reapply_heat_balance and "heat_balance_invalid" in df_cleaned.columns:
                            invalid_count = df_cleaned["heat_balance_invalid"].sum()
                            validation_results.append(f"ç†±å¹³è¡¡ç•°å¸¸: {invalid_count} ç­†")
                        if batch_reapply_affinity and "affinity_law_invalid" in df_cleaned.columns:
                            invalid_count = df_cleaned["affinity_law_invalid"].sum()
                            validation_results.append(f"è¦ªå’ŒåŠ›å®šå¾‹ç•°å¸¸: {invalid_count} ç­†")
                        
                        if validation_results:
                            st.info(" | ".join(validation_results))
                        
                        # Show frozen data detection
                        frozen_cols = [col for col in df_cleaned.columns if '_frozen' in col]
                        if frozen_cols:
                            st.subheader("âš ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
                            for col in frozen_cols:
                                frozen_count = df_cleaned[col].sum()
                                if frozen_count > 0:
                                    st.warning(f"{col.replace('_frozen', '')}: {frozen_count} ç­†å‡çµè³‡æ–™")
                        
                        # Update session state
                        st.session_state['df_clean'] = df_cleaned
                        merged_df = df_cleaned  # Update local reference
                        
                        st.subheader("æ¸…æ´—å¾Œè³‡æ–™é è¦½")
                        st.dataframe(
                            df_cleaned.head(100).to_pandas(),
                            use_container_width=True,
                            height=400
                        )
                        
                    except Exception as e:
                        st.error(f"âŒ æ¸…æ´—éŒ¯èª¤: {str(e)}")
                        st.exception(e)
                
                # Show current cleaning status
                if auto_clean and not batch_reapply_steady_state and not batch_reapply_heat_balance and not batch_reapply_affinity:
                    st.markdown("---")
                    st.success("âœ… æ‰¹æ¬¡è™•ç†æ™‚å·²è‡ªå‹•åŸ·è¡ŒåŸºç¤æ¸…æ´—ï¼ˆé‡æ¡æ¨£ã€æ¿•çƒæº«åº¦ã€å‡çµæª¢æ¸¬ï¼‰")
                    
            else:
                st.error("âŒ æ²’æœ‰å¯æ¸…æ´—çš„è³‡æ–™")
                st.info("è«‹å…ˆåŸ·è¡Œæ‰¹æ¬¡è™•ç†æˆ–è¼‰å…¥è³‡æ–™")
            
        with batch_tab3:
            st.subheader("çµ±è¨ˆè³‡è¨Š")
            
            # Show data status
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™** (å·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼)")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™** (åŸå§‹è³‡æ–™)")
            
            # Select numeric columns (excluding Date/Time)
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            if numeric_cols:
                selected_col = st.selectbox("é¸æ“‡æ¬„ä½", numeric_cols, key="batch_stats_col")
                
                if selected_col:
                    col_data_clean = merged_df[selected_col].drop_nulls()
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        mean_val = col_data_clean.mean()
                        st.metric("å¹³å‡å€¼", f"{mean_val:.2f}" if mean_val is not None else "N/A")
                    with col2:
                        median_val = col_data_clean.median()
                        st.metric("ä¸­ä½æ•¸", f"{median_val:.2f}" if median_val is not None else "N/A")
                    with col3:
                        min_val = col_data_clean.min()
                        st.metric("æœ€å°å€¼", f"{min_val:.2f}" if min_val is not None else "N/A")
                    with col4:
                        max_val = col_data_clean.max()
                        st.metric("æœ€å¤§å€¼", f"{max_val:.2f}" if max_val is not None else "N/A")
                    with col5:
                        std_val = col_data_clean.std()
                        st.metric("æ¨™æº–å·®", f"{std_val:.2f}" if std_val is not None else "N/A")
                    
                    # Distribution
                    st.subheader("æ•¸å€¼åˆ†å¸ƒ")
                    pandas_data = col_data_clean.to_pandas()
                    
                    if len(pandas_data) > 0:
                        import numpy as np
                        import pandas as pd
                        
                        counts, bin_edges = np.histogram(pandas_data, bins=30)
                        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                        hist_df = pd.DataFrame({'value': bin_centers, 'count': counts}).set_index('value')
                        
                        if hist_df['count'].sum() > 0:
                            st.bar_chart(hist_df)
                        
                        data_range = col_data_clean.max() - col_data_clean.min()
                        st.caption(f"è³‡æ–™ç¯„åœ: {data_range:.2f} | éç©ºå€¼æ•¸é‡: {len(pandas_data):,}")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            
        with batch_tab4:
            st.subheader("æ™‚é–“åºåˆ—åˆ†æ")
            
            if 'timestamp' in merged_df.columns:
                numeric_cols = get_analysis_numeric_cols(merged_df)
                
                if numeric_cols:
                    selected_cols = st.multiselect(
                        "é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½ï¼ˆæœ€å¤š3å€‹ï¼‰",
                        numeric_cols,
                        default=[numeric_cols[0]] if numeric_cols else [],
                        max_selections=3,
                        key="batch_timeseries_cols"
                    )
                    
                    if selected_cols:
                        pandas_df = merged_df.select(['timestamp'] + selected_cols).to_pandas()
                        pandas_df = pandas_df.set_index('timestamp')
                        st.line_chart(pandas_df)
                        
                        st.caption(f"æ™‚é–“ç¯„åœ: {merged_df['timestamp'].min()} è‡³ {merged_df['timestamp'].max()}")
                        st.caption(f"è³‡æ–™é»æ•¸: {len(merged_df):,}")
                    else:
                        st.info("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¬„ä½")
                else:
                    st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
            else:
                st.error("è³‡æ–™ä¸­æ²’æœ‰ timestamp æ¬„ä½")
            

        with batch_tab5:
            st.header("ğŸ”— é—œè¯çŸ©é™£ç†±åœ–")
            
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            if numeric_cols:
                st.subheader("é¸æ“‡è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
                
                max_vars = min(15, len(numeric_cols))
                selected_vars = st.multiselect(
                    f"é¸æ“‡è¦åˆ†æçš„è®Šæ•¸ï¼ˆæœ€å¤š {max_vars} å€‹ï¼Œå»ºè­° 5-10 å€‹ï¼‰",
                    numeric_cols,
                    default=numeric_cols[:min(8, len(numeric_cols))],
                    max_selections=max_vars,
                    key="batch_corr_vars"
                )
                
                if len(selected_vars) >= 2:
                    try:
                        import plotly.figure_factory as ff
                        import numpy as np
                        
                        corr_df = merged_df.select(selected_vars).to_pandas()
                        corr_matrix = corr_df.corr()
                        
                        fig = ff.create_annotated_heatmap(
                            z=corr_matrix.values,
                            x=list(corr_matrix.columns),
                            y=list(corr_matrix.index),
                            annotation_text=np.around(corr_matrix.values, decimals=2),
                            colorscale='RdBu',
                            zmid=0,
                            showscale=True
                        )
                        
                        fig.update_layout(
                            title="è®Šæ•¸ç›¸é—œæ€§çŸ©é™£",
                            height=600,
                            xaxis={'side': 'bottom'}
                        )
                        fig.update_xaxes(tickangle=45)
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        st.markdown("---")
                        st.subheader("ğŸ“– ç›¸é—œä¿‚æ•¸è§£è®€")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.markdown("**ğŸ”´ å¼·è² ç›¸é—œ**: -1.0 ~ -0.7")
                        with col2:
                            st.markdown("**âšª ç„¡ç›¸é—œ**: -0.3 ~ 0.3")
                        with col3:
                            st.markdown("**ğŸ”µ å¼·æ­£ç›¸é—œ**: 0.7 ~ 1.0")
                        
                    except Exception as e:
                        st.error(f"è¨ˆç®—ç›¸é—œæ€§å¤±æ•—: {str(e)}")
                else:
                    st.warning("è«‹è‡³å°‘é¸æ“‡ 2 å€‹è®Šæ•¸é€²è¡Œç›¸é—œæ€§åˆ†æ")
            else:
                st.warning("æ²’æœ‰æ•¸å€¼æ¬„ä½å¯ä¾›åˆ†æ")
        
        with batch_tab6:
            st.header("ğŸ¯ è³‡æ–™å“è³ªå„€è¡¨æ¿")
            
            if auto_clean:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šæ¸…æ´—å¾Œè³‡æ–™**")
            else:
                st.info("ğŸ“Š **ç›®å‰åˆ†æï¼šè§£æå¾Œè³‡æ–™**")
            
            st.subheader("ğŸ“ˆ æ•´é«”è³‡æ–™å“è³ª")
            
            total_rows = len(merged_df)
            total_cols = len(merged_df.columns)
            numeric_cols = get_analysis_numeric_cols(merged_df)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ç¸½åˆ—æ•¸", f"{total_rows:,}")
            with col2:
                st.metric("ç¸½æ¬„ä½æ•¸", f"{total_cols}")
            with col3:
                st.metric("æ•¸å€¼æ¬„ä½", f"{len(numeric_cols)}")
            with col4:
                if 'timestamp' in merged_df.columns:
                    time_span = merged_df['timestamp'].max() - merged_df['timestamp'].min()
                    st.metric("æ™‚é–“è·¨åº¦", str(time_span))
            
            st.markdown("---")
            st.subheader("ğŸ” ç¼ºå¤±å€¼åˆ†æ")
            
            # Columns to exclude from missing value analysis (Date/Time related)
            exclude_missing_cols = {'Date', 'Time', 'timestamp', 'date', 'time'}
            
            missing_data = []
            for col in merged_df.columns:
                # Skip Date/Time columns
                if col in exclude_missing_cols:
                    continue
                null_count = merged_df[col].null_count()
                if null_count > 0:
                    null_pct = (null_count / total_rows) * 100
                    missing_data.append({
                        'æ¬„ä½åç¨±': col,
                        'ç¼ºå¤±æ•¸é‡': null_count,
                        'ç¼ºå¤±æ¯”ä¾‹': f"{null_pct:.2f}%",
                        'åš´é‡ç¨‹åº¦': 'ğŸ”´ é«˜' if null_pct > 30 else ('ğŸŸ¡ ä¸­' if null_pct > 10 else 'ğŸŸ¢ ä½')
                    })
            
            if missing_data:
                import pandas as pd
                missing_df = pd.DataFrame(missing_data).sort_values('ç¼ºå¤±æ•¸é‡', ascending=False)
                st.dataframe(missing_df, use_container_width=True)
                
                # Visualize missing data
                import plotly.express as px
                fig = px.bar(
                    missing_df.head(10),
                    x='æ¬„ä½åç¨±',
                    y='ç¼ºå¤±æ•¸é‡',
                    title='å‰ 10 å€‹ç¼ºå¤±å€¼æœ€å¤šçš„æ¬„ä½',
                    labels={'ç¼ºå¤±æ•¸é‡': 'ç¼ºå¤±æ•¸é‡', 'æ¬„ä½åç¨±': 'æ¬„ä½'}
                )
                fig.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.success("âœ… æ²’æœ‰ç¼ºå¤±å€¼ï¼")
            
            # Physics Validation Status Section
            st.markdown("---")
            st.subheader("ğŸ”¬ ç‰©ç†é©—è­‰ç‹€æ…‹")
            
            validation_cols = st.columns(3)
            
            with validation_cols[0]:
                st.markdown("**ğŸ“Š ç©©æ…‹æª¢æ¸¬**")
                if 'is_steady_state' in merged_df.columns:
                    steady_count = merged_df['is_steady_state'].sum()
                    total_count = len(merged_df)
                    steady_pct = (steady_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç©©æ…‹è³‡æ–™", f"{steady_count:,} ({steady_pct:.1f}%)")
                    
                    # Small bar chart
                    steady_data = {'ç‹€æ…‹': ['ç©©æ…‹', 'éç©©æ…‹'], 'æ•¸é‡': [steady_count, total_count - steady_count]}
                    import pandas as pd
                    st.bar_chart(pd.DataFrame(steady_data).set_index('ç‹€æ…‹'))
                else:
                    st.caption("æœªåŸ·è¡Œç©©æ…‹æª¢æ¸¬")
            
            with validation_cols[1]:
                st.markdown("**ğŸŒ¡ï¸ ç†±å¹³è¡¡é©—è­‰**")
                if 'heat_balance_invalid' in merged_df.columns:
                    invalid_count = merged_df['heat_balance_invalid'].sum()
                    total_count = len(merged_df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    if invalid_pct > 20:
                        st.error("ğŸ”´ ç•°å¸¸æ¯”ä¾‹éé«˜")
                    elif invalid_pct > 10:
                        st.warning("ğŸŸ¡ ç•°å¸¸æ¯”ä¾‹ä¸­ç­‰")
                    else:
                        st.success("ğŸŸ¢ ç•°å¸¸æ¯”ä¾‹æ­£å¸¸")
                else:
                    st.caption("æœªåŸ·è¡Œç†±å¹³è¡¡é©—è­‰")
            
            with validation_cols[2]:
                st.markdown("**âš¡ è¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥**")
                if 'affinity_law_invalid' in merged_df.columns:
                    invalid_count = merged_df['affinity_law_invalid'].sum()
                    total_count = len(merged_df)
                    invalid_pct = (invalid_count / total_count * 100) if total_count > 0 else 0
                    st.metric("ç•°å¸¸è³‡æ–™", f"{invalid_count:,} ({invalid_pct:.1f}%)")
                    
                    # Show affinity ratio distribution if available
                    if 'affinity_ratio' in merged_df.columns:
                        ratio_data = merged_df['affinity_ratio'].drop_nulls()
                        if len(ratio_data) > 0:
                            st.caption(f"æ¯”ç‡ç¯„åœ: {ratio_data.min():.4f} ~ {ratio_data.max():.4f}")
                else:
                    st.caption("æœªåŸ·è¡Œè¦ªå’ŒåŠ›å®šå¾‹æª¢æŸ¥")
            
            # Frozen data detection summary
            st.markdown("---")
            st.subheader("â„ï¸ å‡çµè³‡æ–™æª¢æ¸¬")
            
            if auto_clean:
                frozen_cols = [col for col in merged_df.columns if '_frozen' in col]
                
                if frozen_cols:
                    frozen_summary = []
                    for col in frozen_cols:
                        original_col = col.replace('_frozen', '')
                        frozen_count = merged_df[col].sum()
                        if frozen_count > 0:
                            frozen_pct = (frozen_count / total_rows) * 100
                            frozen_summary.append({
                                'æ„Ÿæ¸¬å™¨': original_col,
                                'å‡çµé»æ•¸': frozen_count,
                                'å‡çµæ¯”ä¾‹': f"{frozen_pct:.2f}%",
                                'ç‹€æ…‹': 'ğŸ”´ è­¦å‘Š' if frozen_pct > 5 else 'ğŸŸ¡ æ³¨æ„'
                            })
                    
                    if frozen_summary:
                        import pandas as pd
                        frozen_df = pd.DataFrame(frozen_summary).sort_values('å‡çµé»æ•¸', ascending=False)
                        st.dataframe(frozen_df, use_container_width=True)
                        
                        st.warning("âš ï¸ å‡çµè³‡æ–™å¯èƒ½è¡¨ç¤ºæ„Ÿæ¸¬å™¨æ•…éšœæˆ–æ•¸æ“šå‚³è¼¸å•é¡Œ")
                    else:
                        st.success("âœ… æ²’æœ‰åµæ¸¬åˆ°å‡çµè³‡æ–™")
                else:
                    st.info("è³‡æ–™ä¸­ç„¡å‡çµæ¨™è¨˜æ¬„ä½")
            else:
                st.info("å°šæœªåŸ·è¡Œå‡çµè³‡æ–™åµæ¸¬ï¼ˆéœ€å…ˆæ¸…æ´—è³‡æ–™ï¼‰")
            
            # Data completeness timeline
            if 'timestamp' in merged_df.columns and numeric_cols:
                st.markdown("---")
                st.subheader("ğŸ“… è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸")
                
                # Select a representative column to check completeness
                sample_col = st.selectbox(
                    "é¸æ“‡æ¬„ä½æª¢è¦–å®Œæ•´æ€§",
                    numeric_cols,
                    key="batch_completeness_col"
                )
                
                if sample_col:
                    # Create a binary completeness indicator
                    timeline_df = merged_df.select(['timestamp', sample_col]).to_pandas()
                    timeline_df['å®Œæ•´æ€§'] = (~timeline_df[sample_col].isna()).astype(int)
                    timeline_df = timeline_df.set_index('timestamp')
                    
                    import plotly.graph_objects as go
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=timeline_df.index,
                        y=timeline_df['å®Œæ•´æ€§'],
                        mode='lines',
                        fill='tozeroy',
                        name='è³‡æ–™å­˜åœ¨',
                        line=dict(color='green')
                    ))
                    
                    fig.update_layout(
                        title=f"{sample_col} è³‡æ–™å®Œæ•´æ€§æ™‚é–“è»¸",
                        xaxis_title="æ™‚é–“",
                        yaxis_title="è³‡æ–™å­˜åœ¨ (1=æœ‰, 0=ç„¡)",
                        height=300,
                        yaxis=dict(tickvals=[0, 1], ticktext=['ç¼ºå¤±', 'å­˜åœ¨'])
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
            
            # Data quality score
            st.markdown("---")
            st.subheader("â­ æ•´é«”å“è³ªè©•åˆ†")
            
            # Calculate quality score (0-100)
            quality_score = 100
            
            # Deduct points for missing data
            if missing_data:
                avg_missing_pct = sum([float(d['ç¼ºå¤±æ¯”ä¾‹'].strip('%')) for d in missing_data]) / len(merged_df.columns)
                quality_score -= min(avg_missing_pct, 30)
            
            # Deduct points for frozen data (only if cleaned)
            if auto_clean:
                frozen_cols = [col for col in merged_df.columns if '_frozen' in col]
                if frozen_cols:
                    frozen_count = sum([merged_df[col].sum() for col in frozen_cols])
                    frozen_pct = (frozen_count / (total_rows * len(frozen_cols))) * 100 if frozen_cols else 0
                    quality_score -= min(frozen_pct, 20)
            
            quality_score = max(0, quality_score)
            
            # Display score with color coding
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                st.metric("è³‡æ–™å“è³ªè©•åˆ†", f"{quality_score:.1f}/100")
            
            with col2:
                if quality_score >= 90:
                    st.success("ğŸŸ¢ å„ªç§€")
                elif quality_score >= 75:
                    st.info("ğŸ”µ è‰¯å¥½")
                elif quality_score >= 60:
                    st.warning("ğŸŸ¡ å°šå¯")
                else:
                    st.error("ğŸ”´ éœ€æ”¹å–„")
            
            with col3:
                # Progress bar
                st.progress(quality_score / 100)
            
            # Recommendations
            if quality_score < 90:
                st.markdown("---")
                st.subheader("ğŸ’¡ æ”¹å–„å»ºè­°")
                
                if missing_data and len(missing_data) > 0:
                    st.markdown("- æª¢æŸ¥ç¼ºå¤±æ¯”ä¾‹ > 10% çš„æ¬„ä½ï¼Œè€ƒæ…®è£œå€¼æˆ–ç§»é™¤")
                
                if auto_clean:
                    frozen_cols = [col for col in merged_df.columns if '_frozen' in col]
                    if frozen_cols:
                        frozen_count = sum([merged_df[col].sum() for col in frozen_cols])
                        if frozen_count > 0:
                            st.markdown("- æª¢æŸ¥å‡çµè³‡æ–™çš„æ„Ÿæ¸¬å™¨ï¼Œå¯èƒ½éœ€è¦ç¶­è­·")
                
                st.markdown("- ç¢ºèªè³‡æ–™æ”¶é›†é »ç‡èˆ‡é æœŸä¸€è‡´")
                st.markdown("- è€ƒæ…®é€²è¡Œç•°å¸¸å€¼åµæ¸¬èˆ‡è™•ç†")

        with batch_tab7:
            st.header("åŒ¯å‡ºè³‡æ–™")
            
            # Data selection radio (matching single file mode)
            export_type = st.radio(
                "é¸æ“‡åŒ¯å‡ºè³‡æ–™",
                ["è§£æå¾Œè³‡æ–™", "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰"],
                key="batch_export_type"
            )
            
            df_to_export = None
            if export_type == "è§£æå¾Œè³‡æ–™" and 'df_parsed' in st.session_state:
                df_to_export = st.session_state['df_parsed']
                st.info("ğŸ“Š **åŒ¯å‡ºï¼šè§£æå¾Œè³‡æ–™**ï¼ˆåŸå§‹åˆä½µè³‡æ–™ï¼‰")
            elif export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰" and 'df_clean' in st.session_state:
                df_to_export = st.session_state['df_clean']
                st.info("ğŸ“Š **åŒ¯å‡ºï¼šæ¸…æ´—å¾Œè³‡æ–™**ï¼ˆå·²é‡æ¡æ¨£ä¸¦éæ¿¾ç•°å¸¸å€¼ï¼‰")
            
            if df_to_export is not None:
                col1, col2 = st.columns(2)
                
                with col1:
                    # CSV export
                    csv_data = df_to_export.write_csv()
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ CSV",
                        data=csv_data,
                        file_name=f"hvac_batch_{batch_file_count}files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    # Parquet export
                    from io import BytesIO
                    buffer = BytesIO()
                    df_to_export.write_parquet(buffer)
                    parquet_data = buffer.getvalue()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ Parquet",
                        data=parquet_data,
                        file_name=f"hvac_batch_{batch_file_count}files_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
                        mime="application/octet-stream"
                    )
                
                st.info("ğŸ’¡ Parquet æ ¼å¼è¼ƒå°ä¸”æ•ˆèƒ½æ›´å¥½ï¼Œé©åˆå¤§å‹è³‡æ–™é›†")
            else:
                if export_type == "æ¸…æ´—å¾Œè³‡æ–™ï¼ˆå¦‚å·²æ¸…æ´—ï¼‰":
                    st.warning("è«‹å…ˆåŸ·è¡Œè³‡æ–™æ¸…æ´—æˆ–é¸æ“‡ã€Œè§£æå¾Œè³‡æ–™ã€")
                else:
                    st.warning("æ²’æœ‰å¯åŒ¯å‡ºçš„è³‡æ–™")

elif processing_mode == "âš¡ æœ€ä½³åŒ–æ¨¡æ“¬" and ML_AVAILABLE:
    # Optimization Simulation Mode
    st.header("âš¡ èƒ½è€—æœ€ä½³åŒ–æ¨¡æ“¬")
    st.markdown("**ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œæ‰¾å‡ºæœ€çœé›»çš„è®Šé »å™¨è¨­å®š**")
    
    # Check if model is selected
    if 'selected_model' in dir() and selected_model:
        model_path = Path("models") / selected_model
        
        # Load model
        @st.cache_resource
        def load_model(path):
            return ChillerEnergyModel.load_model(str(path))
        
        try:
            model = load_model(model_path)
            
            # Show model info
            col1, col2, col3 = st.columns(3)
            with col1:
                if model.training_metrics:
                    st.metric("æ¨¡å‹ MAPE", f"{model.training_metrics.get('mape', 0):.2f}%")
            with col2:
                if model.training_metrics:
                    st.metric("æ¨¡å‹ RÂ²", f"{model.training_metrics.get('r2', 0):.4f}")
            with col3:
                st.metric("ç‰¹å¾µæ•¸é‡", f"{len(model.feature_names)}")
            
            st.success(f"âœ… å·²è¼‰å…¥æ¨¡å‹: {selected_model}")
            
            # Create tabs for different functions
            opt_tab1, opt_tab2, opt_tab3, opt_tab4 = st.tabs([
                "ğŸ¯ å³æ™‚æœ€ä½³åŒ–",
                "ğŸ“Š ç‰¹å¾µé‡è¦æ€§",
                "ğŸ“ˆ æ­·å²è¿½è¹¤",
                "ğŸ”§ æ¨¡å‹è¨“ç·´"
            ])
            
            with opt_tab1:
                st.subheader("è¨­å®šç•¶å‰é‹è½‰æ¢ä»¶")
                
                # Input parameters
                st.markdown("#### ğŸ­ è² è¼‰æ¢ä»¶")
                col1, col2 = st.columns(2)
                with col1:
                    load_rt = st.slider(
                        "å†·å‡å™¸è² è¼‰ (RT)",
                        min_value=100,
                        max_value=2000,
                        value=500,
                        step=50,
                        help="ç•¶å‰çš„å†·å»è² è¼‰"
                    )
                with col2:
                    temp_db_out = st.slider(
                        "å®¤å¤–ä¹¾çƒæº«åº¦ (Â°C)",
                        min_value=15.0,
                        max_value=40.0,
                        value=30.0,
                        step=0.5,
                        help="ç•¶å‰å®¤å¤–æº«åº¦"
                    )
                
                st.markdown("#### âš™ï¸ ç•¶å‰è®Šé »å™¨è¨­å®š")
                col1, col2, col3 = st.columns(3)
                with col1:
                    current_chw_pump_hz = st.slider(
                        "å†°æ°´æ³µé »ç‡ (Hz)",
                        min_value=30.0,
                        max_value=60.0,
                        value=50.0,
                        step=1.0,
                        help="CHP è®Šé »å™¨è¼¸å‡º"
                    )
                with col2:
                    current_cw_pump_hz = st.slider(
                        "å†·å»æ°´æ³µé »ç‡ (Hz)",
                        min_value=30.0,
                        max_value=60.0,
                        value=50.0,
                        step=1.0,
                        help="CWP è®Šé »å™¨è¼¸å‡º"
                    )
                with col3:
                    current_ct_fan_hz = st.slider(
                        "å†·å»å¡”é¢¨æ‰‡é »ç‡ (Hz)",
                        min_value=30.0,
                        max_value=60.0,
                        value=50.0,
                        step=1.0,
                        help="CT è®Šé »å™¨è¼¸å‡º"
                    )
                
                st.markdown("---")
                
                # Optimization options
                col1, col2 = st.columns(2)
                with col1:
                    opt_method = st.radio(
                        "æœ€ä½³åŒ–æ–¹æ³•",
                        ["SLSQP (å¿«é€Ÿ)", "Differential Evolution (å…¨åŸŸ)"],
                        help="SLSQP é©åˆå¿«é€Ÿæ±‚è§£ï¼ŒDE é©åˆå°‹æ‰¾å…¨åŸŸæœ€ä½³è§£"
                    )
                
                # Run optimization button
                if st.button("ğŸš€ åŸ·è¡Œæœ€ä½³åŒ–", type="primary", use_container_width=True):
                    with st.spinner("æ­£åœ¨è¨ˆç®—æœ€ä½³è¨­å®š..."):
                        # Create context
                        context = OptimizationContext(
                            load_rt=load_rt,
                            temp_db_out=temp_db_out,
                            current_chw_pump_hz=current_chw_pump_hz,
                            current_cw_pump_hz=current_cw_pump_hz,
                            current_ct_fan_hz=current_ct_fan_hz
                        )
                        
                        # Create optimizer
                        optimizer = ChillerOptimizer(model)
                        
                        # Run optimization
                        if "SLSQP" in opt_method:
                            result = optimizer.optimize_slsqp(context)
                        else:
                            result = optimizer.optimize_global(context, maxiter=50)
                        
                        # Store result and context in session state for persistence
                        st.session_state['last_optimization_result'] = result
                        st.session_state['last_optimization_context'] = {
                            'load_rt': load_rt,
                            'temp_db_out': temp_db_out,
                            'current_chw_pump_hz': current_chw_pump_hz,
                            'current_cw_pump_hz': current_cw_pump_hz,
                            'current_ct_fan_hz': current_ct_fan_hz,
                            'opt_method': opt_method,
                            'model_name': selected_model
                        }
                        st.session_state['optimization_saved'] = False
                
                # Display results if available in session state
                if 'last_optimization_result' in st.session_state and st.session_state['last_optimization_result'] is not None:
                    result = st.session_state['last_optimization_result']
                    ctx = st.session_state.get('last_optimization_context', {})
                    
                    # Display results
                    st.markdown("---")
                    st.subheader("ğŸ“Š æœ€ä½³åŒ–çµæœ")
                    
                    if result.success:
                        st.success("âœ… æœ€ä½³åŒ–æˆåŠŸå®Œæˆï¼")
                    else:
                        st.warning(f"âš ï¸ {result.message}")
                    
                    # Comparison table
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.markdown("##### ğŸ”§ è®Šé »å™¨è¨­å®š")
                        import pandas as pd
                        settings_df = pd.DataFrame({
                            'é …ç›®': ['å†°æ°´æ³µ (Hz)', 'å†·å»æ°´æ³µ (Hz)', 'å†·å»å¡”é¢¨æ‰‡ (Hz)'],
                            'ç›®å‰è¨­å®š': [
                                ctx.get('current_chw_pump_hz', '-'),
                                ctx.get('current_cw_pump_hz', '-'),
                                ctx.get('current_ct_fan_hz', '-')
                            ],
                            'å»ºè­°è¨­å®š': [
                                f"{result.optimal_chw_pump_hz:.1f}",
                                f"{result.optimal_cw_pump_hz:.1f}",
                                f"{result.optimal_ct_fan_hz:.1f}"
                            ]
                        })
                        st.dataframe(settings_df, hide_index=True, use_container_width=True)
                    
                    with col2:
                        st.markdown("##### âš¡ èƒ½è€—æ¯”è¼ƒ")
                        st.metric(
                            "ç›®å‰é ä¼°èƒ½è€—",
                            f"{result.baseline_power_kw:.1f} kW"
                        )
                        st.metric(
                            "æœ€ä½³åŒ–å¾Œèƒ½è€—",
                            f"{result.predicted_power_kw:.1f} kW",
                            delta=f"-{result.savings_kw:.1f} kW" if result.savings_kw > 0 else f"+{-result.savings_kw:.1f} kW",
                            delta_color="inverse"
                        )
                    
                    with col3:
                        st.markdown("##### ğŸ’° ç¯€èƒ½æ•ˆç›Š")
                        st.metric(
                            "ç¯€èƒ½æ¯”ä¾‹",
                            f"{result.savings_percent:.1f}%"
                        )
                        # Estimate annual savings (assuming 8760 hours/year, $0.1/kWh)
                        annual_savings = result.savings_kw * 8760 * 3.5  # TWD per kWh
                        if result.savings_kw > 0:
                            st.metric(
                                "é ä¼°å¹´ç¯€çœ",
                                f"NT$ {annual_savings:,.0f}"
                            )
                    
                    # Constraint violations
                    if result.constraint_violations:
                        st.markdown("---")
                        st.warning("âš ï¸ é™åˆ¶æ¢ä»¶è­¦å‘Š")
                        for v in result.constraint_violations:
                            st.caption(f"â€¢ {v}")
                    
                    # Save result button - only show if not already saved
                    st.markdown("---")
                    if not st.session_state.get('optimization_saved', False):
                        if st.button("ğŸ’¾ å„²å­˜æ­¤æ¬¡çµæœ", key="save_optimization_result"):
                            try:
                                # Initialize history tracker
                                history_tracker = OptimizationHistoryTracker()
                                
                                # Create current and optimal settings dicts
                                current_settings = {
                                    'chw_pump_hz': ctx.get('current_chw_pump_hz', 0),
                                    'cw_pump_hz': ctx.get('current_cw_pump_hz', 0),
                                    'tower_fan_hz': ctx.get('current_ct_fan_hz', 0)
                                }
                                optimal_settings = {
                                    'chw_pump_hz': result.optimal_chw_pump_hz,
                                    'cw_pump_hz': result.optimal_cw_pump_hz,
                                    'tower_fan_hz': result.optimal_ct_fan_hz
                                }
                                
                                # Create record
                                record = create_record_from_result(
                                    model_name=ctx.get('model_name', 'unknown'),
                                    load_rt=ctx.get('load_rt', 0),
                                    outdoor_temp=ctx.get('temp_db_out', 0),
                                    current_settings=current_settings,
                                    optimal_settings=optimal_settings,
                                    current_power=result.baseline_power_kw,
                                    optimal_power=result.predicted_power_kw,
                                    method="SLSQP" if "SLSQP" in ctx.get('opt_method', '') else "Differential Evolution"
                                )
                                
                                # Save record
                                history_tracker.add_record(record)
                                st.session_state['optimization_saved'] = True
                                st.success("âœ… çµæœå·²å„²å­˜ï¼å¯åœ¨ã€ŒğŸ“ˆ æ­·å²è¿½è¹¤ã€åˆ†é æŸ¥çœ‹ã€‚")
                            except Exception as e:
                                st.error(f"å„²å­˜å¤±æ•—: {e}")
                    else:
                        st.info("âœ… æ­¤æ¬¡çµæœå·²å„²å­˜ã€‚åŸ·è¡Œæ–°çš„æœ€ä½³åŒ–å¾Œå¯å†æ¬¡å„²å­˜ã€‚")
            
            with opt_tab2:
                st.subheader("ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ")
                
                importance = model.get_feature_importance()
                
                if importance:
                    import pandas as pd
                    import plotly.express as px
                    
                    # Create dataframe
                    importance_df = pd.DataFrame([
                        {'ç‰¹å¾µ': k, 'é‡è¦æ€§': v}
                        for k, v in list(importance.items())[:15]
                    ])
                    
                    # Bar chart
                    fig = px.bar(
                        importance_df,
                        x='é‡è¦æ€§',
                        y='ç‰¹å¾µ',
                        orientation='h',
                        title='Top 15 ç‰¹å¾µé‡è¦æ€§',
                        labels={'é‡è¦æ€§': 'é‡è¦æ€§åˆ†æ•¸', 'ç‰¹å¾µ': 'ç‰¹å¾µåç¨±'}
                    )
                    fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Table
                    st.markdown("##### å®Œæ•´ç‰¹å¾µé‡è¦æ€§åˆ—è¡¨")
                    full_importance_df = pd.DataFrame([
                        {'æ’å': i+1, 'ç‰¹å¾µ': k, 'é‡è¦æ€§': f"{v:.4f}"}
                        for i, (k, v) in enumerate(importance.items())
                    ])
                    st.dataframe(full_importance_df, hide_index=True, use_container_width=True)
                else:
                    st.info("ç„¡æ³•å–å¾—ç‰¹å¾µé‡è¦æ€§")
            
            with opt_tab3:
                st.subheader("ğŸ“ˆ æœ€ä½³åŒ–æ­·å²è¿½è¹¤")
                st.markdown("è¿½è¹¤éå»çš„æœ€ä½³åŒ–çµæœä¸¦åˆ†æç¯€èƒ½è¶¨å‹¢")
                
                try:
                    # Load history
                    history_tracker = OptimizationHistoryTracker()
                    records = history_tracker.get_all_records()
                    stats = history_tracker.get_total_savings()
                    
                    if records:
                        # Summary metrics
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("ç¸½åŸ·è¡Œæ¬¡æ•¸", f"{stats['total_runs']} æ¬¡")
                        with col2:
                            st.metric("ç´¯è¨ˆç¯€çœ", f"{stats['total_savings_kw']:.1f} kW")
                        with col3:
                            st.metric("å¹³å‡ç¯€èƒ½ç‡", f"{stats['avg_savings_percent']:.1f}%")
                        with col4:
                            st.metric("æœ€é«˜ç¯€èƒ½ç‡", f"{stats['max_savings_percent']:.1f}%")
                        
                        st.markdown("---")
                        
                        # Trend chart
                        import pandas as pd
                        import plotly.express as px
                        import plotly.graph_objects as go
                        
                        # Prepare data for chart
                        history_df = pd.DataFrame([{
                            'æ™‚é–“': r.timestamp[:16].replace('T', ' '),
                            'ç¯€èƒ½ç‡ (%)': r.savings_percent,
                            'ç¯€çœé›»åŠ› (kW)': r.savings_kw,
                            'è² è¼‰ (RT)': r.load_rt,
                            'ç›®å‰èƒ½è€— (kW)': r.current_power_kw,
                            'æœ€ä½³èƒ½è€— (kW)': r.optimal_power_kw
                        } for r in records])
                        
                        # Savings trend chart
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            x=history_df['æ™‚é–“'],
                            y=history_df['ç¯€èƒ½ç‡ (%)'],
                            mode='lines+markers',
                            name='ç¯€èƒ½ç‡ (%)',
                            line=dict(color='#00CC96', width=2),
                            marker=dict(size=8)
                        ))
                        fig.update_layout(
                            title='ç¯€èƒ½ç‡è¶¨å‹¢',
                            xaxis_title='æ™‚é–“',
                            yaxis_title='ç¯€èƒ½ç‡ (%)',
                            height=350
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Power comparison chart
                        fig2 = go.Figure()
                        fig2.add_trace(go.Bar(
                            x=history_df['æ™‚é–“'],
                            y=history_df['ç›®å‰èƒ½è€— (kW)'],
                            name='ç›®å‰èƒ½è€—',
                            marker_color='#EF553B'
                        ))
                        fig2.add_trace(go.Bar(
                            x=history_df['æ™‚é–“'],
                            y=history_df['æœ€ä½³èƒ½è€— (kW)'],
                            name='æœ€ä½³èƒ½è€—',
                            marker_color='#00CC96'
                        ))
                        fig2.update_layout(
                            title='èƒ½è€—æ¯”è¼ƒ',
                            xaxis_title='æ™‚é–“',
                            yaxis_title='èƒ½è€— (kW)',
                            barmode='group',
                            height=350
                        )
                        st.plotly_chart(fig2, use_container_width=True)
                        
                        # History table
                        st.markdown("##### è©³ç´°ç´€éŒ„")
                        st.dataframe(
                            history_df[['æ™‚é–“', 'è² è¼‰ (RT)', 'ç›®å‰èƒ½è€— (kW)', 'æœ€ä½³èƒ½è€— (kW)', 'ç¯€çœé›»åŠ› (kW)', 'ç¯€èƒ½ç‡ (%)']],
                            hide_index=True,
                            use_container_width=True
                        )
                        
                        # Clear history button
                        st.markdown("---")
                        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ­·å²ç´€éŒ„", type="secondary"):
                            history_tracker.clear_history()
                            st.success("å·²æ¸…é™¤æ‰€æœ‰ç´€éŒ„")
                            st.rerun()
                    else:
                        st.info("ğŸ“­ å°šç„¡æ­·å²ç´€éŒ„ã€‚è«‹å…ˆåœ¨ã€ŒğŸ¯ å³æ™‚æœ€ä½³åŒ–ã€åˆ†é åŸ·è¡Œå„ªåŒ–ä¸¦å„²å­˜çµæœã€‚")
                except Exception as e:
                    st.error(f"è¼‰å…¥æ­·å²ç´€éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            with opt_tab4:
                # Model Management Section
                st.subheader("ğŸ—‚ï¸ æ¨¡å‹ç®¡ç†")
                st.markdown("ç®¡ç†å·²è¨“ç·´çš„æ¨¡å‹æª”æ¡ˆ")
                
                model_dir = Path("models")
                if model_dir.exists():
                    model_files = sorted(model_dir.glob("*.joblib"), key=lambda x: x.stat().st_mtime, reverse=True)
                    
                    if model_files:
                        st.write(f"**å·²æ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹ï¼š**")
                        
                        # Create a table of models
                        model_data = []
                        for mf in model_files:
                            stat = mf.stat()
                            size_mb = stat.st_size / (1024 * 1024)
                            mod_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M")
                            model_data.append({
                                "æ¨¡å‹åç¨±": mf.name,
                                "å¤§å°": f"{size_mb:.1f} MB",
                                "å»ºç«‹æ™‚é–“": mod_time
                            })
                        
                        st.dataframe(model_data, use_container_width=True, hide_index=True)
                        
                        # Delete model selection
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            model_to_delete = st.selectbox(
                                "é¸æ“‡è¦åˆªé™¤çš„æ¨¡å‹",
                                [f.name for f in model_files],
                                key="delete_model_select"
                            )
                        with col2:
                            st.markdown("<br>", unsafe_allow_html=True)
                            if st.button("ğŸ—‘ï¸ åˆªé™¤æ¨¡å‹", type="secondary"):
                                try:
                                    delete_path = model_dir / model_to_delete
                                    delete_path.unlink()
                                    st.success(f"âœ… å·²åˆªé™¤: {model_to_delete}")
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"âŒ åˆªé™¤å¤±æ•—: {e}")
                    else:
                        st.info("ğŸ“­ å°šæœªæœ‰ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
                
                st.markdown("---")
                st.subheader("ğŸ”§ è¨“ç·´æ–°æ¨¡å‹")
                st.markdown("ä½¿ç”¨æ‰¹æ¬¡è™•ç†å¾Œçš„è³‡æ–™è¨“ç·´èƒ½è€—é æ¸¬æ¨¡å‹")
                
                # Check if batch data is available
                if 'df_clean' in st.session_state or 'df_parsed' in st.session_state:
                    df_for_training = st.session_state.get('df_clean', st.session_state.get('df_parsed'))
                    
                    st.info(f"ğŸ“Š å¯ç”¨è³‡æ–™: {len(df_for_training):,} ç­†")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        new_model_name = st.text_input(
                            "æ¨¡å‹åç¨±",
                            value=f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        )
                    
                    if st.button("ğŸ“ é–‹å§‹è¨“ç·´", type="primary"):
                        with st.spinner("æ­£åœ¨è¨“ç·´æ¨¡å‹..."):
                            try:
                                # Pre-training diagnostics
                                st.write("ğŸ“‹ **è¨“ç·´å‰è¨ºæ–·:**")
                                st.write(f"- è³‡æ–™å½¢ç‹€: {df_for_training.shape}")
                                
                                # Check required columns using feature mapping if available
                                if 'batch_feature_mapping' in st.session_state and st.session_state.batch_feature_mapping:
                                    mapping = st.session_state.batch_feature_mapping
                                    required_cols = [mapping.target_col] + mapping.get_all_feature_cols()
                                    target_col = mapping.target_col
                                else:
                                    from models.energy_model import ModelConfig
                                    config = ModelConfig()
                                    required_cols = [config.target_col] + config.load_cols + config.chw_pump_hz_cols + config.cw_pump_hz_cols + config.ct_fan_hz_cols + config.temp_cols
                                    target_col = config.target_col
                                
                                missing = [c for c in required_cols if c not in df_for_training.columns]
                                if missing:
                                    st.error(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing}")
                                else:
                                    st.success(f"âœ… æ‰€æœ‰ {len(required_cols)} å€‹å¿…è¦æ¬„ä½éƒ½å­˜åœ¨")
                                
                                # Check target column
                                if target_col in df_for_training.columns:
                                    target_valid = df_for_training[target_col].drop_nulls().len()
                                    st.write(f"- ç›®æ¨™æ¬„ä½ ({target_col}): {target_valid}/{len(df_for_training)} æœ‰æ•ˆ")
                                
                                # Use feature mapping from UI if available
                                if 'current_feature_mapping' in st.session_state and st.session_state.current_feature_mapping:
                                    new_model = ChillerEnergyModel(feature_mapping=st.session_state.current_feature_mapping)
                                    st.info(f"ğŸ“‹ ä½¿ç”¨ Feature Mapping: {len(st.session_state.current_feature_mapping.get_all_feature_cols())} å€‹ç‰¹å¾µ")
                                else:
                                    new_model = ChillerEnergyModel()
                                
                                metrics = new_model.train(df_for_training)
                                
                                # Save model
                                model_path = f"models/{new_model_name}.joblib"
                                new_model.save_model(model_path)
                                
                                st.success(f"âœ… è¨“ç·´å®Œæˆï¼")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("MAPE", f"{metrics['mape']:.2f}%")
                                with col2:
                                    st.metric("RÂ²", f"{metrics['r2']:.4f}")
                                with col3:
                                    st.metric("RMSE", f"{metrics['rmse']:.2f}")
                                
                                st.info(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")
                                st.caption("é‡æ–°æ•´ç†é é¢å³å¯é¸æ“‡æ–°æ¨¡å‹")
                                
                            except Exception as e:
                                st.error(f"âŒ è¨“ç·´å¤±æ•—: {str(e)}")
                else:
                    st.warning("è«‹å…ˆä½¿ç”¨ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼è¼‰å…¥ä¸¦æ¸…æ´—è³‡æ–™")
                    st.caption("1. åˆ‡æ›åˆ°ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼")
                    st.caption("2. é¸æ“‡æª”æ¡ˆä¸¦åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
                    st.caption("3. å›åˆ°æ­¤é é¢é€²è¡Œæ¨¡å‹è¨“ç·´")
        
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
    else:
        st.warning("ğŸ‘ˆ è«‹å¾å·¦å´é¸æ“‡å·²è¨“ç·´çš„æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨ä¸‹æ–¹ã€Œæ¨¡å‹è¨“ç·´ã€åˆ†é è¨“ç·´æ–°æ¨¡å‹")
        
        # Still show tabs so user can train a model
        opt_tab1, opt_tab2, opt_tab3, opt_tab4 = st.tabs([
            "ğŸ¯ å³æ™‚æœ€ä½³åŒ–",
            "ğŸ“Š ç‰¹å¾µé‡è¦æ€§",
            "ğŸ“ˆ æ­·å²è¿½è¹¤",
            "ğŸ”§ æ¨¡å‹è¨“ç·´"
        ])
        
        with opt_tab1:
            st.info("è«‹å…ˆé¸æ“‡æˆ–è¨“ç·´æ¨¡å‹å¾Œæ‰èƒ½ä½¿ç”¨å³æ™‚æœ€ä½³åŒ–åŠŸèƒ½")
            st.markdown("""
            ### å¦‚ä½•é–‹å§‹ï¼Ÿ
            
            #### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ç¾æœ‰æ¨¡å‹
            å¦‚æœå·²ç¶“æœ‰è¨“ç·´å¥½çš„æ¨¡å‹ (`.joblib` æª”æ¡ˆ)ï¼Œè«‹å°‡å®ƒæ”¾åœ¨ `models/` è³‡æ–™å¤¾ä¸­ã€‚
            
            #### æ–¹æ³•äºŒï¼šè¨“ç·´æ–°æ¨¡å‹
            1. é»é¸ä¸Šæ–¹ã€ŒğŸ”§ æ¨¡å‹è¨“ç·´ã€åˆ†é 
            2. è‹¥å°šç„¡è³‡æ–™ï¼Œè«‹å…ˆåˆ‡æ›åˆ°ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼è¼‰å…¥è³‡æ–™
            3. å›åˆ°æ­¤æ¨¡å¼å¾Œå¯ç›´æ¥è¨“ç·´æ¨¡å‹
            """)
        
        with opt_tab2:
            st.info("è«‹å…ˆé¸æ“‡æ¨¡å‹æ‰èƒ½æŸ¥çœ‹ç‰¹å¾µé‡è¦æ€§")
        
        with opt_tab3:
            st.subheader("ğŸ“ˆ æœ€ä½³åŒ–æ­·å²è¿½è¹¤")
            st.markdown("è¿½è¹¤éå»çš„æœ€ä½³åŒ–çµæœä¸¦åˆ†æç¯€èƒ½è¶¨å‹¢")
            
            try:
                # Load history
                history_tracker = OptimizationHistoryTracker()
                records = history_tracker.get_all_records()
                stats = history_tracker.get_total_savings()
                
                if records:
                    # Summary metrics
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ç¸½åŸ·è¡Œæ¬¡æ•¸", f"{stats['total_runs']} æ¬¡")
                    with col2:
                        st.metric("ç´¯è¨ˆç¯€çœ", f"{stats['total_savings_kw']:.1f} kW")
                    with col3:
                        st.metric("å¹³å‡ç¯€èƒ½ç‡", f"{stats['avg_savings_percent']:.1f}%")
                    with col4:
                        st.metric("æœ€é«˜ç¯€èƒ½ç‡", f"{stats['max_savings_percent']:.1f}%")
                    
                    st.markdown("---")
                    
                    # Prepare data for chart
                    import pandas as pd
                    import plotly.graph_objects as go
                    
                    history_df = pd.DataFrame([{
                        'æ™‚é–“': r.timestamp[:16].replace('T', ' '),
                        'ç¯€èƒ½ç‡ (%)': r.savings_percent,
                        'ç¯€çœé›»åŠ› (kW)': r.savings_kw,
                        'è² è¼‰ (RT)': r.load_rt,
                        'ç›®å‰èƒ½è€— (kW)': r.current_power_kw,
                        'æœ€ä½³èƒ½è€— (kW)': r.optimal_power_kw
                    } for r in records])
                    
                    # Savings trend chart
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=history_df['æ™‚é–“'],
                        y=history_df['ç¯€èƒ½ç‡ (%)'],
                        mode='lines+markers',
                        name='ç¯€èƒ½ç‡ (%)',
                        line=dict(color='#00CC96', width=2),
                        marker=dict(size=8)
                    ))
                    fig.update_layout(
                        title='ç¯€èƒ½ç‡è¶¨å‹¢',
                        xaxis_title='æ™‚é–“',
                        yaxis_title='ç¯€èƒ½ç‡ (%)',
                        height=350
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # History table
                    st.markdown("##### è©³ç´°ç´€éŒ„")
                    st.dataframe(
                        history_df[['æ™‚é–“', 'è² è¼‰ (RT)', 'ç›®å‰èƒ½è€— (kW)', 'æœ€ä½³èƒ½è€— (kW)', 'ç¯€çœé›»åŠ› (kW)', 'ç¯€èƒ½ç‡ (%)']],
                        hide_index=True,
                        use_container_width=True
                    )
                    
                    # Clear history button
                    st.markdown("---")
                    if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ­·å²ç´€éŒ„", type="secondary", key="clear_history_no_model"):
                        history_tracker.clear_history()
                        st.success("å·²æ¸…é™¤æ‰€æœ‰ç´€éŒ„")
                        st.rerun()
                else:
                    st.info("ğŸ“­ å°šç„¡æ­·å²ç´€éŒ„ã€‚è«‹å…ˆè¨“ç·´æ¨¡å‹ä¸¦åŸ·è¡Œå„ªåŒ–ã€‚")
            except Exception as e:
                st.error(f"è¼‰å…¥æ­·å²ç´€éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        with opt_tab4:
            # Model Management Section (when no model selected)
            st.subheader("ğŸ—‚ï¸ æ¨¡å‹ç®¡ç†")
            st.markdown("ç®¡ç†å·²è¨“ç·´çš„æ¨¡å‹æª”æ¡ˆ")
            
            model_dir = Path("models")
            if model_dir.exists():
                model_files = sorted(model_dir.glob("*.joblib"), key=lambda x: x.stat().st_mtime, reverse=True)
                
                if model_files:
                    st.write(f"**å·²æ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹ï¼š**")
                    
                    # Create a table of models
                    model_data = []
                    for mf in model_files:
                        stat = mf.stat()
                        size_mb = stat.st_size / (1024 * 1024)
                        mod_time = datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M")
                        model_data.append({
                            "æ¨¡å‹åç¨±": mf.name,
                            "å¤§å°": f"{size_mb:.1f} MB",
                            "å»ºç«‹æ™‚é–“": mod_time
                        })
                    
                    st.dataframe(model_data, use_container_width=True, hide_index=True)
                    
                    # Delete model selection
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        model_to_delete = st.selectbox(
                            "é¸æ“‡è¦åˆªé™¤çš„æ¨¡å‹",
                            [f.name for f in model_files],
                            key="delete_model_select_no_model"
                        )
                    with col2:
                        st.markdown("<br>", unsafe_allow_html=True)
                        if st.button("ğŸ—‘ï¸ åˆªé™¤æ¨¡å‹", type="secondary", key="delete_btn_no_model"):
                            try:
                                delete_path = model_dir / model_to_delete
                                delete_path.unlink()
                                st.success(f"âœ… å·²åˆªé™¤: {model_to_delete}")
                                st.rerun()
                            except Exception as e:
                                st.error(f"âŒ åˆªé™¤å¤±æ•—: {e}")
                else:
                    st.info("ğŸ“­ å°šæœªæœ‰ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
            
            st.markdown("---")
            st.subheader("ğŸ”§ è¨“ç·´æ–°æ¨¡å‹")
            st.markdown("ä½¿ç”¨æ‰¹æ¬¡è™•ç†å¾Œçš„è³‡æ–™è¨“ç·´èƒ½è€—é æ¸¬æ¨¡å‹")
            
            # Check if batch data is available
            if 'df_clean' in st.session_state or 'df_parsed' in st.session_state:
                df_for_training = st.session_state.get('df_clean', st.session_state.get('df_parsed'))
                
                st.info(f"ğŸ“Š å¯ç”¨è³‡æ–™: {len(df_for_training):,} ç­†")
                
                col1, col2 = st.columns(2)
                with col1:
                    new_model_name = st.text_input(
                        "æ¨¡å‹åç¨±",
                        value=f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        key="new_model_name_no_model"
                    )
                
                if st.button("ğŸ“ é–‹å§‹è¨“ç·´", type="primary", key="train_no_model"):
                    with st.spinner("æ­£åœ¨è¨“ç·´æ¨¡å‹..."):
                        try:
                            # Pre-training diagnostics
                            st.write("ğŸ“‹ **è¨“ç·´å‰è¨ºæ–·:**")
                            st.write(f"- è³‡æ–™å½¢ç‹€: {df_for_training.shape}")
                            
                            # Check required columns using feature mapping if available
                            if 'batch_feature_mapping' in st.session_state and st.session_state.batch_feature_mapping:
                                mapping = st.session_state.batch_feature_mapping
                                required_cols = [mapping.target_col] + mapping.get_all_feature_cols()
                                target_col = mapping.target_col
                            else:
                                from models.energy_model import ModelConfig
                                config = ModelConfig()
                                required_cols = [config.target_col] + config.load_cols + config.chw_pump_hz_cols + config.cw_pump_hz_cols + config.ct_fan_hz_cols + config.temp_cols
                                target_col = config.target_col
                            
                            missing = [c for c in required_cols if c not in df_for_training.columns]
                            if missing:
                                st.error(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing}")
                            else:
                                st.success(f"âœ… æ‰€æœ‰ {len(required_cols)} å€‹å¿…è¦æ¬„ä½éƒ½å­˜åœ¨")
                            
                            # Check target column
                            if target_col in df_for_training.columns:
                                target_valid = df_for_training[target_col].drop_nulls().len()
                                st.write(f"- ç›®æ¨™æ¬„ä½ ({target_col}): {target_valid}/{len(df_for_training)} æœ‰æ•ˆ")
                            
                            # Use feature mapping from UI if available
                            if 'current_feature_mapping' in st.session_state and st.session_state.current_feature_mapping:
                                new_model = ChillerEnergyModel(feature_mapping=st.session_state.current_feature_mapping)
                                st.info(f"ğŸ“‹ ä½¿ç”¨ Feature Mapping: {len(st.session_state.current_feature_mapping.get_all_feature_cols())} å€‹ç‰¹å¾µ")
                            else:
                                new_model = ChillerEnergyModel()
                            
                            metrics = new_model.train(df_for_training)
                            
                            # Save model
                            model_path = f"models/{new_model_name}.joblib"
                            new_model.save_model(model_path)
                            
                            st.success(f"âœ… è¨“ç·´å®Œæˆï¼")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("MAPE", f"{metrics['mape']:.2f}%")
                            with col2:
                                st.metric("RÂ²", f"{metrics['r2']:.4f}")
                            with col3:
                                st.metric("RMSE", f"{metrics['rmse']:.2f}")
                            
                            st.info(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")
                            st.caption("é‡æ–°æ•´ç†é é¢å³å¯é¸æ“‡æ–°æ¨¡å‹")
                            
                        except Exception as e:
                            st.error(f"âŒ è¨“ç·´å¤±æ•—: {str(e)}")
            else:
                st.warning("è«‹å…ˆä½¿ç”¨ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼è¼‰å…¥ä¸¦æ¸…æ´—è³‡æ–™")
                st.caption("1. åˆ‡æ›åˆ°ã€Œæ‰¹æ¬¡è™•ç†ã€æ¨¡å¼")
                st.caption("2. é¸æ“‡æª”æ¡ˆä¸¦åŸ·è¡Œæ‰¹æ¬¡è™•ç†")
                st.caption("3. å›åˆ°æ­¤é é¢é€²è¡Œæ¨¡å‹è¨“ç·´")

else:
    # Welcome screen
    st.info("ğŸ‘ˆ è«‹å¾å·¦å´ä¸Šå‚³æª”æ¡ˆæˆ–é¸æ“‡ç¾æœ‰è³‡æ–™é–‹å§‹")
    
    st.markdown("""
    ### åŠŸèƒ½ä»‹ç´¹
    
    #### ğŸ“‹ è§£æè³‡æ–™
    - è‡ªå‹•è§£æå ±è¡¨æ ¼å¼çš„ CSV æª”æ¡ˆ
    - æå– Point å°ç…§è¡¨
    - è½‰æ›æ™‚é–“æˆ³è¨˜
    
    #### ğŸ§¹ æ¸…æ´—è³‡æ–™
    - é‡æ¡æ¨£è‡³å›ºå®šæ™‚é–“é–“éš”ï¼ˆ5åˆ†é˜/15åˆ†é˜ç­‰ï¼‰
    - è¨ˆç®—æ¿•çƒæº«åº¦
    - åµæ¸¬å‡çµè³‡æ–™
    
    #### ğŸ“Š çµ±è¨ˆè³‡è¨Š
    - æŸ¥çœ‹æ¬„ä½çµ±è¨ˆæ•¸æ“š
    - æ•¸å€¼åˆ†å¸ƒè¦–è¦ºåŒ–
    
    #### ğŸ“ˆ æ™‚é–“åºåˆ—
    - å¤šè®Šæ•¸è¶¨å‹¢æ¯”è¼ƒ
    - æ™‚é–“ç¯„åœåˆ†æ
    
    #### ğŸ’¾ åŒ¯å‡º
    - CSV æ ¼å¼
    - Parquet æ ¼å¼ï¼ˆæ¨è–¦ï¼‰
    
    #### ğŸ“¦ æ‰¹æ¬¡è™•ç†
    - ä¸€æ¬¡è™•ç†å¤šå€‹æª”æ¡ˆ
    - è‡ªå‹•åˆä½µè³‡æ–™
    - é€²åº¦è¿½è¹¤
    
    #### âš¡ æœ€ä½³åŒ–æ¨¡æ“¬ (æ–°åŠŸèƒ½!)
    - è¼‰å…¥è¨“ç·´å¥½çš„èƒ½è€—é æ¸¬æ¨¡å‹
    - èª¿æ•´è®Šé »å™¨åƒæ•¸æŸ¥çœ‹é ä¼°èƒ½è€—
    - è‡ªå‹•æ‰¾å‡ºæœ€çœé›»çš„è¨­å®šçµ„åˆ
    - åˆ†æç‰¹å¾µé‡è¦æ€§
    """)

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("**HVAC Analytics** | Spec-Kit Implementation")
st.sidebar.caption(f"ETL Pipeline v1.0")
