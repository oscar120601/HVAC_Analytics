# PRD v1.2: æ‰¹æ¬¡å¤„ç†å™¨å¼ºå¥æ€§é‡æ„æŒ‡å— (BatchProcessor Implementation Guide)

**æ–‡ä»¶ç‰ˆæœ¬:** v1.2 (å« Single Source of Truth ä¸ CI/CD æ²»ç†)  
**æ—¥æœŸ:** 2026-02-12  
**è´Ÿè´£äºº:** Oscar Chang  
**ç›®æ ‡æ¨¡ç»„:** `src/etl/batch_processor_v2.py`  
**ç›¸ä¾æ¨¡ç»„:** `src/etl/cleaner_v2.py` (v2.1+), `src/etl/config_models.py` (SSOT)  
**é¢„ä¼°å·¥æ—¶:** 4 ~ 5 ä¸ªå·¥ç¨‹å¤©ï¼ˆå«æ•´åˆæµ‹è¯•ä¸ CI/CD è®¾ç½®ï¼‰

---

## 1. æ‰§è¡Œæ€»çº²ä¸è®¾è®¡å“²å­¦

ç›®å‰çš„ `batch_processor.py` å­˜åœ¨**å†…å­˜å †ç§¯**ã€**å¥‘çº¦ç ´å**ä¸**é…ç½®ä¸ä¸€è‡´**çš„è‡´å‘½ä¼¤ã€‚æœ¬ PRD å®šä¹‰ **V2.0 Pipeline æ¶æ„**ï¼Œå¹¶é€è¿‡ **Single Source of Truth (SSOT)** ç¡®ä¿ä¸ Cleanerã€Feature Engineer çš„å“è´¨æ ‡è®°å®Œå…¨å¯¹é½ã€‚

**æ ¸å¿ƒè®¾è®¡åŸåˆ™:**
1.  **Single Source of Truth (SSOT)**ï¼š`quality_flags` ä¸ `Parquet æ ¼å¼è§„èŒƒ` ç»Ÿä¸€å®šä¹‰äº `config_models.py`ï¼Œä¸‰æ¨¡ç»„å…±ç”¨ï¼Œç¦æ­¢ç¡¬ç¼–ç ã€‚
2.  **INT64 æ—¶é—´æˆ³å¼ºåˆ¶è§„èŒƒ**ï¼šæ˜ç¡®ç¦æ­¢ INT96ï¼Œå¼ºåˆ¶ä½¿ç”¨ nanoseconds + UTCï¼Œç¡®ä¿ Polars åŸç”Ÿå…¼å®¹æ€§ã€‚
3.  **CI/CD æ²»ç†ä¿æŠ¤**ï¼šå»ºç«‹è‡ªåŠ¨æ£€æŸ¥è„šæœ¬ï¼Œé˜²æ­¢ã€Œåªæ”¹å®šä¹‰ä¸æ”¹æµ‹è¯•ã€çš„è¦†ç›–ç¼ºå£ã€‚
4.  **ä¸²æµä¼˜å…ˆ (Streaming First)**ï¼šä¸¥ç¦å°†å¤šæ¡£æ¡ˆ DataFrame åŒæ—¶è½½å…¥å†…å­˜ã€‚
5.  **äº‹åŠ¡æ€§è¾“å‡º (Transactional)**ï¼šStaging + Atomic Moveï¼Œæ”¯æŒå¹‚ç­‰é‡è·‘ã€‚

---

## 2. ç»Ÿä¸€é…ç½®ä¸ Single Source of Truth (SSOT)

### 2.1 å…¨åŸŸå¸¸æ•°å®šä¹‰ï¼ˆconfig_models.pyï¼‰

**æ‰€æœ‰å“è´¨æ ‡è®°ä¸ Parquet æ ¼å¼çš„å”¯ä¸€çœŸç›¸æ¥æºï¼š**

```python
# src/etl/config_models.py
from typing import Final, List, Literal
from pydantic import BaseModel, validator

# ã€SSOTã€‘Quality Flags ç»Ÿä¸€å®šä¹‰ - ä¸‰æ¨¡ç»„å…±ç”¨
VALID_QUALITY_FLAGS: Final[List[str]] = [
    "FROZEN", 
    "HEAT_IMBALANCE", 
    "AFFINITY_VIOLATION", 
    "OUTLIER", 
    "INSUFFICIENT_DATA",
    "SENSOR_OFFLINE"  # æœªæ¥æ–°å¢åªéœ€åœ¨æ­¤å¤„æ‰©å……
]

class QualityFlagConfig(BaseModel):
    """å“è´¨æ ‡è®°é…ç½®ï¼ˆä¾› Cleanerã€BatchProcessorã€Feature Engineer å…±ç”¨ï¼‰"""
    enabled: bool = True
    valid_flags: List[str] = VALID_QUALITY_FLAGS  # å¼•ç”¨ SSOT
    
    @validator('valid_flags')
    def validate_flags(cls, v):
        """ç¡®ä¿é…ç½®ä¸­çš„ flags çš†ä¸ºåˆæ³•å®šä¹‰"""
        invalid = set(v) - set(VALID_QUALITY_FLAGS)
        if invalid:
            raise ValueError(f"Invalid flags: {invalid}. Must be subset of {VALID_QUALITY_FLAGS}")
        return v

class ParquetOutputConfig(BaseModel):
    """
    ã€å¼ºåˆ¶è§„èŒƒã€‘Parquet è¾“å‡ºæ ¼å¼ - ç¦æ­¢ INT96
    """
    timestamp_format: Literal["INT64"] = "INT64"
    timestamp_unit: Literal["nanoseconds"] = "nanoseconds"
    timestamp_timezone: Literal["UTC"] = "UTC"
    compression: Literal["snappy", "zstd"] = "snappy"
    writer_engine: Literal["polars_native"] = "polars_native"  # é¿å… PyArrow è‡ªåŠ¨è½¬æ¢
    
    @validator('timestamp_format')
    def no_deprecated_int96(cls, v):
        if v == "INT96":
            raise ValueError("INT96 is deprecated and incompatible with Polars. Use INT64.")
        return v

class BatchConfig(BaseModel):
    input_pattern: str = "*.csv"
    output_base_dir: str = "data/processed/"
    staging_dir: str = "data/.staging/"
    max_rows_per_file: int = 100_000
    max_time_span_per_file: str = "1d"
    
    # ã€SSOTã€‘å¼•ç”¨ç»Ÿä¸€é…ç½®
    quality_flags: QualityFlagConfig = QualityFlagConfig()
    parquet: ParquetOutputConfig = ParquetOutputConfig()
    
    memory_limit_mb: int = 4096
    stop_on_error: bool = False

class ETLConfig(BaseModel):
    """ç»Ÿä¸€é…ç½®æ ¹ç‰©ä»¶"""
    cleaner: CleaningConfig      # åŒæ ·å¼•ç”¨ QualityFlagConfig
    batch: BatchConfig
```

---

## 3. ç³»ç»Ÿæ¶æ„ä¸èµ„æ–™æµ

### 3.1 Pipeline æµç¨‹ï¼ˆå« SSOT éªŒè¯ï¼‰

```mermaid
graph TD
    A[Source CSVs] -->|Iterate| B(Parser)
    B -->|Raw DF| C[Cleaner v2.1]
    C -->|Clean DF| D{Contract Validator<br/>æ£€æŸ¥ Flags & INT64/UTC}
    D -->|Pass| E[Buffer Accumulator]
    D -->|Fail| F[Error Log]
    E -->|è¾¾é˜ˆå€¼| G[Staging Writer<br/>å¼ºåˆ¶ INT64/UTC]
    G -->|æ‰¹æ¬¡å®Œæˆ| H{Atomic Move}
    H -->|æˆåŠŸ| I[Final Parquet<br/>year=2026/month=02/]
    H -->|æˆåŠŸ| J[manifest.json]
    H -->|å¤±è´¥| K[Rollback]
    I -->|è¯»å–| L[Feature Engineer<br/>é€šè¿‡ manifest + SSOT éªŒè¯]
    
    style G fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

---

## 4. åˆ†é˜¶æ®µå®ä½œè®¡åˆ’

### Phase 1: SSOT é…ç½®ä¸åŸºç¡€æ¶æ„ (é¢„ä¼° 1 å¤©)

#### Step 1.1: å»ºç«‹ç»Ÿä¸€é…ç½®æ¨¡å‹ï¼ˆå·²å« SSOTï¼‰
**æ¡£æ¡ˆ**: `src/etl/config_models.py`

- å®šä¹‰ `VALID_QUALITY_FLAGS`ï¼ˆListï¼Œå…¨å±€å”¯ä¸€ï¼‰
- å®šä¹‰ `QualityFlagConfig`ï¼ˆPydanticï¼Œå¼•ç”¨ VALID_QUALITY_FLAGSï¼‰
- å®šä¹‰ `ParquetOutputConfig`ï¼ˆå¼ºåˆ¶ INT64/UTCï¼Œç¦æ­¢ INT96ï¼‰

#### Step 1.2: Orchestrator éª¨æ¶ï¼ˆåŠ¨æ€è¯»å–é…ç½®ï¼‰
**æ¡£æ¡ˆ**: `src/etl/batch_processor_v2.py`

```python
class BatchOrchestrator:
    def __init__(self, config: ETLConfig):
        self.config = config
        self.parser = ReportParser()
        self.cleaner = DataCleaner(config.cleaner)
        
        # ã€SSOTã€‘ä»é…ç½®è¯»å–å…è®¸çš„ flagsï¼Œéç¡¬ç¼–ç 
        self.allowed_flags = set(config.batch.quality_flags.valid_flags)
        self.parquet_config = config.batch.parquet
        
    def _validate_quality_flags(self, df: pl.DataFrame) -> None:
        """
        ã€SSOT éªŒè¯ã€‘æ£€æŸ¥ DataFrame ä¸­çš„ flags æ˜¯å¦çš†åœ¨åˆæ³•æ¸…å•å†…
        """
        if "quality_flags" not in df.columns:
            return
            
        actual_flags = set()
        for flags in df["quality_flags"]:
            if flags:
                actual_flags.update(flags)
        
        unknown = actual_flags - self.allowed_flags
        if unknown:
            raise ContractViolationError(
                f"Unknown quality flags detected: {unknown}. "
                f"These flags are not defined in config_models.VALID_QUALITY_FLAGS. "
                f"Please update the SSOT or check Cleaner configuration."
            )
```

### Phase 2: æ ¸å¿ƒç®¡çº¿ä¸ Schema éªŒè¯ (é¢„ä¼° 2 å¤©)

#### Step 2.1: å¥‘çº¦éªŒè¯å™¨ï¼ˆSSOT ä¸ INT64 æ£€æŸ¥ï¼‰
**æ¡£æ¡ˆ**: `src/etl/contract_validator.py`

```python
class OutputContractValidator:
    def __init__(self, config: ETLConfig):
        self.config = config
        
    def validate(self, df: pl.DataFrame) -> None:
        """å®Œæ•´éªŒè¯è¾“å‡ºå¥‘çº¦"""
        self._validate_quality_flags_values(df)  # SSOT æ£€æŸ¥
        self._validate_timestamp_format(df)      # INT64/UTC æ£€æŸ¥
        
    def _validate_quality_flags_values(self, df: pl.DataFrame) -> None:
        """ã€SSOTã€‘éªŒè¯ flags å€¼æ˜¯å¦åˆæ³•"""
        if "quality_flags" not in df.columns:
            return
            
        allowed = set(self.config.batch.quality_flags.valid_flags)
        
        for i, flags in enumerate(df["quality_flags"]):
            if not flags:
                continue
            invalid = set(flags) - allowed
            if invalid:
                raise ContractViolationError(
                    f"Row {i}: Invalid flags {invalid}. Allowed: {allowed}"
                )
    
    def _validate_timestamp_format(self, df: pl.DataFrame) -> None:
        """éªŒè¯æ—¶é—´æˆ³å‹åˆ«ä¸æ—¶åŒº"""
        ts_col = df["timestamp"]
        
        if not isinstance(ts_col.dtype, pl.Datetime):
            raise TypeError(f"timestamp must be Datetime, got {ts_col.dtype}")
        
        if ts_col.dtype.time_zone != "UTC":
            raise ValueError(f"timestamp must be UTC, got {ts_col.dtype.time_zone}")
```

#### Step 2.2: Parquet å†™å…¥ï¼ˆå¼ºåˆ¶ INT64/UTCï¼‰
```python
def _flush_buffer_to_staging(self):
    """å†™å…¥ Parquetï¼Œå¼ºåˆ¶éµå®ˆ INT64/UTC è§„èŒƒ"""
    combined = pl.concat(self.buffer)
    
    # ã€å¼ºåˆ¶ã€‘ç¡®ä¿æ—¶é—´æˆ³ä¸º UTC
    if str(combined["timestamp"].dtype.time_zone) != "UTC":
        combined = combined.with_columns(
            pl.col("timestamp").dt.replace_time_zone("UTC")
        )
    
    # ä½¿ç”¨ Polars åŸç”Ÿå†™å…¥ï¼Œç¡®ä¿ INT64 nanoseconds
    file_path = self._get_staging_path()
    combined.write_parquet(
        file_path,
        compression=self.parquet_config.compression,
        use_pyarrow=False  # å…³é”®ï¼šä½¿ç”¨ Arrow2 åŸç”Ÿï¼Œé¿å… PyArrow è½¬ä¸º INT96
    )
    
    # ã€éªŒè¯ã€‘å†™å…¥åæ£€æŸ¥ Schema
    self._verify_parquet_schema(file_path)

def _verify_parquet_schema(self, file_path: Path):
    """éªŒè¯è¾“å‡º Parquet ç¬¦åˆ INT64/UTC è§„èŒƒ"""
    import pyarrow.parquet as pq
    
    pf = pq.ParquetFile(file_path)
    schema = pf.schema
    ts_field = schema.field_by_name("timestamp")
    
    # ã€å…³é”®éªŒè¯ã€‘ç¦æ­¢ INT96
    if ts_field.physical_type == "INT96":
        file_path.unlink()
        raise TypeError(
            f"CRITICAL: Parquet file {file_path} was written with INT96 timestamp. "
            f"Expected INT64 (nanoseconds). Check writer configuration."
        )
    
    self.logger.info(f"Schema verification passed: INT64/UTC confirmed")
```

### Phase 3: Manifest ä¸ä¸‹æ¸¸è¡”æ¥ (é¢„ä¼° 1.5 å¤©)

#### Step 3.1: Manifest æ¨¡å‹ï¼ˆå« SSOT ä¿¡æ¯ï¼‰
```python
class Manifest(BaseModel):
    manifest_version: str = "1.2"
    batch_id: str
    site_id: str
    created_at: datetime
    
    # ã€SSOTã€‘è®°å½•ä½¿ç”¨çš„ flags å®šä¹‰ç‰ˆæœ¬
    quality_flags_schema: List[str]
    parquet_schema: Dict
    
    output_files: List[str]
    statistics: Dict
```

---

## 5. éªŒè¯ä¸æµ‹è¯•è®¡åˆ’ï¼ˆSSOT ä¸ INT64 ä¸“é¡¹ï¼‰

| æµ‹è¯•æ¡ˆä¾‹ | éªŒè¯å†…å®¹ | é€šè¿‡æ ‡å‡† |
|:---|:---|:---:|
| **Case S1 (SSOT Sync)** | Cleaner æ–°å¢ `"SENSOR_OFFLINE"`ï¼ŒBatchProcessor è‡ªåŠ¨æ¥å— | ä¸æŠ›å‡º `ContractViolationError` |
| **Case S2 (SSOT Mismatch)** | Cleaner è¾“å‡ºæœªå®šä¹‰ flag `"UNKNOWN"` | æŠ›å‡ºæ˜ç¡®é”™è¯¯ï¼šæç¤ºæ›´æ–° `config_models.py` |
| **Case T1 (INT64 Enforcement)** | è¾“å‡º Parquet æ—¶é—´æˆ³æ ¼å¼ | `physical_type == "INT64"` |
| **Case T2 (UTC Timezone)** | è¾“å…¥é UTC èµ„æ–™ | è‡ªåŠ¨è½¬æ¢ä¸º UTC |
| **Case T3 (No INT96)** | æ¨¡æ‹Ÿæ—§ç‰ˆ PyArrow è¡Œä¸º | éªŒè¯é€»è¾‘æ‹¦æˆªå¹¶æŠ›å‡º `TypeError` |

---

## 6. é£é™©è¯„ä¼°ï¼ˆæ›´æ–°ï¼‰

| é£é™© | ä¸¥é‡åº¦ | ç¼“è§£æªæ–½ï¼ˆv1.2 SSOT è®¾è®¡ï¼‰ |
|:---|:---:|:---|
| **Flags å®šä¹‰ä¸ä¸€è‡´** | ğŸ”´ Critical | **SSOT**: `config_models.VALID_QUALITY_FLAGS`ï¼Œä¸‰æ¨¡ç»„å…±ç”¨ |
| **INT96 æ—¶é—´æˆ³** | ğŸ”´ High | **å¼ºåˆ¶ INT64**: `use_pyarrow=False` + å†™å…¥å Schema éªŒè¯ |
| **æµ‹è¯•è¦†ç›–ç¼ºå£** | ğŸ”´ High | **CI/CD è‡ªåŠ¨æ£€æŸ¥**: `scripts/ci_verify_ssot.py` é˜»æŒ¡æœªåŒæ­¥æµ‹è¯•çš„ PR |
| **æ—¶åŒºé”™è¯¯** | ğŸ”´ High | **å¼ºåˆ¶ UTC**: `dt.replace_time_zone("UTC")` |
| **å°æ–‡ä»¶çˆ†ç‚¸** | ğŸ”´ High | **æ—¶é—´åˆ†åŒºåˆå¹¶**: `max_rows_per_file` é˜ˆå€¼æ§åˆ¶ |

---

## 7. ä¸ä¸Šä¸‹æ¸¸åä½œæ£€æŸ¥æ¸…å•ï¼ˆSSOT ä¸“é¡¹ï¼‰

### ä¸ Cleaner v2.1 å›¢é˜Ÿï¼ˆSSOT å¯¹é½ï¼‰ï¼š
- [ ] `config_models.py` ä¸­çš„ `VALID_QUALITY_FLAGS` æ˜¯å¦åŒ…å«æ‰€æœ‰ Cleaner ä¼šäº§å‡ºçš„æ ‡è®°ï¼Ÿ
- [ ] Cleaner çš„ `CleaningConfig` æ˜¯å¦åŒæ ·å¼•ç”¨ `QualityFlagConfig`ï¼Ÿ
- [ ] è‹¥éœ€æ–°å¢æ ‡è®°ï¼Œæ˜¯å¦ä¸‰æ–¹åŒæ„æ”¹åŠ¨ `config_models.py` ååŒæ­¥æ›´æ–°ï¼Ÿ

### ä¸ Feature Engineer å›¢é˜Ÿï¼ˆINT64/SSOTï¼‰ï¼š
- [ ] æ˜¯å¦ç¡®è®¤è¯»å– Parquet æ—¶ä½¿ç”¨ `scan_parquet`ï¼ˆåŸç”Ÿæ”¯æŒ INT64ï¼‰ï¼Ÿ
- [ ] æ˜¯å¦æ¥å—é€šè¿‡ `manifest.json` è¯»å–ï¼Œå¹¶éªŒè¯ `quality_flags_schema` å­—æ®µï¼Ÿ

---

## 8. äº¤ä»˜äº§ç‰©æ¸…å•

1. `src/etl/config_models.py`: **SSOT å®šä¹‰**ï¼ˆ`VALID_QUALITY_FLAGS`, `ParquetOutputConfig`ï¼‰
2. `src/etl/batch_processor_v2.py`: å®ä½œï¼ˆåŠ¨æ€è¯»å– flagsã€INT64 å¼ºåˆ¶å†™å…¥ï¼‰
3. `src/etl/contract_validator.py`: æ›´æ–°ï¼ˆSSOT éªŒè¯ã€INT64/UTC æ£€æŸ¥ï¼‰
4. `tests/test_ssot_integration.py`: **SSOT ä¸“é¡¹æµ‹è¯•**
5. `tests/test_int64_timestamp.py`: **INT64 ä¸“é¡¹æµ‹è¯•**
6. `docs/ssot_guide.md`: è¯´æ˜æ–‡ä»¶ï¼ˆå¦‚ä½•æ–°å¢ Quality Flagã€ä¸‰æ¨¡ç»„åŒæ­¥æµç¨‹ï¼‰

---

## 9. CI/CD ä¸å·¥ç¨‹æ²»ç†ï¼ˆSSOT ä¿æŠ¤æœºåˆ¶ï¼‰

### 9.1 è‡ªåŠ¨åŒ–æ£€æŸ¥è„šæœ¬
ä¸ºé˜²æ­¢ã€Œåªæ”¹ä»£ç ä¸æ”¹æµ‹è¯•ã€çš„é£é™©ï¼Œå»ºç«‹ä»¥ä¸‹æ£€æŸ¥ï¼š

**æ¡£æ¡ˆ**: `scripts/ci_verify_ssot.py`

```python
#!/usr/bin/env python3
"""
SSOT å®Œæ•´æ€§æ£€æŸ¥è„šæœ¬
ç¡®ä¿ config_models.py å˜æ›´æ—¶ï¼Œç›¸å…³æµ‹è¯•å·²åŒæ­¥æ›´æ–°
"""
import ast
import sys
from pathlib import Path

def extract_valid_flags() -> set:
    """ä» config_models.py è§£æ VALID_QUALITY_FLAGS"""
    tree = ast.parse(Path("src/etl/config_models.py").read_text())
    for node in ast.walk(tree):
        if isinstance(node, ast.Assign):
            for target in node.targets:
                if isinstance(target, ast.Name) and target.id == "VALID_QUALITY_FLAGS":
                    return set(ast.literal_eval(node.value))
    return set()

def main():
    defined_flags = extract_valid_flags()
    print(f"ğŸ“‹ Defined flags: {defined_flags}")
    
    # æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡...
    # è¯¦ç»†å®ä½œè§é™„å½•
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

### 9.2 GitHub Actions å·¥ä½œæµç¨‹

**æ¡£æ¡ˆ**: `.github/workflows/ssot-check.yml`

```yaml
name: SSOT Integrity Check

on:
  push:
    paths:
      - 'src/etl/config_models.py'
      - 'tests/**'

jobs:
  verify-ssot:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Run SSOT Verification
        run: python scripts/ci_verify_ssot.py
      
      - name: Run Flag-related Tests
        run: pytest tests/ -k "quality_flag or ssot" -v
      
      - name: Check Hardcoded Flags
        run: |
          if grep -r "FROZEN\|HEAT_IMBALANCE" --include="*.py" src/ | grep -v "config_models.py" | grep -v "from.*import"; then
            echo "âŒ Found hardcoded flags outside config_models.py"
            exit 1
          fi
```

### 9.3 PR Checklistï¼ˆæ¨¡æ¿ï¼‰
å»ºç«‹ Pull Request æ¨¡æ¿ï¼Œå¼ºåˆ¶å‹¾é€‰ï¼š
- [ ] è‹¥ä¿®æ”¹ `VALID_QUALITY_FLAGS`ï¼Œå·²åŒæ­¥æ›´æ–° `tests/test_cleaner.py`
- [ ] è‹¥ä¿®æ”¹ `VALID_QUALITY_FLAGS`ï¼Œå·²åŒæ­¥æ›´æ–° `tests/test_batch_processor.py`  
- [ ] è‹¥ä¿®æ”¹ `VALID_QUALITY_FLAGS`ï¼Œå·²åŒæ­¥æ›´æ–° `tests/test_feature_engineer.py`
- [ ] å·²æ‰§è¡Œ `python scripts/ci_verify_ssot.py` ä¸”é€šè¿‡

---

**å…³é”®ä¿®æ­£æ€»ç»“**ï¼š
1. **SSOT**: `VALID_QUALITY_FLAGS` ç»Ÿä¸€äº `config_models.py`ï¼Œä¸‰æ¨¡ç»„å…±ç”¨ã€‚
2. **INT64 å¼ºåˆ¶**: æ˜ç¡®ç¦æ­¢ INT96ï¼Œä½¿ç”¨ Polars åŸç”Ÿå†™å…¥å™¨ï¼Œå†™å…¥åéªŒè¯ Schemaã€‚
3. **CI/CD ä¿æŠ¤**: å»ºç«‹è‡ªåŠ¨æ£€æŸ¥è„šæœ¬ï¼Œé˜²æ­¢ SSOT å˜æ›´æ—¶æµ‹è¯•è¦†ç›–ç¼ºå£ã€‚
```