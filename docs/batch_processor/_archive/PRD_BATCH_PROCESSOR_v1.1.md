# PRD v1.1: æ‰¹æ¬¡è™•ç†å™¨å¼·å¥æ€§é‡æ§‹æŒ‡å— (BatchProcessor Implementation Guide)

**æ–‡ä»¶ç‰ˆæœ¬:** v1.1 (å« Manifest æ©Ÿåˆ¶èˆ‡ Schema å¥‘ç´„é©—è­‰)  
**æ—¥æœŸ:** 2026-02-12  
**è² è²¬äºº:** Oscar Chang  
**ç›®æ¨™æ¨¡çµ„:** `src/etl/batch_processor_v2.py`  
**ç›¸ä¾æ¨¡çµ„:** `src/etl/cleaner_v2.py` (v2.1+), `src/etl/feature_engineer.py` (v1.2+)  
**é ä¼°å·¥æ™‚:** 4 ~ 5 å€‹å·¥ç¨‹å¤©ï¼ˆå«æ•´åˆæ¸¬è©¦ï¼‰

---

## 1. åŸ·è¡Œç¸½ç¶±èˆ‡è¨­è¨ˆå“²å­¸

ç›®å‰çš„ `batch_processor.py` å­˜åœ¨**è¨˜æ†¶é«”å †ç©**èˆ‡**å¥‘ç´„ç ´å£**çš„è‡´å‘½å‚·ã€‚æœ¬ PRD å®šç¾© **V2.0 Pipeline æ¶æ§‹**ï¼Œå°‡å…¶å¾ã€Œè³‡æ–™å›¤ç©è€…ã€è½‰å‹ç‚ºã€Œé«˜æ•ˆæŒ‡æ®å®˜ã€ï¼Œä¸¦ä½œç‚º Cleaner èˆ‡ Feature Engineer é–“çš„**å¥‘ç´„å®ˆé–€å“¡**ã€‚

**æ ¸å¿ƒè¨­è¨ˆåŸå‰‡:**
1.  **ä¸²æµå„ªå…ˆ (Streaming First)**ï¼šåš´ç¦å°‡å¤šæª”æ¡ˆ DataFrame åŒæ™‚è¼‰å…¥è¨˜æ†¶é«”ã€‚è™•ç†å®Œä¸€å€‹ï¼Œç«‹å³é‡‹æ”¾ã€‚
2.  **æ™‚é–“åˆ†å€èšåˆ (Time-Partitioned)**ï¼šé¿å…ä¸€å°ä¸€ CSV è½‰ Parquet å°è‡´å°æ–‡ä»¶çˆ†ç‚¸ï¼Œæ”¹ç”¨åˆ—æ•¸/æ™‚é–“é–¾å€¼åˆä½µå¯«å…¥ã€‚
3.  **å¥‘ç´„å®ˆè­· (Contract Guardian)**ï¼šé©—è­‰ Cleaner è¼¸å‡ºæ˜¯å¦ç¬¦åˆ `Output Contract`ï¼Œæ””æˆªå‹åˆ¥éŒ¯èª¤ï¼ˆå¦‚ `quality_flags` è¢«èª¤è½‰ç‚º Float64ï¼‰ã€‚
4.  **äº‹å‹™æ€§è¼¸å‡º (Transactional Output)**ï¼šStaging + Atomic Moveï¼Œæ‰¹æ¬¡å¤±æ•—æ™‚è‡ªå‹•å›æ»¾ï¼Œä¸æ±¡æŸ“ä¸‹æ¸¸ã€‚
5.  **å¯è¿½æº¯æ€§ (Traceability)**ï¼šé€é **Manifestï¼ˆæ¸…å–®ï¼‰æ©Ÿåˆ¶**è¨˜éŒ„æ‰¹æ¬¡è¡€ç·£ï¼Œä¾› Feature Engineer ç²¾æº–è®€å–ã€‚

---

## 2. ç³»çµ±æ¶æ§‹ï¼šPipeline æ¨¡å¼ï¼ˆå« Manifestï¼‰

### 2.1 è³‡æ–™æµèˆ‡é—œéµè®Šæ›´

```mermaid
graph TD
    A[Source CSVs] -->|Iterate| B(Parser)
    B -->|Raw DF| C[Cleaner v2.1]
    C -->|Clean DF| D{Schema Validator<br/>æª¢æŸ¥ quality_flags ç­‰}
    D -->|Pass| E[Buffer Accumulator<br/>æ™‚é–“åˆ†å€ç·©è¡]
    D -->|Fail| F[Error Log<br/>è¨˜éŒ„å¤±æ•—æª”æ¡ˆ]
    E -->|é”é–¾å€¼| G[Staging Writer<br/>.staging/{batch_id}]
    G -->|æ‰¹æ¬¡å®Œæˆ| H{Atomic Move<br/>åŸå­æ€§ç§»å‹•}
    H -->|æˆåŠŸ| I[Final Dataset<br/>processed/{site}/year=2026/...]
    H -->|æˆåŠŸ| J[manifest.json<br/>æ¸…å–®èˆ‡çµ±è¨ˆ]
    H -->|å¤±æ•—| K[Rollback<br/>æ¸…ç† Staging]
    I -->|è®€å–| L[Feature Engineer<br/>é€é manifest è€Œé glob]
```

### 2.2 é—œéµè®Šæ›´å°ç…§ï¼ˆvs Legacyï¼‰

| åŠŸèƒ½æ¨¡çµ„ | èˆŠç‰ˆå¯¦ä½œ (Legacy) | æ–°ç‰ˆå¯¦ä½œ (Pipeline v1.1) | é¢¨éšª/å„ªå‹¢ |
|:---|:---|:---|:---|
| **è¨˜æ†¶é«”ç®¡ç†** | `List[DataFrame]` ç´¯ç© | **Process-and-Dump** + ç·©è¡å€ | OOM é¢¨éšªæ­¸é›¶ |
| **è¼¸å‡ºæª”æ¡ˆç­–ç•¥** | ä¸€å°ä¸€ CSVâ†’Parquet | **æ™‚é–“åˆ†å€åˆä½µ**ï¼ˆ100MB/é–¾å€¼ï¼‰ | é¿å…å°æ–‡ä»¶çˆ†ç‚¸ï¼ˆ50è¬æª”æ¡ˆå•é¡Œï¼‰ |
| **å‹åˆ¥è™•ç†** | å¼·åˆ¶å…¨è½‰ Float64 | **Schema å¥‘ç´„é©—è­‰**ï¼ˆä¿ç•™ List[str]ï¼‰ | ä¸æœƒæŠ¹é™¤ `quality_flags` |
| **è¼¸å‡ºäº‹å‹™æ€§** | ç›´æ¥å¯«å…¥æ­£å¼ç›®éŒ„ | **Staging + Atomic Move** | å¤±æ•—æ™‚ç„¡é«’è³‡æ–™ï¼Œæ”¯æ´å†ªç­‰é‡è·‘ |
| **ä¸‹æ¸¸éŠœæ¥** | Feature Engineer glob æœå°‹ | **Manifest æ¸…å–®æ©Ÿåˆ¶** | ç²¾æº–è®€å–ã€æ”¯æ´è¡€ç·£è¿½è¹¤ã€å»é‡ |
| **æ™‚é–“é€£çºŒæ€§** | ç„¡ä¿è­‰ | **å¼·åˆ¶æ’åº + è·¨æª”å»é‡** | Feature Engineer Lag è¨ˆç®—æ­£ç¢º |

---

## 3. è¼¸å…¥è¼¸å‡ºå¥‘ç´„ï¼ˆInterface Contractsï¼‰

### 3.1 ä¸Šæ¸¸è¼¸å…¥ï¼ˆFrom Cleaner v2.1ï¼‰

BatchProcessor **åš´æ ¼é©—è­‰** Cleaner è¼¸å‡ºï¼Œç¢ºä¿ç¬¦åˆ Feature Engineer é æœŸï¼š

| æ¬„ä½ | é æœŸå‹åˆ¥ | é©—è­‰é‚è¼¯ | å¤±æ•—è™•ç† |
|:---|:---|:---|:---|
| `timestamp` | `pl.Datetime(time_zone="UTC")` | å¿…é ˆå­˜åœ¨ã€ç„¡é‡è¤‡ã€åš´æ ¼éå¢ | æ‹‹å‡º `ContractViolationError` |
| `quality_flags` | `pl.List(pl.Utf8)` | ä¸å¯ç‚º Nullã€ä¸å¯è¢«è½‰ç‚º Float64 | æ‹‹å‡º `TypeError`ï¼Œè¨˜éŒ„éŒ¯èª¤æª”æ¡ˆ |
| è³‡æ–™æ¬„ä½ | `pl.Float64`ï¼ˆSI åˆ¶å–®ä½ï¼‰ | ç„¡æ¥µç«¯ç•°å¸¸å€¼ï¼ˆå¦‚ 1e20ï¼‰ | æ¨™è¨˜è­¦å‘Šä½†ç¹¼çºŒè™•ç† |
| æ™‚é–“é€£çºŒæ€§ | é–“éš”æ†å®š | æª¢æŸ¥èˆ‡ `resample_interval` ä¸€è‡´ | è¨˜éŒ„ `INSUFFICIENT_DATA` æ¨™è¨˜ |

### 3.2 ä¸‹æ¸¸è¼¸å‡ºï¼ˆTo Feature Engineerï¼‰

BatchProcessor è¼¸å‡º**ä¿è­‰**ä»¥ä¸‹è¦æ ¼ï¼š

```yaml
è¼¸å‡ºç›®éŒ„çµæ§‹:
data/processed/
â”œâ”€â”€ {site_id}/
â”‚   â”œâ”€â”€ year=2026/
â”‚   â”‚   â”œâ”€â”€ month=02/
â”‚   â”‚   â”‚   â”œâ”€â”€ part-0001.parquet  (100MB ~ 1GB)
â”‚   â”‚   â”‚   â””â”€â”€ part-0002.parquet
â”‚   â”‚   â””â”€â”€ month=03/
â”‚   â”‚       â””â”€â”€ ...
â””â”€â”€ manifests/
    â””â”€â”€ manifest-{batch_id}-{timestamp}.json
```

**Parquet Schema è¦ç¯„**ï¼š
- `timestamp`: `INT64 (nanoseconds)` + UTC æ™‚å€è³‡è¨Šï¼ˆæˆ– `INT96`ï¼‰
- `quality_flags`: Parquet Logical Type `LIST<STRING>`ï¼Œå°æ‡‰ Polars `List[Utf8]`
- å…¶ä»–æ¬„ä½: `FLOAT64`

**Manifest æª”æ¡ˆæ ¼å¼**ï¼š
```json
{
  "manifest_version": "1.0",
  "batch_id": "550e8400-e29b-41d4-a716-446655440000",
  "site_id": "CGMH-TY",
  "created_at": "2026-02-12T14:30:00Z",
  "input_files_count": 150,
  "output_files": [
    "year=2026/month=02/part-0001.parquet",
    "year=2026/month=02/part-0002.parquet"
  ],
  "statistics": {
    "total_rows": 1500000,
    "time_range": ["2026-02-01T00:00:00Z", "2026-02-28T23:55:00Z"],
    "quality_flags_distribution": {
      "FROZEN": 150,
      "HEAT_IMBALANCE": 2300,
      "INSUFFICIENT_DATA": 45
    }
  },
  "schema_hash": "sha256:a1b2c3d4...",  // ç”¨æ–¼å¿«å–å¤±æ•ˆæª¢æ¸¬
  "checksum": "sha256:e5f6g7h8..."      // å®Œæ•´æ€§é©—è­‰
}
```

---

## 4. åˆ†éšæ®µå¯¦ä½œè¨ˆç•«

### Phase 1: åŸºç¤æ¶æ§‹èˆ‡é…ç½® (é ä¼° 1 å¤©)

#### Step 1.1: çµ±ä¸€é…ç½®æ¨¡å‹ï¼ˆETLConfigï¼‰
**æª”æ¡ˆ**: `src/etl/config_models.py`

```python
from pydantic import BaseModel, validator, root_validator
from typing import Literal, Optional
import psutil

class BatchConfig(BaseModel):
    input_pattern: str = "*.csv"
    output_base_dir: str = "data/processed/"
    staging_dir: str = "data/.staging/"  # æš«å­˜å€ï¼ˆäº‹å‹™æ€§ï¼‰
    
    # ã€é—œéµã€‘è¼¸å‡ºæ§åˆ¶ï¼ˆé¿å…å°æ–‡ä»¶çˆ†ç‚¸ï¼‰
    max_rows_per_file: int = 100_000      # å–®ä¸€ Parquet æª”æ¡ˆæœ€å¤§åˆ—æ•¸
    max_time_span_per_file: str = "1d"    # å–®ä¸€æª”æ¡ˆæœ€å¤§æ™‚é–“è·¨åº¦
    output_format: Literal["parquet"] = "parquet"
    compression: str = "snappy"
    
    # è¨˜æ†¶é«”é˜²è­·
    memory_limit_mb: int = 4096
    memory_action: Literal["warn", "throttle", "stop"] = "throttle"
    
    # éŒ¯èª¤è™•ç†
    stop_on_error: bool = False
    max_retry_per_file: int = 3
    
    # ã€é—œéµã€‘Manifest æ©Ÿåˆ¶
    manifest_enabled: bool = True
    manifest_dir: str = "data/manifests/"

class ETLConfig(BaseModel):
    """çµ±ä¸€é…ç½®ï¼Œç¢ºä¿ Cleaner èˆ‡ BatchProcessor ç›¸å®¹"""
    cleaner: CleaningConfig      # è¦‹ Cleaner PRD v2.1
    batch: BatchConfig
    
    @root_validator
    def check_compatibility(cls, values):
        """é©—è­‰ Batch è¼¸å‡ºèˆ‡ Cleaner è¨­å®šç›¸å®¹"""
        cleaner = values.get('cleaner')
        batch = values.get('batch')
        # ç¢ºä¿æ™‚é–“è§£æåº¦ä¸€è‡´ï¼ˆé¿å… Cleaner è¼¸å‡º 5m ä½† Batch ä»¥ç‚ºæ˜¯ 1hï¼‰
        if hasattr(batch, 'time_resolution') and batch.time_resolution != cleaner.resample_interval:
            raise ValueError(f"Batch time_resolution ({batch.time_resolution}) "
                           f"must match Cleaner resample_interval ({cleaner.resample_interval})")
        return values
```

#### Step 1.2: å»ºç«‹ Orchestrator éª¨æ¶ï¼ˆå«è¨˜æ†¶é«”ç›£æ§ï¼‰
**æª”æ¡ˆ**: `src/etl/batch_processor_v2.py`

```python
import psutil
import time
from pathlib import Path
from typing import List, Set, Dict
import polars as pl
from uuid import uuid4

class BatchOrchestrator:
    def __init__(self, config: ETLConfig):
        self.config = config
        self.parser = ReportParser()
        self.cleaner = DataCleaner(config.cleaner)
        self.batch_config = config.batch
        
        # ç·©è¡å€ï¼ˆç´¯ç©å°æª”æ¡ˆï¼Œé”é–¾å€¼å¾Œå¯«å…¥ï¼‰
        self.buffer: List[pl.DataFrame] = []
        self.buffer_rows = 0
        self.current_batch_id = str(uuid4())
        
        # æ™‚é–“è¿½è¹¤ï¼ˆç”¨æ–¼æª”æ¡ˆå‘½åèˆ‡å»é‡ï¼‰
        self.seen_timestamps: Set[str] = set()  # å¯é¸ï¼šè·¨æª”å»é‡
        
        # çµ±è¨ˆ
        self.stats = {
            "processed_files": 0,
            "failed_files": [],
            "total_rows": 0,
            "quality_flags_dist": {}
        }
        
    def _check_memory(self):
        """è¨˜æ†¶é«”ç›£æ§èˆ‡é˜²è­·"""
        mem_mb = psutil.Process().memory_info().rss / 1024 / 1024
        if mem_mb > self.batch_config.memory_limit_mb:
            if self.batch_config.memory_action == "stop":
                raise MemoryError(f"Memory limit exceeded: {mem_mb:.0f}MB")
            elif self.batch_config.memory_action == "throttle":
                self.logger.warning(f"High memory usage: {mem_mb:.0f}MB, throttling...")
                time.sleep(1)
```

### Phase 2: æ ¸å¿ƒç®¡ç·šå¯¦ä½œï¼ˆSchema é©—è­‰èˆ‡äº‹å‹™æ€§ï¼‰(é ä¼° 2 å¤©)

#### Step 2.1: Schema å¥‘ç´„é©—è­‰å™¨
**æª”æ¡ˆ**: `src/etl/contract_validator.py`

```python
class OutputContractValidator:
    """é©—è­‰ Cleaner è¼¸å‡ºç¬¦åˆ Feature Engineer é æœŸ"""
    
    REQUIRED_COLUMNS = ["timestamp", "quality_flags"]
    ALLOWED_FLAG_TYPES = ["FROZEN", "HEAT_IMBALANCE", "AFFINITY_VIOLATION", 
                          "OUTLIER", "INSUFFICIENT_DATA"]
    
    @classmethod
    def validate(cls, df: pl.DataFrame) -> None:
        # 1. æª¢æŸ¥å¿…è¦æ¬„ä½
        missing = set(cls.REQUIRED_COLUMNS) - set(df.columns)
        if missing:
            raise ContractViolationError(f"Missing required columns: {missing}")
        
        # 2. ã€é—œéµã€‘é©—è­‰ quality_flags å‹åˆ¥ï¼ˆé˜²æ­¢è¢«è½‰ç‚º Float64ï¼‰
        qf_dtype = df["quality_flags"].dtype
        if qf_dtype != pl.List(pl.Utf8):
            raise TypeError(
                f"Column 'quality_flags' must be List[str] (Polars: List[Utf8]), "
                f"got {qf_dtype}. This usually means accidental casting to numeric."
            )
        
        # 3. é©—è­‰æ™‚é–“æˆ³
        if df["timestamp"].dtype != pl.Datetime:
            raise TypeError(f"timestamp must be Datetime, got {df['timestamp'].dtype}")
        
        # 4. é©—è­‰ç„¡æ¥µç«¯æœªä¾†è³‡æ–™ï¼ˆé˜² Data Leakageï¼‰
        if df["timestamp"].max() > datetime.now(timezone.utc) + timedelta(minutes=5):
            raise ValueError("Data contains future timestamps > 5 minutes from now")
```

#### Step 2.2: å–®æª”è™•ç†åŸå­å‡½æ•¸ï¼ˆå«ç·©è¡ç´¯ç©ï¼‰
```python
def process_single_file(self, file_path: Path) -> BatchResult:
    """
    è™•ç†å–®ä¸€æª”æ¡ˆï¼Œå¯«å…¥ç·©è¡å€ï¼Œé”é–¾å€¼æ™‚è§¸ç™¼å¯«å…¥ Staging
    """
    try:
        # 1. è§£æ
        raw_df = self.parser.parse_file(str(file_path))
        
        # 2. æ¸…æ´—
        clean_df = self.cleaner.clean(raw_df)
        
        # 3. ã€é—œéµã€‘Schema å¥‘ç´„é©—è­‰ï¼ˆæ””æˆªå‹åˆ¥éŒ¯èª¤ï¼‰
        OutputContractValidator.validate(clean_df)
        
        # 4. æ™‚é–“æ’åºèˆ‡å»é‡ï¼ˆç¢ºä¿ Feature Engineer Lag è¨ˆç®—æ­£ç¢ºï¼‰
        clean_df = clean_df.sort("timestamp")
        if self.batch_config.deduplicate_timestamps:
            clean_df = clean_df.unique(subset=["timestamp"], keep="first")
        
        # 5. ç´¯ç©åˆ°ç·©è¡å€ï¼ˆæ§åˆ¶è¼¸å‡ºæª”æ¡ˆå¤§å°ï¼‰
        self._accumulate_to_buffer(clean_df, file_path.stem)
        
        self.stats["processed_files"] += 1
        return BatchResult(status="success", rows=len(clean_df))
        
    except ContractViolationError as e:
        self.logger.error(f"Contract violation in {file_path}: {e}")
        self.stats["failed_files"].append({"file": str(file_path), "error": str(e)})
        return BatchResult(status="contract_failed", error=str(e))
    except Exception as e:
        self.logger.error(f"Processing failed {file_path}: {e}")
        if self.batch_config.stop_on_error:
            raise
        self.stats["failed_files"].append({"file": str(file_path), "error": str(e)})
        return BatchResult(status="failed", error=str(e))

def _accumulate_to_buffer(self, df: pl.DataFrame, source_name: str):
    """
    ç´¯ç©è³‡æ–™åˆ°ç·©è¡å€ï¼Œé”åˆ°åˆ—æ•¸æˆ–æ™‚é–“é–¾å€¼æ™‚å¯«å…¥ Staging
    """
    self.buffer.append(df)
    self.buffer_rows += len(df)
    
    # æª¢æŸ¥æ˜¯å¦é”å¯«å…¥é–¾å€¼
    if self.buffer_rows >= self.batch_config.max_rows_per_file:
        self._flush_buffer_to_staging()
    
    # è¨˜æ†¶é«”æª¢æŸ¥
    self._check_memory()
```

#### Step 2.3: Staging å¯«å…¥èˆ‡äº‹å‹™æ€§ï¼ˆAtomic Moveï¼‰
```python
def _flush_buffer_to_staging(self):
    """å°‡ç·©è¡å€å¯«å…¥ Staging ç›®éŒ„"""
    if not self.buffer:
        return
    
    # åˆä½µç·©è¡å€
    combined = pl.concat(self.buffer)
    
    # æ™‚é–“åˆ†å€è·¯å¾‘ï¼šyear=2026/month=02/part-{uuid}.parquet
    min_ts = combined["timestamp"].min()
    year, month = min_ts.year, min_ts.month
    part_file = f"part-{uuid4().hex[:8]}.parquet"
    
    staging_path = Path(self.batch_config.staging_dir) / self.current_batch_id / f"year={year}" / f"month={month:02d}"
    staging_path.mkdir(parents=True, exist_ok=True)
    
    file_path = staging_path / part_file
    
    # å¯«å…¥ Parquetï¼ˆä¿ç•™ List[str] å‹åˆ¥ï¼‰
    combined.write_parquet(
        file_path,
        compression=self.batch_config.compression,
        use_pyarrow=True  # ç¢ºä¿ List å‹åˆ¥æ­£ç¢º
    )
    
    self.logger.info(f"Flushed {len(combined)} rows to {file_path}")
    
    # æ¸…ç©ºç·©è¡å€
    self.buffer = []
    self.buffer_rows = 0

def finalize_batch(self) -> Manifest:
    """
    æ‰¹æ¬¡å®Œæˆï¼šåŸå­æ€§ç§»å‹• Staging â†’ Finalï¼Œç”¢ç”Ÿ Manifest
    """
    try:
        # 1. æ¸…ç©ºæœ€å¾Œç·©è¡å€
        self._flush_buffer_to_staging()
        
        # 2. ç”¢ç”Ÿ Manifest
        manifest = self._generate_manifest()
        
        # 3. ã€é—œéµã€‘åŸå­æ€§ç§»å‹•ï¼šStaging â†’ Final
        staging_base = Path(self.batch_config.staging_dir) / self.current_batch_id
        final_base = Path(self.batch_config.output_base_dir)
        
        if staging_base.exists():
            # ç§»å‹•æ‰€æœ‰æª”æ¡ˆåˆ°æ­£å¼ç›®éŒ„
            for src_file in staging_base.rglob("*.parquet"):
                rel_path = src_file.relative_to(staging_base)
                dst_file = final_base / rel_path
                dst_file.parent.mkdir(parents=True, exist_ok=True)
                src_file.replace(dst_file)  # åŸå­æ€§ç§»å‹•ï¼ˆåŒæª”æ¡ˆç³»çµ±å…§ï¼‰
            
            # å¯«å…¥ Manifest
            manifest_path = Path(self.batch_config.manifest_dir) / f"manifest-{self.current_batch_id}.json"
            manifest_path.parent.mkdir(parents=True, exist_ok=True)
            manifest_path.write_text(manifest.json(), encoding='utf-8')
            
            # 4. æ¸…ç† Staging
            import shutil
            shutil.rmtree(staging_base)
            
            self.logger.info(f"Batch {self.current_batch_id} committed successfully")
            return manifest
            
    except Exception as e:
        self.logger.error(f"Batch finalization failed: {e}")
        self._rollback_staging()
        raise

def _rollback_staging(self):
    """å¤±æ•—æ™‚å›æ»¾ï¼šæ¸…ç† Staging ç›®éŒ„"""
    staging_path = Path(self.batch_config.staging_dir) / self.current_batch_id
    if staging_path.exists():
        import shutil
        shutil.rmtree(staging_path)
        self.logger.info(f"Rolled back staging: {staging_path}")
```

### Phase 3: Manifest æ©Ÿåˆ¶èˆ‡ä¸‹æ¸¸éŠœæ¥ (é ä¼° 1.5 å¤©)

#### Step 3.1: Manifest æ¨¡å‹èˆ‡ç”¢ç”Ÿ
```python
from pydantic import BaseModel
from datetime import datetime
from typing import List, Dict

class Manifest(BaseModel):
    manifest_version: str = "1.0"
    batch_id: str
    site_id: str
    created_at: datetime
    input_files_count: int
    output_files: List[str]
    statistics: Dict
    schema_hash: str
    checksum: str
    
    def save(self, path: Path):
        path.write_text(self.json(indent=2), encoding='utf-8')

def _generate_manifest(self) -> Manifest:
    """ç”¢ç”Ÿæ‰¹æ¬¡æ¸…å–®"""
    staging_base = Path(self.batch_config.staging_dir) / self.current_batch_id
    output_files = [
        str(f.relative_to(staging_base)) 
        for f in staging_base.rglob("*.parquet")
    ]
    
    # è¨ˆç®— Schema é›œæ¹Šï¼ˆç”¨æ–¼å¿«å–å¤±æ•ˆï¼‰
    schema_str = str(sorted([(c, str(t)) for c, t in self.buffer[0].schema.items()])) if self.buffer else ""
    schema_hash = hashlib.sha256(schema_str.encode()).hexdigest()[:16]
    
    return Manifest(
        batch_id=self.current_batch_id,
        site_id=self.config.cleaner.site_id,  # å‡è¨­ CleanerConfig æœ‰ site_id
        created_at=datetime.now(timezone.utc),
        input_files_count=self.stats["processed_files"],
        output_files=output_files,
        statistics={
            "total_rows": self.stats["total_rows"],
            "quality_flags_distribution": self.stats["quality_flags_dist"]
        },
        schema_hash=schema_hash,
        checksum="sha256:..."  # å¯¦éš›è¨ˆç®—æª”æ¡ˆé›œæ¹Š
    )
```

#### Step 3.2: æä¾› Feature Engineer è®€å–ç¯„ä¾‹
**æ–‡ä»¶**: `docs/batch_to_feature_engineer_interface.md`

```python
# Feature Engineer è®€å–ç¯„ä¾‹ï¼ˆé€é Manifestï¼‰
def load_from_manifest(manifest_path: Path) -> pl.LazyFrame:
    """é€é Manifest ç²¾æº–è®€å–ï¼Œé¿å… glob å°æ–‡ä»¶çˆ†ç‚¸"""
    import json
    manifest = json.loads(manifest_path.read_text())
    
    base_dir = manifest_path.parent.parent / "processed"  # èª¿æ•´è·¯å¾‘
    files = [base_dir / f for f in manifest["output_files"]]
    
    # ä½¿ç”¨ scan_parquet æƒ°æ€§è®€å–ï¼ˆè¨˜æ†¶é«”å‹å¥½ï¼‰
    return pl.scan_parquet(files)

# é©—è­‰ Schema ä¸€è‡´æ€§
def validate_schema(df: pl.LazyFrame, expected_manifest: dict):
    actual_schema = df.schema
    # é©—è­‰ quality_flags ç‚º List[str]
    assert actual_schema["quality_flags"] == pl.List(pl.Utf8), \
        "Schema mismatch: quality_flags must be List[str]"
```

### Phase 4: é©—è­‰èˆ‡ç›£æ§ (é ä¼° 0.5 å¤©)

#### Step 4.1: è¨˜æ†¶é«”ç©©å®šæ€§æ¸¬è©¦
```python
def test_memory_stability():
    """é©—è­‰è™•ç† 1000 å€‹æª”æ¡ˆæ™‚è¨˜æ†¶é«”æŒå¹³"""
    initial_mem = psutil.Process().memory_info().rss
    
    for i in range(1000):
        orchestrator.process_single_file(mock_file)
        if i % 100 == 0:
            current_mem = psutil.Process().memory_info().rss
            assert current_mem < initial_mem * 1.5, "Memory leak detected"
```

#### Step 4.2: å¥‘ç´„é©—è­‰æ¸¬è©¦ï¼ˆé˜² quality_flags è¢«æŠ¹é™¤ï¼‰
```python
def test_quality_flags_preserved():
    """é©—è­‰è¼¸å‡º Parquet ä¿ç•™ List[str] å‹åˆ¥"""
    # åŸ·è¡Œæ‰¹æ¬¡è™•ç†
    orchestrator.run()
    
    # è®€å–è¼¸å‡ºçš„ Parquet
    output_file = list(Path("data/processed").rglob("*.parquet"))[0]
    df = pl.read_parquet(output_file)
    
    # é—œéµé©—è­‰ï¼šquality_flags å¿…é ˆæ˜¯ List[str]ï¼Œè€Œé Null æˆ– Float64
    assert df["quality_flags"].dtype == pl.List(pl.Utf8)
    assert df["quality_flags"].null_count() == 0  # å¯ç‚ºç©ºåˆ—è¡¨ï¼Œä¸å¯ç‚º Null
```

---

## 5. é¢¨éšªè©•ä¼°èˆ‡ç·©è§£ï¼ˆæ›´æ–°ï¼‰

| é¢¨éšª | åš´é‡åº¦ | ç·©è§£æªæ–½ï¼ˆv1.1 è¨­è¨ˆï¼‰ |
|:---|:---:|:---|
| **å°æ–‡ä»¶çˆ†ç‚¸** | ğŸ”´ Critical | **æ™‚é–“åˆ†å€åˆä½µ**ï¼ˆ`max_rows_per_file: 100000`ï¼‰ï¼Œé¿å…ä¸€å°ä¸€ CSVâ†’Parquet |
| **quality_flags è¢«æŠ¹é™¤** | ğŸ”´ Critical | **Schema å¥‘ç´„é©—è­‰**ï¼ˆ`OutputContractValidator`ï¼‰ï¼Œå‹åˆ¥éŒ¯èª¤ç«‹å³æ‹‹å‡º |
| **æ‰¹æ¬¡å¤±æ•—æ®˜ç•™é«’è³‡æ–™** | ğŸ”´ High | **Staging + Atomic Move** äº‹å‹™æ©Ÿåˆ¶ï¼Œå¤±æ•—è‡ªå‹• Rollback |
| **è¨˜æ†¶é«” OOM** | ğŸ”´ High | **Process-and-Dump** + è¨˜æ†¶é«”ç›£æ§ï¼ˆ`memory_limit_mb`ï¼‰ |
| **Feature Engineer è®€å–éŒ¯èª¤** | ğŸŸ  High | **Manifest æ©Ÿåˆ¶**æ›¿ä»£ globï¼Œç²¾æº–è¿½è¹¤è¼¸å‡ºæª”æ¡ˆèˆ‡ Schema é›œæ¹Š |
| **æ™‚é–“é †åºéŒ¯äº‚** | ğŸŸ  Medium | **å¼·åˆ¶æ’åº**ï¼ˆ`sort("timestamp")`ï¼‰+ è·¨æª”å»é‡ï¼ˆé¸é…ï¼‰ |
| **é…ç½®ä¸ä¸€è‡´** | ğŸŸ¡ Medium | **ETLConfig çµ±ä¸€é©—è­‰**ï¼Œç¢ºä¿ Cleaner èˆ‡ Batch æ™‚é–“è§£æåº¦ç›¸å®¹ |

---

## 6. äº¤ä»˜ç”¢ç‰©æ¸…å–®

1. `src/etl/batch_processor_v2.py`: å…¨æ–° Orchestratorï¼ˆå« Stagingã€Manifestã€è¨˜æ†¶é«”ç›£æ§ï¼‰
2. `src/etl/contract_validator.py`: Schema å¥‘ç´„é©—è­‰å™¨ï¼ˆé˜² quality_flags å‹åˆ¥éŒ¯èª¤ï¼‰
3. `src/etl/config_models.py`: æ›´æ–° `BatchConfig`, `ETLConfig`ï¼ˆçµ±ä¸€é©—è­‰ï¼‰
4. `src/etl/manifest.py`: Manifest è³‡æ–™æ¨¡å‹èˆ‡ç®¡ç†
5. `tests/test_batch_processor_v2.py`: 
   - è¨˜æ†¶é«”ç©©å®šæ€§æ¸¬è©¦ï¼ˆ1000 æª”æ¡ˆè¿´åœˆï¼‰
   - Schema å¥‘ç´„é©—è­‰æ¸¬è©¦ï¼ˆé˜² Float64 è½‰å‹ï¼‰
   - äº‹å‹™æ€§æ¸¬è©¦ï¼ˆStaging â†’ Atomic Move â†’ Rollbackï¼‰
6. `tests/integration/test_cleaner_to_batch.py`: Cleaner è¼¸å‡º â†’ Batch è¼¸å…¥æ•´åˆæ¸¬è©¦
7. `docs/batch_to_feature_engineer_interface.md`: çµ¦ Feature Engineer åœ˜éšŠçš„è®€å–ç¯„ä¾‹ï¼ˆå« Manifest ä½¿ç”¨ï¼‰
8. `scripts/run_batch_pipeline.py`: CLI å…¥å£è…³æœ¬ï¼ˆå« argparseï¼‰

---

## 7. èˆ‡ä¸Šä¸‹æ¸¸å”ä½œæª¢æŸ¥æ¸…å–®

åœ¨éƒ¨ç½²å‰ï¼Œè«‹èˆ‡ç›¸é—œè² è²¬äººç¢ºèªï¼š

### èˆ‡ Cleaner v2.1 åœ˜éšŠï¼š
- [ ] `quality_flags` è¼¸å‡ºæ˜¯å¦ä¿è­‰ç‚º `List[str]`ï¼ˆPolars `List[Utf8]`ï¼‰ï¼Ÿ
- [ ] æ™‚é–“æˆ³æ˜¯å¦å·²æ’åºï¼Ÿï¼ˆBatchProcessor æœƒäºŒæ¬¡æ’åºï¼Œä½†é æ’åºå¯æå‡æ•ˆèƒ½ï¼‰
- [ ] `resample_interval` è¨­å®šå€¼ï¼ˆç”¨æ–¼é©—è­‰æ™‚é–“é€£çºŒæ€§ï¼‰

### èˆ‡ Feature Engineer åœ˜éšŠï¼š
- [ ] æ˜¯å¦æ¥å—é€é `manifest.json` è®€å–æª”æ¡ˆæ¸…å–®ï¼ˆè€Œé `glob("*.parquet")`ï¼‰ï¼Ÿ
- [ ] Parquet æ™‚é–“æˆ³æ ¼å¼åå¥½ï¼ˆ`INT64 (nanoseconds)` vs `INT96`ï¼‰ï¼Ÿ
- [ ] å–®ä¸€ Parquet æª”æ¡ˆå¤§å°åå¥½ï¼ˆå»ºè­° 100MB ~ 1GBï¼‰ï¼Ÿ
- [ ] æ˜¯å¦éœ€è¦ `schema_hash` ç”¨æ–¼ç‰¹å¾µå¿«å–å¤±æ•ˆæª¢æ¸¬ï¼Ÿ

### èˆ‡ç¶­é‹åœ˜éšŠï¼š
- [ ] æª”æ¡ˆç³»çµ±æ˜¯å¦æ”¯æ´ Atomic Moveï¼ˆåŒåˆ†å‰²å€å…§ `mv` ç‚ºåŸå­æ€§ï¼‰ï¼Ÿ
- [ ] Staging ç›®éŒ„ï¼ˆ`data/.staging/`ï¼‰æ˜¯å¦æœ‰è¶³å¤ ç£ç¢Ÿç©ºé–“ï¼ˆé ä¼°ç‚ºè¼¸å‡ºè³‡æ–™çš„ 2 å€ï¼‰ï¼Ÿ
- [ ] æ˜¯å¦éœ€è¦æ•´åˆ Prometheus/Grafana ç›£æ§ï¼ˆè¼¸å‡º `batch_rows_processed` ç­‰æŒ‡æ¨™ï¼‰ï¼Ÿ

---

**é—œéµä¿®æ”¹ç¸½çµ**ï¼š
1. **Schema å¥‘ç´„é©—è­‰**ï¼šæ””æˆª `quality_flags` è¢«èª¤è½‰ç‚º Float64 çš„é¢¨éšªï¼ˆCriticalï¼‰
2. **æ™‚é–“åˆ†å€åˆä½µ**ï¼šé¿å… 50 è¬å°æ–‡ä»¶å•é¡Œï¼ˆCriticalï¼‰
3. **Staging + Atomic Move**ï¼šäº‹å‹™æ€§è¼¸å‡ºï¼Œæ”¯æ´å†ªç­‰é‡è·‘ï¼ˆHighï¼‰
4. **Manifest æ©Ÿåˆ¶**ï¼šæ›¿ä»£ globï¼Œæä¾›è¡€ç·£è¿½è¹¤èˆ‡ç²¾æº–è®€å–ï¼ˆHighï¼‰
5. **çµ±ä¸€ ETLConfig**ï¼šç¢ºä¿ Cleaner èˆ‡ BatchProcessor é…ç½®ç›¸å®¹ï¼ˆMediumï¼‰
```