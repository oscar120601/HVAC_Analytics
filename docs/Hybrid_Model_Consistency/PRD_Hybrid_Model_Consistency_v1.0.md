# PRD v1.0: Hybrid Model Consistency Validation Specification
# æ··åˆæ¨¡å‹ä¸€è‡´æ€§é©—è­‰è¦ç¯„

**æ–‡ä»¶ç‰ˆæœ¬:** v1.0 (Golden Standard & Copula Effect Handling)  
**æ—¥æœŸ:** 2026-02-13  
**è² è²¬äºº:** Oscar Chang  
**ç›®æ¨™æ¨¡çµ„:** `src/modeling/validation/hybrid_consistency.py`, `src/modeling/validation/consistency_checker.py`  
**ä¸Šæ¸¸å¥‘ç´„:** `src/modeling/training_pipeline.py` (v1.2+, BatchTrainingCoordinator)  
**ä¸‹æ¸¸å¥‘ç´„:** `src/optimization/engine.py` (v1.1+, ModelRegistry)  
**é—œéµç›¸ä¾:** `src/modeling/artifacts.py` (MultiModelArtifact), `src/features/annotation_manager.py`  
**é ä¼°å·¥æ™‚:** 3 ~ 4 å€‹å·¥ç¨‹å¤©

---

## 1. åŸ·è¡Œç¸½ç¶±èˆ‡è¨­è¨ˆå“²å­¸

### 1.1 æ ¸å¿ƒç›®æ¨™

å»ºç«‹**æ¨™æº–åŒ–ã€å¯é‡ç¾ã€é ˜åŸŸæ„ŸçŸ¥**çš„Hybrid Modelä¸€è‡´æ€§é©—è­‰æ¡†æ¶ï¼Œè§£æ±ºä»¥ä¸‹é—œéµæ¨¡ç³Šåœ°å¸¶ï¼š

1. **é»ƒé‡‘æ¨™æº–å®šç¾© (Golden Standard Definition)**ï¼šæ˜ç¢ºå®šç¾©æ¯”è¼ƒçš„åŸºæº–è³‡æ–™é›†ï¼ˆé©—è­‰é›† vs æ¸¬è©¦é›†ï¼‰ã€æ¯”è¼ƒç¶­åº¦ï¼ˆé€ç­† vs æ•´é«”ï¼‰ã€ä»¥åŠçµ±è¨ˆæ–¹æ³•è«–
2. **å®¹å·®è¨ˆç®—æ¨™æº–åŒ– (Tolerance Calculation Standardization)**ï¼šå»ºç«‹è€ƒæ…®HVACè¨­å‚™è€¦åˆæ•ˆæ‡‰ï¼ˆCopula Effectï¼‰çš„å‹•æ…‹å®¹å·®æ¨¡å‹ï¼Œè€Œéç¡¬æ€§å›ºå®š5%é–€æª»
3. **éƒ¨åˆ†ç¼ºå¤±è™•ç† (Partial Missing Handling)**ï¼šå®šç¾©ç•¶Component Modelsåƒ…éƒ¨åˆ†å­˜åœ¨æ™‚ï¼ˆå¦‚åªæœ‰chiller_1_kwï¼Œç„¡chiller_2_kwï¼‰çš„åŠ ç¸½é‚è¼¯èˆ‡ç½®ä¿¡åº¦è©•ä¼°
4. **è€¦åˆæ•ˆæ‡‰è£œå„Ÿ (Copula Effect Compensation)**ï¼šè­˜åˆ¥ä¸¦é‡åŒ–è¨­å‚™é–“äº¤äº’ä½œç”¨å°è‡´çš„ç³»çµ±æ€§åå·®ï¼ˆå¦‚å…±ç”¨ç®¡è·¯å£“åŠ›æå¤±ã€ç†±äº¤äº’ä½œç”¨ï¼‰ï¼Œé¿å…èª¤åˆ¤æ­£å¸¸ç‰©ç†ç¾è±¡ç‚ºæ¨¡å‹éŒ¯èª¤

### 1.2 è¨­è¨ˆåŸå‰‡

1. **åˆ†å±¤é©—è­‰ç­–ç•¥ (Hierarchical Validation)**ï¼š
   - **L1ï¼šæ¨£æœ¬ç´šé©—è­‰ (Sample-Level)**ï¼šå–®ç­†è³‡æ–™é»çš„çµ•å°èª¤å·®æª¢æŸ¥ï¼ˆç”¨æ–¼ç•°å¸¸æª¢æ¸¬ï¼‰
   - **L2ï¼šèšåˆç´šé©—è­‰ (Aggregate-Level)**ï¼šåˆ†ç¾¤çµ±è¨ˆï¼ˆä¾è² è¼‰å€é–“ã€å¤–æ°£æ¢ä»¶ã€é‹è¡Œæ¨¡å¼åˆ†ç¾¤ï¼‰
   - **L3ï¼šè¶¨å‹¢ç´šé©—è­‰ (Trend-Level)**ï¼šæ™‚é–“åºåˆ—ç›¸é—œæ€§èˆ‡æ®˜å·®åˆ†æï¼ˆç¢ºä¿å…©æ¨¡å‹è¶¨å‹¢ä¸€è‡´ï¼‰

2. **å‹•æ…‹å®¹å·®é–¾å€¼ (Dynamic Tolerance Threshold)**ï¼š
   - åŸºæ–¼**ç³»çµ±è² è¼‰ç‡ (System Load Percentage)** å‹•æ…‹èª¿æ•´å®¹å·®ï¼ˆä½è² è¼‰æ™‚å…è¨±è¼ƒé«˜ç›¸å°èª¤å·®ï¼‰
   - åŸºæ–¼**è¨­å‚™çµ„åˆè¤‡é›œåº¦ (Equipment Combination Complexity)** èª¿æ•´ï¼ˆå¤šè¨­å‚™ä¸¦è¯æ™‚è€¦åˆæ•ˆæ‡‰å¢å¼·ï¼‰
   - å€åˆ†**çµ±è¨ˆèª¤å·® (Statistical Error)** èˆ‡**çµæ§‹æ€§åå·® (Structural Bias)**

3. **ç‰©ç†å¯è§£é‡‹æ€§ (Physical Interpretability)**ï¼š
   - æ‰€æœ‰ä¸€è‡´æ€§é•è¦å¿…é ˆæ¨™è¨˜**ç‰©ç†åŸå› æ¨™ç±¤**ï¼ˆå¦‚"ç®¡è·¯å£“æ"ã€"ç†±çŸ­è·¯"ã€"æ¸¬é‡èª¤å·®"ï¼‰
   - æä¾›**è€¦åˆæ•ˆæ‡‰ç†±å€åœ– (Copula Heatmap)** è¦–è¦ºåŒ–è¨­å‚™é–“äº¤äº’ä½œç”¨å¼·åº¦

4. **å‘å¾Œç›¸å®¹æ€§ (Backward Compatibility)**ï¼š
   - èˆ‡Training v1.2çš„`BatchTrainingCoordinator._validate_hybrid_consistency()`ç„¡ç¸«æ•´åˆ
   - èˆ‡Optimization v1.1çš„`ModelRegistry.validate_hybrid_consistency()`ä»‹é¢ä¸€è‡´
   - æ”¯æ´æ—¢æœ‰çš„5%ç¡¬æ€§é–€æª»ä½œç‚ºä¿å®ˆ fallback é¸é …

### 1.3 é—œéµè¡“èªå®šç¾©

| è¡“èª | å®šç¾© | æ•¸å­¸è¡¨é” |
|:---|:---|:---|
| **System Model ($S$)** | é æ¸¬ç³»çµ±ç¸½è€—é›»çš„æ¨¡å‹ï¼ˆé»‘ç›’ï¼‰ | $\hat{y}_S = f_S(X)$ |
| **Component Models ($C_i$)** | é æ¸¬å€‹åˆ¥è¨­å‚™è€—é›»çš„æ¨¡å‹é›†åˆ | $\hat{y}_{C_i} = f_{C_i}(X_i)$ |
| **åŠ ç¸½é æ¸¬ ($C_{sum}$)** | Component Models çš„ç®—è¡“å’Œ | $\hat{y}_{C_{sum}} = \sum_{i=1}^n \hat{y}_{C_i}$ |
| **è€¦åˆæå¤± ($\delta_{copula}$)** | è¨­å‚™é–“äº¤äº’ä½œç”¨å°è‡´çš„ç³»çµ±æ€§åå·® | $\delta_{copula} = \mathbb{E}[C_{sum} - S]$ |
| **é»ƒé‡‘è³‡æ–™é›† ($D_{golden}$)** | ç”¨æ–¼æ¯”è¼ƒçš„æ¬Šå¨è³‡æ–™é›†ï¼ˆå®šç¾©è¦‹2.1ç¯€ï¼‰ | $D_{test}$ æˆ– $D_{val}$ |
| **å‹•æ…‹å®¹å·® ($\tau$)** | ä¾é‹è¡Œæ¢ä»¶è®ŠåŒ–çš„èª¤å·®ä¸Šé™ | $\tau = \tau_{base} \cdot \alpha_{load} \cdot \beta_{complexity}$ |

---

## 2. é»ƒé‡‘æ¨™æº–å®šç¾© (Golden Standard Definition)

### 2.1 æ¯”è¼ƒè³‡æ–™é›†é¸æ“‡è¦ç¯„

**åŸå‰‡ï¼šæ¸¬è©¦é›†å„ªå…ˆ (Test Set Priority)**

| æƒ…å¢ƒ | é»ƒé‡‘è³‡æ–™é›†é¸æ“‡ | ç†ç”± | éŒ¯èª¤ä»£ç¢¼ |
|:---|:---|:---|:---:|
| **æ¨™æº–æƒ…å¢ƒ** | **æ¸¬è©¦é›† ($D_{test}$)** | æœªåƒèˆ‡ä»»ä½•æ¨¡å‹è¨“ç·´ï¼Œæœ€èƒ½åæ˜ çœŸå¯¦æ³›åŒ–èª¤å·® | - |
| æ¸¬è©¦é›†ä¸å¯ç”¨ | é©—è­‰é›† ($D_{val}$) | é›–ç”¨æ–¼æ—©åœï¼Œä½†æœªç”¨æ–¼åƒæ•¸å„ªåŒ–ï¼Œå¯æ¥å— | E759 (Info) |
| æ¨£æœ¬æ•¸ä¸è¶³ (<100) | åˆä½µé©—è­‰é›†+æ¸¬è©¦é›† | çµ±è¨ˆé¡¯è‘—æ€§è€ƒé‡ï¼Œéœ€æ¨™è¨˜ | E759 (Warn) |
| æ™‚é–“åºåˆ—é©—è­‰ | ä¾æ™‚é–“åˆ‡åˆ†çš„ç¨ç«‹å€æ®µ | é¿å…è³‡æ–™æ´©æ¼ï¼Œç¢ºä¿æ™‚é–“ç›¸ä¾æ€§ | - |

**å¯¦ä½œè¦ç¯„**ï¼š
```python
class GoldenDatasetSelector:
    """
    é»ƒé‡‘è³‡æ–™é›†é¸æ“‡å™¨
    æ ¹æ“šè³‡æ–™å¯ç”¨æ€§èˆ‡å“è³ªè‡ªå‹•é¸æ“‡æœ€é©åˆçš„æ¯”è¼ƒåŸºæº–
    """
    
    def select(self, artifacts: Dict[str, MultiModelArtifact]) -> Tuple[pd.DataFrame, str]:
        """
        Returns:
            dataset: ç”¨æ–¼æ¯”è¼ƒçš„ç‰¹å¾µçŸ©é™£èˆ‡çœŸå¯¦å€¼
            source_tag: è³‡æ–™ä¾†æºæ¨™è¨˜ ("test", "val", "combined", "insufficient")
        """
        # å„ªå…ˆæª¢æŸ¥ Test Set
        if self._has_sufficient_samples(artifacts, "test", min_samples=100):
            return self._load_test_set(artifacts), "test"
        
        # æ¬¡é¸ Validation Set
        elif self._has_sufficient_samples(artifacts, "val", min_samples=100):
            logger.warning("E759: ä½¿ç”¨é©—è­‰é›†é€²è¡Œä¸€è‡´æ€§æª¢æŸ¥ï¼Œçµæœå¯èƒ½éåº¦æ¨‚è§€")
            return self._load_val_set(artifacts), "val"
        
        # æ¨£æœ¬ä¸è¶³æ™‚åˆä½µï¼ˆéœ€ç¢ºä¿ç„¡é‡ç–Šï¼‰
        elif self._can_combine_safely(artifacts):
            logger.warning("E759: åˆä½µé©—è­‰é›†èˆ‡æ¸¬è©¦é›†ä»¥ç¢ºä¿çµ±è¨ˆé¡¯è‘—æ€§")
            return self._load_combined(artifacts), "combined"
        
        else:
            raise ConsistencyError("E750", "æ¨£æœ¬æ•¸ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œå¯é çš„ä¸€è‡´æ€§é©—è­‰")
```

### 2.2 æ¯”è¼ƒç¶­åº¦èˆ‡çµ±è¨ˆæ–¹æ³•è«–

**ä¸‰ç¶­åº¦æ¯”è¼ƒæ¡†æ¶**ï¼š

#### ç¶­åº¦ Aï¼šé€ç­†çµ•å°èª¤å·® (Point-wise Absolute Error)
- **ç”¨é€”**ï¼šæª¢æ¸¬æ¥µç«¯ç•°å¸¸å€¼ï¼ˆOutliersï¼‰
- **æŒ‡æ¨™**ï¼š$e_i = |\hat{y}_{S,i} - \hat{y}_{C_{sum},i}|$
- **é–¾å€¼**ï¼š$e_i < \tau_{absolute}$ï¼ˆé è¨­ 10 kWï¼Œå¯é…ç½®ï¼‰
- **è¼¸å‡º**ï¼šç•°å¸¸æ¨£æœ¬æ¨™è¨˜ï¼ˆç”¨æ–¼å¾ŒçºŒæ ¹å› åˆ†æï¼‰

#### ç¶­åº¦ Bï¼šåˆ†ç¾¤ç›¸å°èª¤å·® (Binned Relative Error) - **ä¸»è¦æ±ºç­–ä¾æ“š**
- **ç”¨é€”**ï¼šé¿å…æ¥µç«¯å€¼ä¸»å°æ•´é«”åˆ¤æ–·ï¼Œè€ƒé‡HVACéç·šæ€§ç‰¹æ€§
- **åˆ†ç¾¤ç­–ç•¥**ï¼š
  1. **ä¾ç³»çµ±è² è¼‰åˆ†ç¾¤ (By System Load)**ï¼š
     - è¼•è¼‰ (0-30% RT)
     - ä¸­è¼‰ (30-70% RT) 
     - é‡è¼‰ (70-100% RT)
  2. **ä¾å¤–æ°£æ¿•çƒæº«åº¦åˆ†ç¾¤ (By Wet-bulb Temp)**ï¼š
     - ä½æº« (<20Â°C)
     - ä¸­æº« (20-26Â°C)
     - é«˜æº« (>26Â°C)
  3. **ä¾è¨­å‚™çµ„åˆåˆ†ç¾¤ (By Equipment Combo)**ï¼š
     - å–®æ©Ÿé‹è¡Œ
     - é›™æ©Ÿä¸¦è¯
     - å¤šæ©Ÿ (>2) ä¸¦è¯

- **æŒ‡æ¨™**ï¼šå„ç¾¤çµ„çš„ MAPE (Mean Absolute Percentage Error)
  $$MAPE_{bin} = \frac{1}{n_{bin}} \sum_{i \in bin} \frac{|\hat{y}_{S,i} - \hat{y}_{C_{sum},i}|}{\hat{y}_{S,i}} \times 100\%$$

- **é€šéæ¨™æº–**ï¼š**æ‰€æœ‰ç¾¤çµ„**çš„ $MAPE_{bin} < \tau_{dynamic}$ï¼ˆå‹•æ…‹é–¾å€¼ï¼Œè¦‹ç¬¬3ç¯€ï¼‰

#### ç¶­åº¦ Cï¼šæ•´é«”çµ±è¨ˆä¸€è‡´æ€§ (Overall Statistical Consistency)
- **ç”¨é€”**ï¼šç¢ºä¿é•·æœŸè¶¨å‹¢èˆ‡çµ±è¨ˆåˆ†ä½ˆä¸€è‡´
- **æŒ‡æ¨™**ï¼š
  1. **æ•´é«” MAPE**ï¼š$MAPE_{overall} < 5\%$ï¼ˆå‚³çµ±ç¡¬æ€§æ¨™æº–ï¼Œä½œç‚ºå¾Œå‚™æª¢æŸ¥ï¼‰
  2. **ç›¸é—œä¿‚æ•¸**ï¼š$Corr(\hat{y}_S, \hat{y}_{C_{sum}}) > 0.95$
  3. **å¹³å‡åå·® (Mean Bias)**ï¼š$Bias = \frac{1}{n}\sum(\hat{y}_{C_{sum}} - \hat{y}_S)$
     - ç”¨æ–¼æª¢æ¸¬ç³»çµ±æ€§é«˜ä¼°/ä½ä¼°ï¼ˆè€¦åˆæ•ˆæ‡‰æŒ‡æ¨™ï¼‰

---

## 3. å‹•æ…‹å®¹å·®è¨ˆç®—èˆ‡è€¦åˆæ•ˆæ‡‰è£œå„Ÿ

### 3.1 å‹•æ…‹å®¹å·®æ¨¡å‹ (Dynamic Tolerance Model)

**å‘Šåˆ¥å›ºå®š5%**ï¼šæ¡ç”¨å¤šå› å­èª¿æ•´æ¨¡å‹

$$\tau_{dynamic} = \tau_{base} \cdot \alpha_{load} \cdot \beta_{complexity} \cdot \gamma_{copula} + \delta_{measurement}$$

**åƒæ•¸å®šç¾©**ï¼š

| åƒæ•¸ | é è¨­å€¼ | èª¿æ•´é‚è¼¯ | èªªæ˜ |
|:---|:---:|:---|:---|
| **åŸºç¤å®¹å·® $\tau_{base}$** | 5% | ä¾æ¡ˆå ´æ­·å²è³‡æ–™æ ¡æº– | å‚³çµ±ç¡¬æ€§é–€æª»ä½œç‚ºåŸºæº– |
| **è² è¼‰èª¿æ•´å› å­ $\alpha_{load}$** | 1.0 | è¼•è¼‰(<30%)æ™‚ 1.5ï¼Œä¸­è¼‰ 1.0ï¼Œé‡è¼‰ 0.8 | ä½è² è¼‰æ™‚ç›¸å°èª¤å·®æ”¾å¤§ |
| **è¤‡é›œåº¦èª¿æ•´å› å­ $\beta_{complexity}$** | 1.0 | å–®æ©Ÿ 0.9ï¼Œé›™æ©Ÿ 1.0ï¼Œä¸‰æ©Ÿä»¥ä¸Š 1.2 | è¨­å‚™è¶Šå¤šï¼Œè€¦åˆæ•ˆæ‡‰è¶Šå¼· |
| **è€¦åˆè£œå„Ÿé … $\gamma_{copula}$** | 1.0 | ä¾ç®¡è·¯æ‹“æ’²èˆ‡æ­·å²åå·®æ ¡æº– | è¦‹3.2ç¯€ |
| **é‡æ¸¬èª¤å·®é … $\delta_{measurement}$** | 0.5% | å›ºå®šè£œå„Ÿï¼ˆCTã€PTç²¾åº¦é™åˆ¶ï¼‰ | ç¡¬é«”é™åˆ¶ç·©è¡ |

**å¯¦ä½œç¯„ä¾‹**ï¼š
```python
class DynamicToleranceCalculator:
    """
    å‹•æ…‹å®¹å·®è¨ˆç®—å™¨
    æ ¹æ“šé‹è¡Œæ¢ä»¶è¨ˆç®—ç•¶ä¸‹å¯æ¥å—çš„èª¤å·®ç¯„åœ
    """
    
    def calculate(
        self, 
        system_load_percent: float,
        active_equipment_count: int,
        ambient_wb_temp: float,
        site_topology: str = "standard"
    ) -> float:
        """
        è¨ˆç®—å‹•æ…‹å®¹å·®é–¾å€¼ (%)
        """
        # åŸºç¤å®¹å·®
        tau_base = self.config.base_tolerance  # 5.0
        
        # è² è¼‰èª¿æ•´ï¼ˆè¼•è¼‰æ”¾å¯¬ï¼Œé‡è¼‰æ”¶ç·Šï¼‰
        if system_load_percent < 0.3:
            alpha = 1.5  # è¼•è¼‰ï¼š5% * 1.5 = 7.5%
        elif system_load_percent > 0.7:
            alpha = 0.8  # é‡è¼‰ï¼š5% * 0.8 = 4.0%
        else:
            alpha = 1.0
            
        # è¤‡é›œåº¦èª¿æ•´ï¼ˆä¸¦è¯æ©Ÿçµ„æ•¸é‡ï¼‰
        if active_equipment_count == 1:
            beta = 0.9
        elif active_equipment_count == 2:
            beta = 1.0
        else:
            beta = 1.0 + (active_equipment_count - 2) * 0.1  # æ¯å¤šä¸€å°+10%
        
        # è€¦åˆæ•ˆæ‡‰è£œå„Ÿï¼ˆä¾æ¡ˆå ´æ‹“æ’²ï¼‰
        gamma = self.copula_factors.get(site_topology, 1.0)
        
        # é‡æ¸¬èª¤å·®ç·©è¡
        delta = 0.5
        
        tau_dynamic = tau_base * alpha * beta * gamma + delta
        
        return min(tau_dynamic, 15.0)  # ä¸Šé™15%ï¼Œé˜²æ­¢éåº¦æ”¾å¯¬
```

### 3.2 HVACè€¦åˆæ•ˆæ‡‰ (Copula Effect) è™•ç†

**ç‰©ç†æ©Ÿåˆ¶è­˜åˆ¥**ï¼š

| è€¦åˆé¡å‹ | ç‰©ç†æè¿° | é æœŸåå·®æ–¹å‘ | è£œå„Ÿä¿‚æ•¸ $\gamma$ |
|:---|:---|:---:|:---:|
| **ç®¡è·¯å£“æ (Piping Loss)** | å¤šæ©Ÿä¸¦è¯æ™‚å…±ç”¨ç®¡è·¯æ‘©æ“¦æå¤±å¢åŠ  | $C_{sum} > S$ (åŠ ç¸½é«˜ä¼°) | 1.02 ~ 1.05 |
| **ç†±çŸ­è·¯ (Thermal Short-circuit)** | å†·å»æ°´å¡”å›æ°´æº«åº¦äº¤äº’å½±éŸ¿ | $C_{sum} > S$ | 1.01 ~ 1.03 |
| **é›»åŠ›è«§æ³¢ (Harmonics)** | å¤šè®Šé »å™¨ä¸¦è¯ç”¢ç”Ÿè«§æ³¢æå¤± | $C_{sum} < S$ (åŠ ç¸½ä½ä¼°) | 0.98 ~ 0.99 |
| **æ§åˆ¶å»¶é² (Control Lag)** | è¨­å‚™é–“æ§åˆ¶ä¸åŒæ­¥å°è‡´æ•ˆç‡æå¤± | $C_{sum} > S$ | 1.01 ~ 1.02 |
| **æ¸¬é‡ç–ŠåŠ èª¤å·® (Measurement Stack)** | å€‹åˆ¥è¨­å‚™é›»è¡¨èª¤å·®ç´¯åŠ  | ä¸ç¢ºå®š | 1.01 ~ 1.03 |

**è€¦åˆä¿‚æ•¸è‡ªå‹•æ ¡æº–**ï¼š
```python
def calibrate_copula_factor(
    self, 
    historical_data: pd.DataFrame,
    system_predictions: np.ndarray,
    component_predictions: Dict[str, np.ndarray]
) -> Dict[str, float]:
    """
    åŸºæ–¼æ­·å²é‹è¡Œè³‡æ–™è‡ªå‹•æ ¡æº–è€¦åˆä¿‚æ•¸
    åŸ·è¡Œæ™‚æ©Ÿï¼šæ¨¡å‹éƒ¨ç½²åˆæœŸï¼ˆç¬¬ä¸€å€‹æœˆé‹è¡Œå¾Œï¼‰
    
    Returns:
        copula_factors: ä¾è¨­å‚™çµ„åˆèˆ‡è² è¼‰å€é–“çš„è£œå„Ÿä¿‚æ•¸è¡¨
    """
    # è¨ˆç®—æ­·å²å¹³å‡åå·®
    component_sum = sum(component_predictions.values())
    historical_bias = np.mean(component_sum - system_predictions)
    
    # ä¾è² è¼‰å€é–“åˆ†ç¾¤è¨ˆç®—
    for load_bin in ['light', 'medium', 'heavy']:
        mask = self._get_load_mask(historical_data, load_bin)
        bias_bin = np.mean((component_sum - system_predictions)[mask])
        
        # è½‰æ›ç‚ºè£œå„Ÿä¿‚æ•¸ï¼ˆåå·®>0è¡¨ç¤ºåŠ ç¸½é«˜ä¼°ï¼Œéœ€æ”¾å¯¬å®¹å·®ï¼‰
        if bias_bin > 0:
            gamma = 1.0 + (bias_bin / np.mean(system_predictions[mask]))
        else:
            gamma = 1.0
            
        self.copula_factors[load_bin] = round(gamma, 3)
    
    return self.copula_factors
```

---

## 4. éƒ¨åˆ†ç¼ºå¤±Component Modelsè™•ç†é‚è¼¯

### 4.1 å¯ç”¨æ€§åˆ†ç´šèˆ‡é™ç´šç­–ç•¥

ç•¶ä¸¦éæ‰€æœ‰Component Modelséƒ½å­˜åœ¨æ™‚ï¼ˆä¾‹å¦‚åƒ…è¨“ç·´äº†chiller_1_kwï¼Œç¼ºå°‘pumpèˆ‡toweræ¨¡å‹ï¼‰ï¼š

| ç­‰ç´š | å¯ç”¨Components | è™•ç†ç­–ç•¥ | ç½®ä¿¡åº¦æ¨™è¨˜ | ä½¿ç”¨é™åˆ¶ |
|:---|:---|:---|:---:|:---|
| **L3 (å®Œæ•´)** | ä¸»æ©Ÿ+æ°´æ³µ+æ°´å¡”+å…¶ä»– | æ¨™æº–åŠ ç¸½æ¯”è¼ƒ | ğŸ”µ é«˜ | ç„¡é™åˆ¶ |
| **L2 (éƒ¨åˆ†)** | åƒ…ä¸»æ©Ÿ (Chillers only) | ä¸»æ©ŸåŠ ç¸½ vs System - è¼”åŠ©è¨­å‚™åŸºç·š | ğŸŸ¡ ä¸­ | åƒ…ä¾›è¶¨å‹¢åƒè€ƒï¼Œä¸ä½œç‚ºçµ•å°æ¨™æº– |
| **L1 (æ¥µé™)** | åƒ…å–®ä¸€ä¸»æ©Ÿ | ç„¡æ³•é€²è¡Œä¸€è‡´æ€§æª¢æŸ¥ | ğŸ”´ ä½ | ç¦æ­¢é€²è¡ŒHybridé©—è­‰ï¼Œæ”¹ç”¨å–®ä¸€æ¨¡å‹é©—è­‰ |
| **L0 (ç¼ºå¤±)** | ç„¡Components | è·³éHybridæª¢æŸ¥ | âšª N/A | åƒ…ä½¿ç”¨System Model |

**éƒ¨åˆ†ç¼ºå¤±æ™‚çš„æ•¸å­¸è™•ç†**ï¼š

ç•¶åƒ…æœ‰ä¸»æ©Ÿæ¨¡å‹å­˜åœ¨æ™‚ï¼ˆL2æƒ…å¢ƒï¼‰ï¼š
$$C_{partial} = \sum_{i \in chillers} \hat{y}_{C_i} + \hat{y}_{aux,baseline}$$

å…¶ä¸­ $\hat{y}_{aux,baseline}$ ç‚ºè¼”åŠ©è¨­å‚™ï¼ˆæ°´æ³µã€æ°´å¡”ï¼‰çš„**ç‰©ç†ä¼°è¨ˆå€¼**æˆ–**æ­·å²å¹³å‡å€¼**ï¼Œè€Œéæ¨¡å‹é æ¸¬ã€‚

**ç½®ä¿¡åº¦åŠ æ¬Š**ï¼š
$$Confidence = \frac{N_{available}}{N_{total}} \times \frac{Energy_{available}}{Energy_{total}}$$

- $N_{available}$ï¼šå¯ç”¨çš„Component Modelsæ•¸é‡
- $Energy_{available}$ï¼šé€™äº›è¨­å‚™åœ¨ç¸½èƒ½è€—ä¸­çš„æ­·å²ä½”æ¯”

### 4.2 ç¼ºå¤±è£œå„Ÿæ©Ÿåˆ¶ (Missing Component Imputation)

**ç­–ç•¥é¸æ“‡**ï¼š

```python
class MissingComponentHandler:
    """
    è™•ç†éƒ¨åˆ†Component Modelsç¼ºå¤±çš„æƒ…æ³
    """
    
    IMPUTATION_STRATEGIES = {
        'physical_model': 'ä½¿ç”¨ç‰©ç†æ¨¡å‹ä¼°ç®—ï¼ˆå¦‚æ³µ affinity lawsï¼‰',
        'historical_mean': 'ä½¿ç”¨æ­·å²åŒæ¢ä»¶ä¸‹çš„å¹³å‡å€¼',
        'ml_imputation': 'ä½¿ç”¨ç¨ç«‹çš„è¼•é‡ç´šMLæ¨¡å‹æ’è£œ',
        'proportional_split': 'ä¾å·²çŸ¥Componentsçš„æ¯”ä¾‹åˆ†é…æ®˜å·®'
    }
    
    def handle(
        self,
        available_components: Dict[str, np.ndarray],
        missing_components: List[str],
        system_prediction: np.ndarray,
        context: Dict  # é‹è¡Œå·¥æ³
    ) -> Tuple[np.ndarray, float]:
        """
        Returns:
            imputed_sum: è£œå„Ÿå¾Œçš„åŠ ç¸½é æ¸¬
            confidence: ç½®ä¿¡åº¦åˆ†æ•¸ (0.0 ~ 1.0)
        """
        available_sum = sum(available_components.values())
        
        if len(missing_components) == 0:
            return available_sum, 1.0
        
        # è¨ˆç®—å·²çŸ¥éƒ¨åˆ†çš„èƒ½è€—ä½”æ¯”
        energy_ratio = self._get_energy_ratio(available_components.keys())
        
        if energy_ratio > 0.8:  # å·²çŸ¥éƒ¨åˆ†ä½”æ¯”>80%ï¼Œå¯é€²è¡Œæ®˜å·®åˆ†é…
            # ç­–ç•¥ï¼šProportional Split
            residual = system_prediction - available_sum
            # ä¾æ­·å²æ¯”ä¾‹å°‡æ®˜å·®åˆ†é…çµ¦ç¼ºå¤±è¨­å‚™ï¼ˆç¢ºä¿åŠ ç¸½ç­‰æ–¼Systemï¼‰
            imputed_sum = available_sum + (residual * 0.5)  # ä¿å®ˆä¼°è¨ˆ
            confidence = energy_ratio
            
        elif len(missing_components) <= 2:
            # ç­–ç•¥ï¼šPhysical Model / Historical Mean
            imputed_missing = self._estimate_by_physics(missing_components, context)
            imputed_sum = available_sum + imputed_missing
            confidence = energy_ratio * 0.8  # é™ç´š
            
        else:
            # ç¼ºå¤±éå¤šï¼Œç„¡æ³•å¯é ä¼°è¨ˆ
            return None, 0.0
```

---

## 5. æ ¸å¿ƒæ¼”ç®—æ³•èˆ‡å¯¦ä½œè¦ç¯„

### 5.1 HybridConsistencyChecker é¡åˆ¥è¨­è¨ˆ

```python
class HybridConsistencyChecker:
    """
    Hybrid Modelä¸€è‡´æ€§æª¢æŸ¥å™¨ï¼ˆPRD v1.0 æ ¸å¿ƒå¯¦ä½œï¼‰
    æ•´åˆæ‰€æœ‰é©—è­‰é‚è¼¯èˆ‡å®¹å·®è¨ˆç®—
    """
    
    def __init__(
        self,
        config: ConsistencyConfig,
        annotation_manager: FeatureAnnotationManager
    ):
        self.config = config
        self.tolerance_calc = DynamicToleranceCalculator(config)
        self.missing_handler = MissingComponentHandler()
        self.copula_calibrator = CopulaEffectCalibrator()
        
        # é©—è­‰çµæœå¿«å–ï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
        self._cache = {}
    
    def validate(
        self,
        system_artifact: MultiModelArtifact,
        component_artifacts: Dict[str, MultiModelArtifact],
        dataset_source: str = "auto",  # "test", "val", "auto"
        context: Optional[Dict] = None
    ) -> ConsistencyReport:
        """
        åŸ·è¡Œå®Œæ•´çš„ä¸‰ç¶­åº¦ä¸€è‡´æ€§é©—è­‰
        
        Args:
            system_artifact: System-Levelæ¨¡å‹ç”¢å‡ºç‰©
            component_artifacts: Component-Levelæ¨¡å‹ç”¢å‡ºç‰©å­—å…¸
            dataset_source: æŒ‡å®šé»ƒé‡‘è³‡æ–™é›†ä¾†æº
            
        Returns:
            ConsistencyReport: è©³ç´°é©—è­‰å ±å‘Šï¼ˆè¦‹5.2ç¯€ï¼‰
        """
        # Step 1: é¸æ“‡é»ƒé‡‘è³‡æ–™é›†
        golden_data, source_tag = self._select_golden_dataset(
            system_artifact, dataset_source
        )
        
        # Step 2: æª¢æŸ¥Componentå®Œæ•´æ€§
        availability_level, missing = self._assess_component_availability(
            component_artifacts, context
        )
        
        # Step 3: åŸ·è¡Œé æ¸¬
        y_system = system_artifact.predict(golden_data)
        y_components = {
            name: art.predict(golden_data) 
            for name, art in component_artifacts.items()
        }
        
        # Step 4: è™•ç†ç¼ºå¤±ï¼ˆè‹¥éœ€è¦ï¼‰
        if availability_level in ['L2', 'L1']:
            y_component_sum, confidence = self.missing_handler.handle(
                y_components, missing, y_system, context
            )
        else:
            y_component_sum = sum(y_components.values())
            confidence = 1.0
        
        # Step 5: è¨ˆç®—ä¸‰ç¶­åº¦æŒ‡æ¨™
        sample_errors = np.abs(y_system - y_component_sum)
        binned_metrics = self._calculate_binned_metrics(
            y_system, y_component_sum, golden_data
        )
        overall_stats = self._calculate_overall_stats(
            y_system, y_component_sum
        )
        
        # Step 6: å‹•æ…‹å®¹å·®è©•ä¼°
        tolerances = self._calculate_dynamic_tolerances(golden_data, context)
        
        # Step 7: ç”Ÿæˆåˆ¤æ±º
        verdict = self._generate_verdict(
            sample_errors, binned_metrics, overall_stats, 
            tolerances, availability_level
        )
        
        return ConsistencyReport(
            verdict=verdict,
            metrics={
                'sample_level': sample_errors,
                'binned': binned_metrics,
                'overall': overall_stats
            },
            tolerances=tolerances,
            copula_analysis=self._analyze_copula_effects(
                y_system, y_components, golden_data
            ),
            availability_level=availability_level,
            confidence=confidence,
            golden_dataset_source=source_tag
        )
    
    def _calculate_binned_metrics(
        self, 
        y_system: np.ndarray, 
        y_component_sum: np.ndarray,
        data: pd.DataFrame
    ) -> Dict[str, Dict]:
        """
        è¨ˆç®—åˆ†ç¾¤çµ±è¨ˆæŒ‡æ¨™ï¼ˆä¾è² è¼‰ã€å¤–æ°£æº«åº¦ã€è¨­å‚™çµ„åˆï¼‰
        """
        bins = {}
        
        # ä¾ç³»çµ±è² è¼‰åˆ†ç¾¤
        for bin_name, mask in self._get_load_bins(data).items():
            if np.sum(mask) < 10:  # æ¨£æœ¬æ•¸ä¸è¶³è·³é
                continue
                
            bins[f"load_{bin_name}"] = {
                'mape': np.mean(np.abs(y_system[mask] - y_component_sum[mask]) / y_system[mask]) * 100,
                'mae': np.mean(np.abs(y_system[mask] - y_component_sum[mask])),
                'bias': np.mean(y_component_sum[mask] - y_system[mask]),
                'n_samples': np.sum(mask)
            }
        
        # ä¾å¤–æ°£æ¿•çƒæº«åº¦åˆ†ç¾¤
        for bin_name, mask in self._get_ambient_bins(data).items():
            if np.sum(mask) < 10:
                continue
                
            bins[f"ambient_{bin_name}"] = {
                'mape': np.mean(np.abs(y_system[mask] - y_component_sum[mask]) / y_system[mask]) * 100,
                'n_samples': np.sum(mask)
            }
        
        return bins
```

### 5.2 é©—è­‰å ±å‘Šæ ¼å¼ (ConsistencyReport)

```python
@dataclass
class ConsistencyReport:
    """
    Hybridä¸€è‡´æ€§é©—è­‰å ±å‘Šçµæ§‹
    """
    verdict: str  # "PASS", "WARNING", "FAIL", "INSUFFICIENT_DATA"
    verdict_code: str  # "E750", "E751", etc.
    
    # ä¸‰ç¶­åº¦æŒ‡æ¨™
    metrics: Dict[str, Any]
    
    # å®¹å·®è³‡è¨Š
    tolerances: Dict[str, float]  # å„ç¾¤çµ„çš„å‹•æ…‹å®¹å·®é–¾å€¼
    
    # è€¦åˆæ•ˆæ‡‰åˆ†æ
    copula_analysis: Dict[str, Any]
    
    # è³‡æ–™å“è³ª
    availability_level: str  # "L3", "L2", "L1", "L0"
    confidence: float  # 0.0 ~ 1.0
    golden_dataset_source: str
    
    # è¨ºæ–·è³‡è¨Š
    violations: List[Dict]  # é•è¦è©³ç´°è³‡è¨Š
    recommendations: List[str]  # æ”¹å–„å»ºè­°
    
    def to_dict(self) -> Dict:
        """åºåˆ—åŒ–ç‚ºå­—å…¸ï¼ˆä¾›JSONè¼¸å‡ºï¼‰"""
        return {
            'verdict': self.verdict,
            'verdict_code': self.verdict_code,
            'summary': self._generate_summary(),
            'metrics': self.metrics,
            'tolerance_analysis': {
                'applied_tolerances': self.tolerances,
                'base_tolerance': self.config.base_tolerance
            },
            'copula_effects': self.copula_analysis,
            'data_quality': {
                'availability_level': self.availability_level,
                'confidence': self.confidence,
                'golden_dataset': self.golden_dataset_source
            }
        }
    
    def _generate_summary(self) -> str:
        """ç”Ÿæˆäººé¡å¯è®€çš„æ‘˜è¦"""
        if self.verdict == "PASS":
            return f"âœ… Hybridä¸€è‡´æ€§æª¢æŸ¥é€šéï¼ˆä¿¡å¿ƒæ°´æº–ï¼š{self.confidence:.0%}ï¼‰"
        elif self.verdict == "WARNING":
            return f"âš ï¸ å­˜åœ¨è¼•å¾®åå·®ï¼Œå»ºè­°æª¢è¦–è€¦åˆæ•ˆæ‡‰ï¼ˆä¿¡å¿ƒæ°´æº–ï¼š{self.confidence:.0%}ï¼‰"
        else:
            return f"âŒ ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—ï¼Œå»ºè­°é‡æ–°è¨“ç·´Component Modelsæˆ–æª¢æŸ¥è³‡æ–™å“è³ª"
```

---

## 6. éŒ¯èª¤ä»£ç¢¼èˆ‡è­¦å‘Šä»£ç¢¼å°ç…§è¡¨

| ä»£ç¢¼ | é¡åˆ¥ | åç¨± | èªªæ˜ | è™•ç†å»ºè­° |
|:---|:---:|:---|:---|:---|
| **C001** | âœ… Pass | `FULL_CONSISTENCY` | æ‰€æœ‰ç¶­åº¦å‡é€šéå‹•æ…‹å®¹å·®æª¢æŸ¥ | ç„¡éœ€è™•ç† |
| **E758** | âš ï¸ Warning | `COPULA_EFFECT_DETECTED` | åµæ¸¬åˆ°é¡¯è‘—è€¦åˆæ•ˆæ‡‰ï¼ˆåå·®>3%ï¼‰ï¼Œä½†åœ¨å®¹å·®å…§ | è¨˜éŒ„ä¸¦ç›£æ§ï¼Œå¿…è¦æ™‚æ ¡æº–è€¦åˆä¿‚æ•¸ |
| **E757** | âš ï¸ Warning | `LIGHT_LOAD_HIGH_VARIANCE` | è¼•è¼‰å€é–“èª¤å·®è¼ƒé«˜ï¼ˆæ­£å¸¸ç¾è±¡ï¼‰ | ç¢ºèªç‚ºçµ±è¨ˆè®Šç•°ï¼Œéæ¨¡å‹éŒ¯èª¤ |
| **E756** | âš ï¸ Warning | `PARTIAL_COMPONENTS_L2` | åƒ…ä½¿ç”¨L2ç­‰ç´šï¼ˆéƒ¨åˆ†Componentsï¼‰é©—è­‰ | è£œå……è¨“ç·´ç¼ºå¤±çš„Component Models |
| **E751** | âŒ Fail | `EXCEEDS_DYNAMIC_TOLERANCE` | è¶…éå‹•æ…‹å®¹å·®é–¾å€¼ï¼ˆè‡³å°‘ä¸€å€‹ç¾¤çµ„ï¼‰ | æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹ä¸€è‡´æ€§æˆ–é‡æ–°è¨“ç·´ |
| **E752** | âŒ Fail | `SYSTEMATIC_BIAS_DETECTED` | åµæ¸¬åˆ°ç³»çµ±æ€§åå·® (Bias > 5%) | æª¢æŸ¥è¨­å‚™ä¾è³´é—œä¿‚æˆ–è³‡æ–™æ¨™è¨»éŒ¯èª¤ |
| **E753** | âŒ Fail | `TREND_MISMATCH` | ç›¸é—œä¿‚æ•¸ < 0.95ï¼Œè¶¨å‹¢ä¸ä¸€è‡´ | åš´é‡æ¨¡å‹éŒ¯èª¤ï¼Œéœ€æª¢æŸ¥è¨“ç·´è³‡æ–™æ™‚é–“å°é½Š |
| **E754** | âŒ Fail | `OUTLIER_VIOLATION` | å­˜åœ¨æ¥µç«¯ç•°å¸¸å€¼ï¼ˆå–®ç­†èª¤å·® > 50 kWï¼‰ | æª¢æŸ¥è©²æ™‚é–“é»çš„è³‡æ–™å“è³ªæˆ–è¨­å‚™ç•°å¸¸ |
| **E755** | âŒ Error | `INSUFFICIENT_COMPONENTS` | L1ç­‰ç´šï¼ˆåƒ…å–®ä¸€Componentï¼‰ï¼Œç„¡æ³•é©—è­‰ | è‡³å°‘éœ€è¨“ç·´æ‰€æœ‰ä¸»æ©Ÿæ¨¡å‹ |
| **E750** | âŒ Error | `GOLDEN_DATASET_UNAVAILABLE` | ç„¡å¯ç”¨çš„æ¸¬è©¦é›†æˆ–é©—è­‰é›† | é‡æ–°åŸ·è¡Œè¨“ç·´ç®¡ç·šç”¢ç”Ÿè³‡æ–™åˆ†å‰² |
| **E759** | âš ï¸ Warning | `DATASET_QUALITY_WARNING` | ä½¿ç”¨é©—è­‰é›†æˆ–åˆä½µè³‡æ–™é›†ï¼ˆåŸ W801/W802ï¼‰ | å»ºè­°é‡æ–°è¨“ç·´ä»¥ç¢ºä¿åš´è¬¹æ€§ |

---

## 7. èˆ‡ä¸Šä¸‹æ¸¸æ¨¡çµ„çš„æ•´åˆä»‹é¢

### 7.1 èˆ‡Training Pipelineæ•´åˆ (BatchTrainingCoordinator)

å–ä»£Training v1.2ä¸­ç°¡åŒ–çš„`_validate_hybrid_consistency()`æ–¹æ³•ï¼š

```python
# In BatchTrainingCoordinator (Training v1.2)
def _validate_hybrid_consistency(self, tolerance: float = 0.05):
    """
    å¼·åŒ–ç‰ˆHybridä¸€è‡´æ€§æª¢æŸ¥ï¼ˆå‘¼å«HybridConsistencyCheckerï¼‰
    """
    from src.modeling.validation.hybrid_consistency import HybridConsistencyChecker
    
    checker = HybridConsistencyChecker(
        config=ConsistencyConfig(base_tolerance=tolerance),
        annotation_manager=self.annotation_manager
    )
    
    report = checker.validate(
        system_artifact=self.artifacts['system_total_kw'],
        component_artifacts={
            k: v for k, v in self.artifacts.items() 
            if k != 'system_total_kw'
        },
        dataset_source="test",  # å¼·åˆ¶ä½¿ç”¨æ¸¬è©¦é›†
        context={'site_id': self.site_id}
    )
    
    # æ ¹æ“šå ±å‘Šæ±ºç­–
    if report.verdict == "FAIL":
        logger.error(f"âŒ Hybridä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—ï¼š{report.verdict_code}")
        # è§¸ç™¼E903ï¼ˆTraining v1.2å®šç¾©ï¼‰æˆ– E75x (Hybrid Consistency)
        raise HybridConsistencyViolation(report)
    elif report.verdict == "WARNING":
        logger.warning(f"âš ï¸ Hybridä¸€è‡´æ€§è­¦å‘Šï¼š{report.to_dict()['summary']}")
    
    return report
```

### 7.2 èˆ‡Optimization Engineæ•´åˆ (ModelRegistry)

å–ä»£Optimization v1.1ä¸­ç°¡åŒ–çš„`validate_hybrid_consistency()`ï¼š

```python
# In ModelRegistry (Optimization v1.1)
def validate_hybrid_consistency(
    self, 
    site_id: str,
    config: Dict, 
    ambient: Dict,
    tolerance: float = 0.05
) -> Tuple[bool, Dict]:
    """
    å›å‚³ï¼š(æ˜¯å¦é€šé, è©³ç´°å ±å‘Š)
    """
    checker = HybridConsistencyChecker(
        config=ConsistencyConfig(base_tolerance=tolerance),
        annotation_manager=self.annotation_manager
    )
    
    # è¼‰å…¥æ¨¡å‹
    system_art = self.load_from_registry(site_id, "system_total_kw")
    component_arts = {}
    for i in range(1, 5):  # å‡è¨­æœ€å¤š4å°ä¸»æ©Ÿ
        try:
            component_arts[f"chiller_{i}_kw"] = self.load_from_registry(
                site_id, f"chiller_{i}_kw"
            )
        except:
            continue
    
    # å»ºç«‹è©•ä¼°è³‡æ–™ï¼ˆå–®é»æˆ–æ‰¹æ¬¡ï¼‰
    eval_data = self._create_evaluation_data(config, ambient)
    
    report = checker.validate(
        system_artifact=system_art,
        component_artifacts=component_arts,
        dataset_source="auto",
        context={'evaluation_mode': 'single_point', 'config': config}
    )
    
    is_consistent = report.verdict in ["PASS", "WARNING"]
    
    return is_consistent, report.to_dict()
```

---

## 8. æ¸¬è©¦èˆ‡é©—è­‰è¨ˆç•«

### 8.1 å–®å…ƒæ¸¬è©¦

| æ¸¬è©¦ID | æè¿° | è¼¸å…¥ | é æœŸçµæœ |
|:---|:---|:---|:---|
| HCV-001 | é»ƒé‡‘è³‡æ–™é›†é¸æ“‡ | æ¸¬è©¦é›†æ¨£æœ¬æ•¸=150 | é¸æ“‡æ¸¬è©¦é›†ï¼Œç„¡è­¦å‘Š |
| HCV-002 | é»ƒé‡‘è³‡æ–™é›†é™ç´š | æ¸¬è©¦é›†æ¨£æœ¬æ•¸=50ï¼Œé©—è­‰é›†=80 | é¸æ“‡é©—è­‰é›†ï¼Œè§¸ç™¼W801 |
| HCV-003 | å‹•æ…‹å®¹å·®è¨ˆç®— | è¼•è¼‰(20%)ã€é›™æ©Ÿ | å®¹å·®=5%*1.5*1.0+0.5%=8.0% |
| HCV-004 | å‹•æ…‹å®¹å·®è¨ˆç®— | é‡è¼‰(80%)ã€ä¸‰æ©Ÿ | å®¹å·®=5%*0.8*1.2+0.5%=5.3% |
| HCV-005 | åˆ†ç¾¤MAPEè¨ˆç®— | è² è¼‰åˆ†ä½ˆï¼šè¼•è¼‰30ç­†ã€ä¸­è¼‰50ç­†ã€é‡è¼‰20ç­† | å›å‚³3å€‹ç¾¤çµ„çš„MAPEï¼Œè¼•è¼‰ç¾¤çµ„å®¹å·®è¼ƒé«˜ |
| HCV-006 | éƒ¨åˆ†ç¼ºå¤±è™•ç† | åƒ…chiller_1_kwå¯ç”¨ | å›å‚³L2ç­‰ç´šï¼Œä¿¡å¿ƒæ°´æº–ä¾èƒ½è€—ä½”æ¯”èª¿æ•´ |
| HCV-007 | è€¦åˆä¿‚æ•¸æ ¡æº– | æ­·å²åå·®+3%ï¼ˆåŠ ç¸½é«˜ä¼°ï¼‰ | æ ¡æº–å¾ŒÎ³=1.03 |
| HCV-008 | ç³»çµ±æ€§åå·®æª¢æ¸¬ | Bias=+6% | å›å‚³E752éŒ¯èª¤ |
| HCV-009 | æ¥µç«¯ç•°å¸¸å€¼æª¢æ¸¬ | å–®ç­†èª¤å·®=60kW | å›å‚³E754éŒ¯èª¤ |
| HCV-010 | å®Œæ•´é€šéæƒ…å¢ƒ | æ‰€æœ‰ç¾¤çµ„MAPE<å‹•æ…‹å®¹å·® | å›å‚³C001ï¼Œç„¡è­¦å‘Š |

### 8.2 æ•´åˆæ¸¬è©¦

| æ¸¬è©¦ID | æè¿° | é©—è­‰ç›®æ¨™ |
|:---|:---|:---|
| INT-HCV-001 | E2E Trainingæ•´åˆ | BatchTrainingCoordinatoræ­£ç¢ºå‘¼å«ä¸¦è™•ç†å ±å‘Š |
| INT-HCV-002 | E2E Optimizationæ•´åˆ | ModelRegistryæ­£ç¢ºä½¿ç”¨Checkerä¸¦å›å‚³ç›¸å®¹æ ¼å¼ |
| INT-HCV-003 | å¤šæ¡ˆå ´æ‹“æ’²é©é… | ä¸åŒç®¡è·¯æ‹“æ’²ï¼ˆä¸²è¯/ä¸¦è¯ï¼‰æ­£ç¢ºæ‡‰ç”¨ä¸åŒè€¦åˆä¿‚æ•¸ |
| INT-HCV-004 | å­£ç¯€æ€§è€¦åˆè®ŠåŒ– | å¤å­£/å†¬å­£è€¦åˆæ•ˆæ‡‰ä¸åŒï¼Œå‹•æ…‹èª¿æ•´å®¹å·® |
| INT-HCV-005 | æ¥µç«¯å·¥æ³å£“åŠ›æ¸¬è©¦ | 100%è² è¼‰+é«˜å¤–æ°£æº«ï¼Œé©—è­‰å®¹å·®ä¸æ”¶ç·Šè‡³ä¸å¯è¡Œ |

### 8.3 é©—æ”¶æ¨™æº–

- [ ] **é»ƒé‡‘æ¨™æº–**ï¼šé è¨­ä½¿ç”¨æ¸¬è©¦é›†ï¼Œæ¨£æœ¬ä¸è¶³æ™‚è‡ªå‹•é™ç´šä¸¦è§¸ç™¼è­¦å‘Š
- [ ] **ä¸‰ç¶­åº¦é©—è­‰**ï¼šåŒæ™‚è¼¸å‡ºæ¨£æœ¬ç´šã€åˆ†ç¾¤ç´šã€æ•´é«”ç´šæŒ‡æ¨™
- [ ] **å‹•æ…‹å®¹å·®**ï¼šä¾è² è¼‰å€é–“è‡ªå‹•èª¿æ•´å®¹å·®ï¼ˆè¼•è¼‰7.5%ã€é‡è¼‰4%ï¼‰
- [ ] **è€¦åˆè£œå„Ÿ**ï¼šè‡ªå‹•æ ¡æº–ä¸¦æ‡‰ç”¨è€¦åˆä¿‚æ•¸ï¼ˆÎ³=0.98~1.05ï¼‰
- [ ] **éƒ¨åˆ†ç¼ºå¤±**ï¼šL2ç­‰ç´šæ™‚æ­£ç¢ºè¨ˆç®—ä¿¡å¿ƒæ°´æº–ä¸¦é™ç´šä½¿ç”¨
- [ ] **éŒ¯èª¤åˆ†é¡**ï¼šæ­£ç¢ºå€åˆ†C001(é€šé)ã€E758(è­¦å‘Š)ã€E751(å¤±æ•—)
- [ ] **ç‰©ç†æ¨™ç±¤**ï¼šæ‰€æœ‰é•è¦å ±å‘Šé™„å¸¶ç‰©ç†åŸå› æ¨™ç±¤ï¼ˆå£“æã€ç†±çŸ­è·¯ç­‰ï¼‰
- [ ] **æ•ˆèƒ½**ï¼šè™•ç†10,000ç­†è³‡æ–™çš„é©—è­‰<2ç§’
- [ ] **ç›¸å®¹æ€§**ï¼šèˆ‡Training v1.2èˆ‡Optimization v1.1ç„¡ç¸«æ•´åˆ

---

## 9. é™„éŒ„

### Appendix A: è€¦åˆæ•ˆæ‡‰ç†±å€åœ–ç¯„ä¾‹

```python
# è¦–è¦ºåŒ–è¨­å‚™é–“äº¤äº’ä½œç”¨å¼·åº¦
copula_heatmap = {
    'chiller_1': {'chiller_2': 0.03, 'pump_1': 0.01, 'tower_1': 0.02},
    'chiller_2': {'chiller_1': 0.03, 'pump_2': 0.01, 'tower_2': 0.02},
    'pump_1': {'chiller_1': 0.01, 'pump_2': 0.005},  # æ°´åŠ›è€¦åˆ
    'tower_1': {'tower_2': 0.04}  # ç†±çŸ­è·¯æ•ˆæ‡‰
}
# æ•¸å€¼ä»£è¡¨é æœŸåå·®ç™¾åˆ†æ¯”ï¼ˆ+è¡¨ç¤ºåŠ ç¸½é«˜ä¼°ï¼‰
```

### Appendix B: é…ç½®ç¯„ä¾‹ (consistency_config.yaml)

```yaml
schema_version: "1.0"

# åŸºç¤å®¹å·®è¨­å®š
base_tolerance: 5.0  # %
measurement_error_buffer: 0.5  # %

# å‹•æ…‹èª¿æ•´åƒæ•¸
dynamic_adjustment:
  load_factors:
    light: 1.5    # <30%
    medium: 1.0   # 30-70%
    heavy: 0.8    # >70%
  
  complexity_factors:
    single: 0.9
    dual: 1.0
    multiple: 1.2  # >2 units, +0.1 per additional unit

# è€¦åˆæ•ˆæ‡‰é è¨­å€¼ï¼ˆä¾æ¡ˆå ´é¡å‹ï¼‰
copula_defaults:
  standard_parallel: 1.02
  series_connected: 1.05  # ä¸²è¯ç®¡è·¯å£“æè¼ƒå¤§
  primary_secondary: 1.03  # ä¸€æ¬¡å´äºŒæ¬¡å´ç³»çµ±

# é©—è­‰é€šéæ¨™æº–
pass_criteria:
  max_mape_per_bin: dynamic  # ä½¿ç”¨å‹•æ…‹å®¹å·®
  max_overall_mape: 5.0      # ç¡¬æ€§ä¸Šé™ï¼ˆå¾Œå‚™ï¼‰
  min_correlation: 0.95
  max_systematic_bias: 3.0   # %
  max_single_error: 50.0     # kWï¼Œçµ•å°å€¼

# è³‡æ–™é›†é¸æ“‡åå¥½
dataset_preference:
  priority: ["test", "val"]
  min_samples_per_bin: 10
  allow_combination: true
```

### Appendix C: å ±å‘Šè¼¸å‡ºç¯„ä¾‹ (JSON)

```json
{
  "verdict": "WARNING",
  "verdict_code": "E758",
  "summary": "âš ï¸ å­˜åœ¨è¼•å¾®åå·®ï¼Œå»ºè­°æª¢è¦–è€¦åˆæ•ˆæ‡‰ï¼ˆä¿¡å¿ƒæ°´æº–ï¼š95%ï¼‰",
  "metrics": {
    "overall": {
      "mape": 3.2,
      "mae": 12.5,
      "bias": 2.8,
      "correlation": 0.98
    },
    "binned": {
      "load_light": {"mape": 6.5, "tolerance": 7.5, "pass": true},
      "load_medium": {"mape": 2.8, "tolerance": 5.5, "pass": true},
      "load_heavy": {"mape": 2.1, "tolerance": 4.5, "pass": true},
      "ambient_high": {"mape": 4.2, "tolerance": 5.5, "pass": true}
    }
  },
  "copula_analysis": {
    "detected_effects": ["piping_loss", "thermal_short"],
    "estimated_bias": 2.8,
    "calibrated_gamma": 1.03,
    "confidence": "medium"
  },
  "data_quality": {
    "availability_level": "L3",
    "confidence": 0.95,
    "golden_dataset": "test"
  },
  "recommendations": [
    "è¼•è¼‰å€é–“èª¤å·®è¼ƒé«˜ç‚ºæ­£å¸¸ç¾è±¡ï¼Œå»ºè­°ç›£æ§ä½†ä¸éœ€ä¿®æ­£æ¨¡å‹",
    "åµæ¸¬åˆ°ç®¡è·¯å£“æè€¦åˆæ•ˆæ‡‰ï¼Œå»ºè­°åœ¨Optimizationéšæ®µåŠ å…¥å£“æè£œå„Ÿä¿‚æ•¸"
  ]
}
```

---

**é—œéµè¨­è¨ˆç¢ºèª**:
1. **é»ƒé‡‘æ¨™æº–æ˜ç¢ºåŒ–**ï¼šå¼·åˆ¶ä½¿ç”¨æ¸¬è©¦é›†ï¼Œå®šç¾©ä¸‰ç¶­åº¦ï¼ˆæ¨£æœ¬/åˆ†ç¾¤/æ•´é«”ï¼‰æ¯”è¼ƒæ–¹æ³•è«–
2. **å‹•æ…‹å®¹å·®**ï¼šå–ä»£å›ºå®š5%ï¼Œæ¡ç”¨è² è¼‰èˆ‡è¤‡é›œåº¦èª¿æ•´æ¨¡å‹ï¼Œæ›´ç¬¦åˆHVACç‰©ç†ç‰¹æ€§
3. **è€¦åˆæ•ˆæ‡‰è£œå„Ÿ**ï¼šè­˜åˆ¥ä¸¦é‡åŒ–è¨­å‚™é–“äº¤äº’ä½œç”¨ï¼Œé¿å…èª¤åˆ¤æ­£å¸¸ç‰©ç†ç¾è±¡
4. **å„ªé›…é™ç´š**ï¼šå®šç¾©L0-L3ç­‰ç´šï¼Œè™•ç†Component Modelséƒ¨åˆ†ç¼ºå¤±æƒ…å¢ƒ
5. **ç‰©ç†å¯è§£é‡‹æ€§**ï¼šæ‰€æœ‰åå·®æ¨™è¨˜ç‰©ç†åŸå› ï¼ˆå£“æã€ç†±çŸ­è·¯ç­‰ï¼‰ï¼Œæ”¯æ´å·¥ç¨‹æ±ºç­–
6. **ç„¡ç¸«æ•´åˆ**ï¼šå‘å¾Œç›¸å®¹Training v1.2èˆ‡Optimization v1.1çš„æ—¢æœ‰ä»‹é¢
```