<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PRD_Model_Training_v1.0</title>

<style>
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
    font-size: 16px;
    line-height: 1.5;
    word-wrap: break-word;
    color: #24292e;
    background-color: #fff;
    max-width: 900px;
    margin: 0 auto;
    padding: 2rem;
}
h1, h2, h3, h4, h5, h6 {
    margin-top: 24px;
    margin-bottom: 16px;
    font-weight: 600;
    line-height: 1.25;
}
h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
h3 { font-size: 1.25em; }
code {
    padding: 0.2em 0.4em;
    margin: 0;
    font-size: 85%;
    background-color: #f6f8fa;
    border-radius: 6px;
    font-family: SFMono-Regular, Consolas, "Liberation Mono", Menlo, monospace;
}
pre {
    padding: 16px;
    overflow: auto;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f6f8fa;
    border-radius: 6px;
}
pre code {
    display: inline;
    padding: 0;
    margin: 0;
    overflow: visible;
    line-height: inherit;
    word-wrap: normal;
    background-color: initial;
    border: 0;
}
blockquote {
    padding: 0 1em;
    color: #6a737d;
    border-left: 0.25em solid #dfe2e5;
    margin: 0;
}
table {
    display: block;
    width: 100%;
    margin-top: 0;
    margin-bottom: 16px;
    overflow: auto;
    border-spacing: 0;
    border-collapse: collapse;
}
table th, table td {
    padding: 6px 13px;
    border: 1px solid #dfe2e5;
}
table tr {
    background-color: #fff;
    border-top: 1px solid #c6cbd1;
}
table tr:nth-child(2n) {
    background-color: #f6f8fa;
}
a { color: #0366d6; text-decoration: none; }
a:hover { text-decoration: underline; }
hr {
    height: 0.25em;
    padding: 0;
    margin: 24px 0;
    background-color: #e1e4e8;
    border: 0;
}
</style>

</head>
<body>
<h1 id="prd-v10-model-training-pipeline-implementation-guide">PRD v1.0: æ¨¡å‹è¨“ç·´ç®¡ç·šå¯¦ä½œæŒ‡å— (Model Training Pipeline Implementation Guide)</h1>
<p><strong>æ–‡ä»¶ç‰ˆæœ¬:</strong> v1.0 (Multi-Model Ensemble &amp; Temporal-Aware Training)<br />
<strong>æ—¥æœŸ:</strong> 2026-02-13<br />
<strong>è² è²¬äºº:</strong> Oscar Chang<br />
<strong>ç›®æ¨™æ¨¡çµ„:</strong> <code>src/modeling/training_pipeline.py</code>, <code>src/modeling/trainers/</code><br />
<strong>ä¸Šæ¸¸å¥‘ç´„:</strong> <code>src/etl/feature_engineer.py</code> (v1.3-FA+, æª¢æŸ¥é» #4)<br />
<strong>ä¸‹æ¸¸å¥‘ç´„:</strong> <code>src/optimization/engine.py</code> (v1.0+, è¼¸å…¥æª¢æŸ¥é»)<br />
<strong>æ”¯æ´æ¨¡å‹:</strong> <br />
- <strong>XGBoost</strong> (Extreme Gradient Boosting) - é«˜ç²¾åº¦ã€å¯è§£é‡‹æ€§å¼·<br />
- <strong>LightGBM</strong> (Light Gradient Boosting Machine) - å¤§è¦æ¨¡è³‡æ–™ã€è¨“ç·´æ¥µé€Ÿ<br />
- <strong>Random Forest</strong> (Bagging Ensemble) - é«˜é²æ£’æ€§ã€æŠ—éæ“¬åˆã€åŸºå‡†æ¨¡å‹<br />
<strong>é ä¼°å·¥æ™‚:</strong> 6 ~ 7 å€‹å·¥ç¨‹å¤©ï¼ˆå«ä¸‰æ¨¡å‹æ•´åˆã€è¶…åƒæ•¸æœå°‹ã€æ¨¡å‹é¸æ“‡æ©Ÿåˆ¶ï¼‰</p>
<hr />
<h2 id="1">1. åŸ·è¡Œç¸½ç¶±èˆ‡è¨­è¨ˆå“²å­¸</h2>
<h3 id="11">1.1 æ ¸å¿ƒç›®æ¨™</h3>
<p>å»ºç«‹<strong>å¤šæ¨¡å‹å¹³è¡Œè¨“ç·´ (Multi-Model Training)</strong>ã€<strong>è‡ªå‹•æ¨¡å‹é¸æ“‡ (AutoML Selection)</strong>ã€<strong>æ™‚åºæ„ŸçŸ¥ (Temporal-Aware)</strong> çš„è¨“ç·´ç®¡ç·šï¼š</p>
<ol>
<li><strong>ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´</strong>: åŒæ™‚è¨“ç·´ XGBoostã€LightGBMã€Random Forestï¼Œè‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹</li>
<li><strong>é›¶è³‡æ–™æ´©æ¼ (Zero Data Leakage)</strong>: åš´æ ¼éµå®ˆ <code>temporal_cutoff</code>ï¼Œè¨“ç·´è³‡æ–™çµ•ä¸åŒ…å«é©—è­‰/æ¸¬è©¦æœŸçš„æœªä¾†è³‡è¨Š</li>
<li><strong>Device Role æ„ŸçŸ¥</strong>: æ­£ç¢ºè™•ç† <code>device_role</code>ï¼ˆprimary/backup/seasonalï¼‰ï¼Œä½œç‚º<strong>æ¨£æœ¬æ¬Šé‡ (Sample Weighting)</strong> èˆ‡<strong>åˆ†å±¤ä¾æ“š (Stratification Basis)</strong></li>
<li><strong>è¶…åƒæ•¸è‡ªå‹•æœå°‹</strong>: æ”¯æ´ Optuna è‡ªå‹•åŒ–è¶…åƒæ•¸èª¿å„ªï¼ˆå¯é¸ï¼‰</li>
<li><strong>ç‰ˆæœ¬å¯è¿½æº¯ (Version Traceability)</strong>: æ¯å€‹è¨“ç·´ç”¢å‡ºçš„æ¨¡å‹å¿…é ˆç¶å®šç•¶æ™‚çš„ <code>schema_version</code>ã€<code>inheritance_chain</code> èˆ‡ <code>yaml_checksum</code></li>
</ol>
<h3 id="12">1.2 ä¸‰æ¨¡å‹ç‰¹æ€§æ¯”è¼ƒèˆ‡é©ç”¨å ´æ™¯</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">æ¨¡å‹</th>
<th style="text-align: center;">æ¼”ç®—æ³•é¡å‹</th>
<th style="text-align: left;">å„ªå‹¢</th>
<th style="text-align: left;">æœ€ä½³é©ç”¨å ´æ™¯</th>
<th style="text-align: center;">ç‰¹å¾µé‡è¦æ€§</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>XGBoost</strong></td>
<td style="text-align: center;">Gradient Boosting (Level-wise)</td>
<td style="text-align: left;">ç²¾åº¦æ¥µé«˜ã€æ­£å‰‡åŒ–å¼·ã€ä¸æ˜“éæ“¬åˆã€ç”Ÿæ…‹ç³»å®Œæ•´</td>
<td style="text-align: left;">ä¸­ç­‰è³‡æ–™é‡ (&lt;100è¬ç­†)ã€é«˜ç¶­åº¦ç‰¹å¾µã€éœ€å¯è§£é‡‹æ€§</td>
<td style="text-align: center;">Gain-based</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LightGBM</strong></td>
<td style="text-align: center;">Gradient Boosting (Leaf-wise)</td>
<td style="text-align: left;">è¨“ç·´é€Ÿåº¦æ¥µå¿«ã€è¨˜æ†¶é«”æ•ˆç‡é«˜ã€æ”¯æ´é¡åˆ¥ç‰¹å¾µè‡ªå‹•ç·¨ç¢¼</td>
<td style="text-align: left;">å¤§è¦æ¨¡è³‡æ–™ (&gt;10è¬ç­†)ã€å³æ™‚è¨“ç·´éœ€æ±‚ã€é«˜åŸºæ•¸é¡åˆ¥ç‰¹å¾µ</td>
<td style="text-align: center;">Split-based</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Random Forest</strong></td>
<td style="text-align: center;">Bagging (Parallel Trees)</td>
<td style="text-align: left;">æ¥µé«˜é²æ£’æ€§ã€å¤©ç„¶æ”¯æ´å¹³è¡Œé‹ç®—ã€å°ç•°å¸¸å€¼ä¸æ•æ„Ÿã€ç„¡éœ€å¤§é‡èª¿åƒ</td>
<td style="text-align: left;">å¿«é€ŸåŸºå‡†æ¸¬è©¦ã€è³‡æ–™å«å™ªéŸ³ã€éœ€ç©©å®šé æ¸¬å€é–“</td>
<td style="text-align: center;">Mean Decrease Impurity</td>
</tr>
</tbody>
</table>
<p><strong>é è¨­ç­–ç•¥</strong>: ä¸‰æ¨¡å‹åŒæ™‚è¨“ç·´ï¼Œä¾é©—è­‰é›† RÂ² åˆ†æ•¸è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ï¼Œæˆ–ä¿ç•™ä¸‰æ¨¡å‹åš Ensemble æŠ•ç¥¨ã€‚</p>
<hr />
<h2 id="2-interface-contracts">2. ä»‹é¢å¥‘ç´„è¦ç¯„ (Interface Contracts)</h2>
<h3 id="21-input-contract-from-feature-engineer-v13">2.1 è¼¸å…¥å¥‘ç´„ (Input Contract from Feature Engineer v1.3)</h3>
<p><strong>æª¢æŸ¥é» #7: Feature Engineer â†’ Model Training</strong></p>
<pre><code class="language-python">class TrainingInputContract(BaseModel):
    &quot;&quot;&quot;æ¨¡å‹è¨“ç·´è¼¸å…¥è³‡æ–™è¦ç¯„&quot;&quot;&quot;

    # 1. ç‰¹å¾µçŸ©é™£ (ä¾†è‡ª Feature Engineer)
    feature_matrix: pl.DataFrame

    # 2. ç›®æ¨™è®Šæ•¸è³‡è¨Š
    target_variable: str
    target_metadata: FeatureMetadata

    # 3. æ™‚é–“æˆ³è¨˜
    timestamp_col: str = &quot;timestamp&quot;
    time_range: Dict[str, str]

    # 4. Annotation ä¸Šä¸‹æ–‡ï¼ˆç‰ˆæœ¬ç¶å®šï¼‰
    annotation_context: Dict = {
        &quot;schema_version&quot;: &quot;1.2&quot;,
        &quot;inheritance_chain&quot;: &quot;base -&gt; cgmh_ty&quot;,
        &quot;yaml_checksum&quot;: &quot;sha256:abc123...&quot;,
        &quot;group_policies_applied&quot;: [&quot;chillers&quot;, &quot;towers&quot;],
        &quot;feature_engineer_version&quot;: &quot;1.3-FA&quot;
    }

    # 5. ç‰¹å¾µå…ƒè³‡æ–™ï¼ˆä¸å« device_roleï¼‰
    feature_metadata: Dict[str, FeatureMetadata]

    # 6. Quality Flag ç‰¹å¾µåˆ—è¡¨
    quality_flag_features: List[str]

    # 7. é˜² Data Leakage è³‡è¨Š
    train_test_split_info: Dict = {
        &quot;temporal_cutoff&quot;: &quot;2025-10-01T00:00:00Z&quot;,
        &quot;strict_past_only&quot;: True
    }

    # 8. æ¨£æœ¬æ¬Šé‡å»ºè­°ï¼ˆå¯é¸ï¼‰
    suggested_sample_weights: Optional[pl.Series] = None

    # 9. è³‡æ–™è¦æ¨¡æ¨™è¨˜ï¼ˆç”¨æ–¼æ¨¡å‹é¸æ“‡å»ºè­°ï¼‰
    n_samples: int
    n_features: int
</code></pre>
<table>
<thead>
<tr>
<th style="text-align: left;">æª¢æŸ¥é …</th>
<th style="text-align: left;">è¦æ ¼</th>
<th style="text-align: center;">éŒ¯èª¤ä»£ç¢¼</th>
<th style="text-align: left;">è™•ç†</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Annotation Context å­˜åœ¨æ€§</strong></td>
<td style="text-align: left;">å¿…é ˆéç©ºä¸”åŒ…å« <code>schema_version</code>, <code>inheritance_chain</code>, <code>yaml_checksum</code></td>
<td style="text-align: center;">E601</td>
<td style="text-align: left;">æ‹’çµ•è¨“ç·´</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Schema ç‰ˆæœ¬ç›¸å®¹</strong></td>
<td style="text-align: left;"><code>schema_version</code> å¿…é ˆç­‰æ–¼ç•¶å‰ <code>FEATURE_ANNOTATION_CONSTANTS['expected_schema_version']</code></td>
<td style="text-align: center;">E602</td>
<td style="text-align: left;">æ‹’çµ•è¨“ç·´</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ç›®æ¨™è®Šæ•¸å­˜åœ¨</strong></td>
<td style="text-align: left;"><code>target_variable</code> å¿…é ˆå­˜åœ¨æ–¼ <code>feature_matrix</code> æ¬„ä½ä¸­</td>
<td style="text-align: center;">E603</td>
<td style="text-align: left;">æ‹’çµ•è¨“ç·´</td>
</tr>
<tr>
<td style="text-align: left;"><strong>æ™‚é–“æˆ³å‹åˆ¥</strong></td>
<td style="text-align: left;"><code>timestamp</code> å¿…é ˆç‚º <code>Datetime(ns, UTC)</code></td>
<td style="text-align: center;">E604</td>
<td style="text-align: left;">æ‹’çµ•è¨“ç·´</td>
</tr>
<tr>
<td style="text-align: left;"><strong>è³‡æ–™è¦æ¨¡æª¢æŸ¥</strong></td>
<td style="text-align: left;"><code>n_samples</code> å¿…é ˆ &gt;= 100ï¼ˆæ¯å€‹æ¨¡å‹æœ€ä½éœ€æ±‚ï¼‰</td>
<td style="text-align: center;">E607</td>
<td style="text-align: left;">æ‹’çµ•è¨“ç·´</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="3-phase-based-implementation">3. åˆ†éšæ®µå¯¦ä½œè¨ˆç•« (Phase-Based Implementation)</h2>
<h3 id="phase-0-day-1">Phase 0: åŸºç¤å»ºè¨­èˆ‡å¤šæ¨¡å‹æ¶æ§‹ (Day 1)</h3>
<h4 id="step-01">Step 0.1: çµ±ä¸€è¨“ç·´é…ç½®æ¨¡å‹ï¼ˆä¸‰æ¨¡å‹æ”¯æ´ï¼‰</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/config_models.py</code></p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">from typing import Dict, List, Optional, Literal, Final, Union
from pydantic import BaseModel, Field, validator
from datetime import datetime

from src.etl.config_models import (
    VALID_QUALITY_FLAGS,
    TIMESTAMP_CONFIG,
    FEATURE_ANNOTATION_CONSTANTS
)

EXPECTED_SCHEMA_VERSION: Final[str] = FEATURE_ANNOTATION_CONSTANTS['expected_schema_version']

# ==========================================
# æ¨¡å‹ç‰¹å®šè¶…åƒæ•¸é…ç½®
# ==========================================

class XGBoostConfig(BaseModel):
    &quot;&quot;&quot;XGBoost å°ˆå±¬é…ç½® - Level-wise ç”Ÿé•·ç­–ç•¥ï¼Œç²¾åº¦å°å‘&quot;&quot;&quot;
    n_estimators: int = 1000
    learning_rate: float = 0.05
    max_depth: int = 6
    min_child_weight: int = 1
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    reg_alpha: float = 0.1  # L1 æ­£å‰‡
    reg_lambda: float = 1.0  # L2 æ­£å‰‡
    gamma: float = 0  # ç¯€é»åˆ†è£‚æœ€å°æå¤±æ¸›å°‘
    early_stopping_rounds: int = 50
    eval_metric: str = &quot;rmse&quot;
    tree_method: str = &quot;hist&quot;  # 'exact', 'approx', 'hist'

    # é€²éšåŠŸèƒ½
    enable_monotonic_constraints: bool = False  # è‹¥ç‰©ç†é—œä¿‚å·²çŸ¥ï¼ˆå¦‚æº«åº¦è¶Šé«˜è€—é›»è¶Šé«˜ï¼‰
    monotone_constraints: Optional[Dict[str, int]] = None  # {&quot;temp_outdoor&quot;: 1, &quot;efficiency&quot;: -1}

class LightGBMConfig(BaseModel):
    &quot;&quot;&quot;LightGBM å°ˆå±¬é…ç½® - Leaf-wise ç”Ÿé•·ç­–ç•¥ï¼Œé€Ÿåº¦å°å‘&quot;&quot;&quot;
    n_estimators: int = 1000
    learning_rate: float = 0.05
    num_leaves: int = 31  # æ§åˆ¶æ¨¡å‹è¤‡é›œåº¦ï¼Œç›¸ç•¶æ–¼ 2^max_depth
    max_depth: int = -1  # -1 è¡¨ç¤ºç„¡é™åˆ¶ï¼Œç”± num_leaves æ§åˆ¶
    min_child_samples: int = 20
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    reg_alpha: float = 0.1
    reg_lambda: float = 1.0
    early_stopping_rounds: int = 50
    eval_metric: str = &quot;rmse&quot;
    boosting_type: str = &quot;gbdt&quot;  # 'gbdt', 'dart', 'goss'

    # å¤§è¦æ¨¡è³‡æ–™å„ªåŒ–
    feature_pre_filter: bool = False
    histogram_pool_size: Optional[int] = None  # è¨˜æ†¶é«”é™åˆ¶æ™‚è¨­å®š

class RandomForestConfig(BaseModel):
    &quot;&quot;&quot;Random Forest å°ˆå±¬é…ç½® - Bagging ç­–ç•¥ï¼Œé²æ£’æ€§å°å‘&quot;&quot;&quot;
    n_estimators: int = 500
    max_depth: Optional[int] = None  # None è¡¨ç¤ºå®Œå…¨ç”Ÿé•·
    min_samples_split: int = 5
    min_samples_leaf: int = 2
    max_features: str = &quot;sqrt&quot;  # 'sqrt', 'log2', None
    bootstrap: bool = True
    oob_score: bool = True  # Out-of-Bag é©—è­‰
    n_jobs: int = -1  # ä½¿ç”¨æ‰€æœ‰ CPU
    warm_start: bool = False  # å¯å¢é‡è¨“ç·´

    # å€é–“é æ¸¬ï¼ˆä½¿ç”¨æ¨¹çš„è‘‰ç¯€é»çµ±è¨ˆï¼‰
    quantile_regression: bool = False  # è‹¥å•Ÿç”¨ï¼Œè¨“ç·´ä¸‰å€‹æ¨¡å‹ (Q10, Q50, Q90)

# ==========================================
# è¨“ç·´ç®¡ç·šä¸»é…ç½®
# ==========================================

class ModelTrainingConfig(BaseModel):
    &quot;&quot;&quot;æ¨¡å‹è¨“ç·´çµ±ä¸€é…ç½®ï¼ˆæ”¯æ´ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´ï¼‰&quot;&quot;&quot;

    # åŸºæœ¬é…ç½®
    random_state: int = 42
    parallel_training: bool = True  # æ˜¯å¦åŒæ™‚è¨“ç·´ä¸‰æ¨¡å‹
    auto_select_best: bool = True   # è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹
    ensemble_voting: bool = False   # æ˜¯å¦ä¿ç•™ä¸‰æ¨¡å‹åšæŠ•ç¥¨å¹³å‡

    # æ™‚åºé…ç½®
    temporal_split: TemporalSplitConfig = TemporalSplitConfig()

    # Device Role è™•ç†
    device_role_handling: DeviceRoleHandlingConfig = DeviceRoleHandlingConfig()

    # ç‰¹å¾µå·¥ç¨‹ï¼ˆè¨“ç·´æœŸï¼‰
    handle_missing_values: Literal[&quot;drop&quot;, &quot;impute_mean&quot;, &quot;impute_median&quot;] = &quot;impute_median&quot;
    scale_features: bool = True  # å° SVM/NN å¿…è¦ï¼Œå°æ¨¹æ¨¡å‹å¯é¸ä½†å»ºè­°çµ±ä¸€

    # Quality Flags è™•ç†
    use_quality_flags_as_features: bool = True
    exclude_bad_quality_samples: bool = True

    # ä¸‰æ¨¡å‹é…ç½®
    xgboost: XGBoostConfig = XGBoostConfig()
    lightgbm: LightGBMConfig = LightGBMConfig()
    random_forest: RandomForestConfig = RandomForestConfig()

    # è¶…åƒæ•¸æœå°‹ï¼ˆå¯é¸ï¼‰
    enable_hyperparameter_search: bool = False
    hyperparameter_trials: int = 50  # Optuna trials
    hyperparameter_timeout: int = 3600  # ç§’

    # è¼¸å‡º
    model_output_dir: str = &quot;models/trained&quot;
    metadata_output_dir: str = &quot;models/metadata&quot;

    @validator('device_role_handling')
    def validate_no_feature_leakage(cls, v):
        if v.use_as_feature:
            raise ValueError(&quot;E701: device_role ç¦æ­¢ä½œç‚ºç›´æ¥ç‰¹å¾µè¼¸å…¥&quot;)
        return v
</code></pre>
<h4 id="step-02">Step 0.2: å¤šæ¨¡å‹è¨“ç·´å™¨åŸºç¤é¡åˆ¥</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/trainers/base_trainer.py</code></p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple, Optional
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

class BaseModelTrainer(ABC):
    &quot;&quot;&quot;
    æ¨¡å‹è¨“ç·´å™¨æŠ½è±¡åŸºç¤é¡åˆ¥

    æ‰€æœ‰å…·é«”æ¨¡å‹è¨“ç·´å™¨ï¼ˆXGBoostTrainer, LightGBMTrainer, RandomForestTrainerï¼‰
    å¿…é ˆå¯¦ä½œä»¥ä¸‹ä»‹é¢ã€‚
    &quot;&quot;&quot;

    def __init__(self, config: Any, random_state: int = 42):
        self.config = config
        self.random_state = random_state
        self.model = None
        self.feature_importance = {}
        self.training_history = {}

    @abstractmethod
    def train(
        self, 
        X_train: np.ndarray, 
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;
        åŸ·è¡Œæ¨¡å‹è¨“ç·´

        Returns:
            Dict åŒ…å«:
            - model: è¨“ç·´å¥½çš„æ¨¡å‹ç‰©ä»¶
            - best_iteration: æœ€ä½³è¿­ä»£æ¬¡æ•¸ï¼ˆæ¢¯åº¦æå‡é¡ï¼‰
            - training_history: è¨“ç·´éç¨‹æŒ‡æ¨™
            - feature_importance: ç‰¹å¾µé‡è¦æ€§å­—å…¸
            - oob_score: Out-of-Bag åˆ†æ•¸ï¼ˆè‹¥æœ‰ï¼‰
        &quot;&quot;&quot;
        pass

    @abstractmethod
    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        &quot;&quot;&quot;åŸ·è¡Œé æ¸¬&quot;&quot;&quot;
        pass

    @abstractmethod
    def get_feature_importance(self) -&gt; Dict[str, float]:
        &quot;&quot;&quot;å–å¾—æ¨™æº–åŒ–ç‰¹å¾µé‡è¦æ€§ï¼ˆç¸½å’Œç‚º1ï¼‰&quot;&quot;&quot;
        pass

    def evaluate(self, X: np.ndarray, y_true: np.ndarray) -&gt; Dict[str, float]:
        &quot;&quot;&quot;çµ±ä¸€è©•ä¼°æŒ‡æ¨™&quot;&quot;&quot;
        y_pred = self.predict(X)
        return {
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred),
            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        }
</code></pre>
<hr />
<h3 id="phase-1-day-2-3">Phase 1: ä¸‰æ¨¡å‹å…·é«”å¯¦ä½œ (Day 2-3)</h3>
<h4 id="step-11-xgboost">Step 1.1: XGBoost è¨“ç·´å™¨å¯¦ä½œ</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/trainers/xgboost_trainer.py</code></p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">import xgboost as xgb
import numpy as np
from typing import Dict, Any, Optional, List
from src.modeling.trainers.base_trainer import BaseModelTrainer

class XGBoostTrainer(BaseModelTrainer):
    &quot;&quot;&quot;
    XGBoost è¨“ç·´å™¨å¯¦ä½œ

    ç‰¹æ€§:
    - Level-wise æ¨¹ç”Ÿé•·ï¼ˆå¹³è¡¡æ¨¹æ·±åº¦ï¼‰
    - å…§å»ºæ—©åœæ©Ÿåˆ¶ (Early Stopping)
    - æ”¯æ´æ¨£æœ¬æ¬Šé‡ (Sample Weight)
    - ç‰¹å¾µé‡è¦æ€§åŸºæ–¼ Gainï¼ˆåˆ†è£‚æå¤±æ”¹å–„ï¼‰
    &quot;&quot;&quot;

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;åŸ·è¡Œ XGBoost è¨“ç·´&quot;&quot;&quot;

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = xgb.XGBRegressor(
            n_estimators=self.config.n_estimators,
            learning_rate=self.config.learning_rate,
            max_depth=self.config.max_depth,
            min_child_weight=self.config.min_child_weight,
            subsample=self.config.subsample,
            colsample_bytree=self.config.colsample_bytree,
            reg_alpha=self.config.reg_alpha,
            reg_lambda=self.config.reg_lambda,
            gamma=self.config.gamma,
            eval_metric=self.config.eval_metric,
            tree_method=self.config.tree_method,
            random_state=self.random_state,
            n_jobs=-1,
            verbosity=0
        )

        # è‹¥æœ‰å–®èª¿æ€§ç´„æŸï¼ˆç‰©ç†é—œä¿‚ï¼‰
        if self.config.enable_monotonic_constraints and self.config.monotone_constraints:
            # è½‰æ›ç‚º XGBoost æ ¼å¼: (0: ç„¡, 1: æ­£ç›¸é—œ, -1: è² ç›¸é—œ)
            mono_constraints = [self.config.monotone_constraints.get(f, 0) for f in feature_names]
            self.model.set_params(monotone_constraints=mono_constraints)

        # è¨“ç·´ï¼ˆå«æ—©åœï¼‰
        eval_set = [(X_train, y_train), (X_val, y_val)]
        eval_metric = self.config.eval_metric

        self.model.fit(
            X_train, y_train,
            sample_weight=sample_weights,
            eval_set=eval_set,
            eval_metric=eval_metric,
            early_stopping_rounds=self.config.early_stopping_rounds,
            verbose=False
        )

        # æå–è¨“ç·´æ­·å²
        results = self.model.evals_result()
        self.training_history = {
            'train_rmse': results['validation_0'][eval_metric],
            'val_rmse': results['validation_1'][eval_metric],
            'best_iteration': self.model.best_iteration,
            'best_score': self.model.best_score
        }

        # æå–ç‰¹å¾µé‡è¦æ€§ (Gain-based)
        importance = self.model.feature_importances_
        if feature_names:
            self.feature_importance = dict(zip(feature_names, importance))
        else:
            self.feature_importance = {f&quot;feat_{i}&quot;: imp for i, imp in enumerate(importance)}

        return {
            'model': self.model,
            'best_iteration': self.model.best_iteration,
            'training_history': self.training_history,
            'feature_importance': self.feature_importance,
            'oob_score': None  # XGBoost ä¸æ”¯æ´ OOB
        }

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        if self.model is None:
            raise RuntimeError(&quot;æ¨¡å‹å°šæœªè¨“ç·´&quot;)
        return self.model.predict(X, iteration_range=(0, self.model.best_iteration + 1))

    def get_feature_importance(self) -&gt; Dict[str, float]:
        # æ¨™æº–åŒ–è‡³ç¸½å’Œç‚º1
        total = sum(self.feature_importance.values())
        return {k: v/total for k, v in self.feature_importance.items()}
</code></pre>
<h4 id="step-12-lightgbm">Step 1.2: LightGBM è¨“ç·´å™¨å¯¦ä½œ</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/trainers/lightgbm_trainer.py</code></p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">import lightgbm as lgb
import numpy as np
from typing import Dict, Any, Optional, List
from src.modeling.trainers.base_trainer import BaseModelTrainer

class LightGBMTrainer(BaseModelTrainer):
    &quot;&quot;&quot;
    LightGBM è¨“ç·´å™¨å¯¦ä½œ

    ç‰¹æ€§:
    - Leaf-wise æ¨¹ç”Ÿé•·ï¼ˆæ›´é«˜æ•ˆï¼Œä½†éœ€æ§åˆ¶ max_depth é¿å…éæ“¬åˆï¼‰
    - åŸç”Ÿæ”¯æ´é¡åˆ¥ç‰¹å¾µï¼ˆä½† HVAC å¤šç‚ºæ•¸å€¼ï¼‰
    - è¨“ç·´é€Ÿåº¦æ¥µå¿«ï¼Œè¨˜æ†¶é«”æ•ˆç‡é«˜
    - ç‰¹å¾µé‡è¦æ€§åŸºæ–¼ Split æ¬¡æ•¸
    &quot;&quot;&quot;

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;åŸ·è¡Œ LightGBM è¨“ç·´&quot;&quot;&quot;

        # å»ºç«‹ Datasetï¼ˆLightGBM å°ˆç”¨è³‡æ–™çµæ§‹ï¼Œè¨˜æ†¶é«”æ•ˆç‡é«˜ï¼‰
        train_data = lgb.Dataset(
            X_train, 
            label=y_train, 
            weight=sample_weights,
            feature_name=feature_names,
            free_raw_data=False
        )
        val_data = lgb.Dataset(
            X_val, 
            label=y_val,
            reference=train_data,
            feature_name=feature_names
        )

        # è¶…åƒæ•¸
        params = {
            'objective': 'regression',
            'metric': self.config.eval_metric,
            'boosting_type': self.config.boosting_type,
            'num_leaves': self.config.num_leaves,
            'max_depth': self.config.max_depth,
            'learning_rate': self.config.learning_rate,
            'feature_fraction': self.config.colsample_bytree,
            'bagging_fraction': self.config.subsample,
            'bagging_freq': 5,
            'lambda_l1': self.config.reg_alpha,
            'lambda_l2': self.config.reg_lambda,
            'min_child_samples': self.config.min_child_samples,
            'verbose': -1,
            'random_state': self.random_state,
            'feature_pre_filter': self.config.feature_pre_filter
        }

        if self.config.histogram_pool_size:
            params['histogram_pool_size'] = self.config.histogram_pool_size

        # è¨“ç·´ï¼ˆå«æ—©åœï¼‰
        self.model = lgb.train(
            params,
            train_data,
            num_boost_round=self.config.n_estimators,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'val'],
            callbacks=[lgb.early_stopping(stopping_rounds=self.config.early_stopping_rounds, verbose=False)]
        )

        # æå–è¨“ç·´æ­·å²
        self.training_history = {
            'train_rmse': self.model.params.get('train', {}).get('rmse', []),
            'val_rmse': self.model.params.get('val', {}).get('rmse', []),
            'best_iteration': self.model.best_iteration,
            'best_score': self.model.best_score
        }

        # ç‰¹å¾µé‡è¦æ€§ (Split-based)
        importance_split = self.model.feature_importance(importance_type='split')
        importance_gain = self.model.feature_importance(importance_type='gain')

        if feature_names:
            self.feature_importance = dict(zip(feature_names, importance_gain))  # ä½¿ç”¨ Gain è¼ƒç©©å®š
        else:
            self.feature_importance = {f&quot;feat_{i}&quot;: imp for i, imp in enumerate(importance_gain)}

        return {
            'model': self.model,
            'best_iteration': self.model.best_iteration,
            'training_history': self.training_history,
            'feature_importance': self.feature_importance,
            'oob_score': None
        }

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        if self.model is None:
            raise RuntimeError(&quot;æ¨¡å‹å°šæœªè¨“ç·´&quot;)
        return self.model.predict(X, num_iteration=self.model.best_iteration)

    def get_feature_importance(self) -&gt; Dict[str, float]:
        total = sum(self.feature_importance.values())
        return {k: v/total for k, v in self.feature_importance.items()}
</code></pre>
<h4 id="step-13-random-forest">Step 1.3: Random Forest è¨“ç·´å™¨å¯¦ä½œ</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/trainers/random_forest_trainer.py</code></p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
import numpy as np
from typing import Dict, Any, Optional, List
from src.modeling.trainers.base_trainer import BaseModelTrainer

class RandomForestTrainer(BaseModelTrainer):
    &quot;&quot;&quot;
    Random Forest è¨“ç·´å™¨å¯¦ä½œ

    ç‰¹æ€§:
    - Bagging ç­–ç•¥ï¼ˆå¹³è¡Œæ¨¹ï¼Œé™ä½æ–¹å·®ï¼‰
    - å¤©ç„¶æ”¯æ´ OOB (Out-of-Bag) é©—è­‰ï¼ˆç„¡éœ€ç¨ç«‹é©—è­‰é›†ï¼‰
    - å°ç•°å¸¸å€¼é²æ£’
    - å¯è¼¸å‡ºé æ¸¬å€é–“ï¼ˆä½¿ç”¨æ‰€æœ‰æ¨¹çš„é æ¸¬åˆ†ä½ˆï¼‰
    - ç‰¹å¾µé‡è¦æ€§åŸºæ–¼ Mean Decrease Impurity (MDI)
    &quot;&quot;&quot;

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray = None,  # RF å¯ä¸ä½¿ç”¨ç¨ç«‹é©—è­‰é›†ï¼ˆä½¿ç”¨ OOBï¼‰
        y_val: np.ndarray = None,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;åŸ·è¡Œ Random Forest è¨“ç·´&quot;&quot;&quot;

        self.model = RandomForestRegressor(
            n_estimators=self.config.n_estimators,
            max_depth=self.config.max_depth,
            min_samples_split=self.config.min_samples_split,
            min_samples_leaf=self.config.min_samples_leaf,
            max_features=self.config.max_features,
            bootstrap=self.config.bootstrap,
            oob_score=self.config.oob_score,
            n_jobs=self.config.n_jobs,
            random_state=self.random_state,
            warm_start=self.config.warm_start,
            verbose=0
        )

        # è¨“ç·´ï¼ˆRF ä¸æ”¯æ´æ—©åœï¼Œä½†æ”¯æ´ warm_start å¢é‡è¨“ç·´ï¼‰
        self.model.fit(X_train, y_train, sample_weight=sample_weights)

        # OOB åˆ†æ•¸ï¼ˆè‹¥å•Ÿç”¨ bootstrapï¼‰
        oob_score = self.model.oob_score_ if self.config.oob_score and self.config.bootstrap else None

        # è¨“ç·´æ­·å²ï¼ˆRF ç„¡è¿­ä»£æ­·å²ï¼Œè¨˜éŒ„æœ€çµ‚æ€§èƒ½ï¼‰
        train_metrics = self.evaluate(X_train, y_train)
        val_metrics = self.evaluate(X_val, y_val) if X_val is not None else {}

        self.training_history = {
            'train_rmse': train_metrics['rmse'],
            'val_rmse': val_metrics.get('rmse'),
            'oob_r2': oob_score,
            'n_estimators': self.config.n_estimators
        }

        # ç‰¹å¾µé‡è¦æ€§ (MDI - Mean Decrease Impurity)
        importance = self.model.feature_importances_
        if feature_names:
            self.feature_importance = dict(zip(feature_names, importance))
        else:
            self.feature_importance = {f&quot;feat_{i}&quot;: imp for i, imp in enumerate(importance)}

        return {
            'model': self.model,
            'best_iteration': None,  # RF ç„¡è¿­ä»£æ¦‚å¿µ
            'training_history': self.training_history,
            'feature_importance': self.feature_importance,
            'oob_score': oob_score
        }

    def predict(self, X: np.ndarray) -&gt; np.ndarray:
        if self.model is None:
            raise RuntimeError(&quot;æ¨¡å‹å°šæœªè¨“ç·´&quot;)
        return self.model.predict(X)

    def predict_with_interval(self, X: np.ndarray, confidence: float = 0.9) -&gt; Dict[str, np.ndarray]:
        &quot;&quot;&quot;
        è¼¸å‡ºé æ¸¬å€é–“ï¼ˆä½¿ç”¨æ‰€æœ‰æ¨¹çš„é æ¸¬åˆ†ä½ˆï¼‰

        Returns:
            {
                'mean': å¹³å‡é æ¸¬å€¼,
                'lower': ä¸‹ç•Œ (Q5),
                'upper': ä¸Šç•Œ (Q95),
                'std': æ¨™æº–å·®
            }
        &quot;&quot;&quot;
        if self.model is None:
            raise RuntimeError(&quot;æ¨¡å‹å°šæœªè¨“ç·´&quot;)

        # å–å¾—æ‰€æœ‰æ¨¹çš„é æ¸¬ (n_samples, n_trees)
        all_predictions = np.array([tree.predict(X) for tree in self.model.estimators_])

        mean_pred = np.mean(all_predictions, axis=0)
        std_pred = np.std(all_predictions, axis=0)

        # è¨ˆç®—åˆ†ä½æ•¸
        lower = np.percentile(all_predictions, (1 - confidence) * 100 / 2, axis=0)
        upper = np.percentile(all_predictions, 100 - (1 - confidence) * 100 / 2, axis=0)

        return {
            'mean': mean_pred,
            'lower': lower,
            'upper': upper,
            'std': std_pred
        }

    def get_feature_importance(self) -&gt; Dict[str, float]:
        total = sum(self.feature_importance.values())
        return {k: v/total for k, v in self.feature_importance.items()}
</code></pre>
<hr />
<h3 id="phase-2-day-4">Phase 2: å¤šæ¨¡å‹è¨“ç·´ç®¡ç·šæ•´åˆ (Day 4)</h3>
<h4 id="step-21">Step 2.1: å¹³è¡Œè¨“ç·´èˆ‡æ¨¡å‹é¸æ“‡é‚è¼¯</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/training_pipeline.py</code>ï¼ˆæ ¸å¿ƒæ›´æ–°ï¼‰</p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Tuple, Any
import numpy as np

from src.modeling.trainers.xgboost_trainer import XGBoostTrainer
from src.modeling.trainers.lightgbm_trainer import LightGBMTrainer
from src.modeling.trainers.random_forest_trainer import RandomForestTrainer

class TrainingPipeline:
    &quot;&quot;&quot;
    å¤šæ¨¡å‹è¨“ç·´ç®¡ç·š v1.0

    åŒæ™‚è¨“ç·´ XGBoostã€LightGBMã€Random Forestï¼Œ
    ä¸¦ä¾é©—è­‰æŒ‡æ¨™è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹æˆ–ä¿ç•™ Ensembleã€‚
    &quot;&quot;&quot;

    def __init__(self, config: ModelTrainingConfig, site_id: str, yaml_base_dir: str = &quot;config/features/sites&quot;):
        self.config = config
        self.site_id = site_id
        self.annotation_manager = FeatureAnnotationManager(site_id=site_id, yaml_base_dir=yaml_base_dir)
        self._validate_annotation_compatibility()

        self.trainers = {}
        self.results = {}
        self.best_model_name = None

    def train_all_models(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;
        è¨“ç·´æ‰€æœ‰ä¸‰å€‹æ¨¡å‹

        è‹¥ parallel_training=Trueï¼Œä½¿ç”¨å¤šé€²ç¨‹å¹³è¡Œè¨“ç·´ï¼›
        å¦å‰‡ä¾åºè¨“ç·´ï¼ˆé©åˆè¨˜æ†¶é«”å—é™ç’°å¢ƒï¼‰ã€‚
        &quot;&quot;&quot;
        trainers_config = {
            'xgboost': (XGBoostTrainer, self.config.xgboost),
            'lightgbm': (LightGBMTrainer, self.config.lightgbm),
            'random_forest': (RandomForestTrainer, self.config.random_forest)
        }

        if self.config.parallel_training:
            # å¹³è¡Œè¨“ç·´ï¼ˆæ³¨æ„ï¼šXGBoost èˆ‡ LightGBM å„è‡ªæœƒä½¿ç”¨å¤šåŸ·è¡Œç·’ï¼Œéœ€æ§åˆ¶ç¸½è³‡æºï¼‰
            with ProcessPoolExecutor(max_workers=3) as executor:
                futures = {}
                for name, (TrainerClass, model_config) in trainers_config.items():
                    future = executor.submit(
                        self._train_single_model,
                        name, TrainerClass, model_config,
                        X_train, y_train, X_val, y_val,
                        sample_weights, feature_names
                    )
                    futures[future] = name

                for future in as_completed(futures):
                    name = futures[future]
                    try:
                        self.results[name] = future.result()
                        self.logger.info(f&quot;âœ… {name} è¨“ç·´å®Œæˆ&quot;)
                    except Exception as e:
                        self.logger.error(f&quot;âŒ {name} è¨“ç·´å¤±æ•—: {e}&quot;)
                        self.results[name] = {'error': str(e)}
        else:
            # ä¾åºè¨“ç·´
            for name, (TrainerClass, model_config) in trainers_config.items():
                try:
                    self.results[name] = self._train_single_model(
                        name, TrainerClass, model_config,
                        X_train, y_train, X_val, y_val,
                        sample_weights, feature_names
                    )
                    self.logger.info(f&quot;âœ… {name} è¨“ç·´å®Œæˆ&quot;)
                except Exception as e:
                    self.logger.error(f&quot;âŒ {name} è¨“ç·´å¤±æ•—: {e}&quot;)
                    self.results[name] = {'error': str(e)}

        # é¸æ“‡æœ€ä½³æ¨¡å‹
        if self.config.auto_select_best:
            self.best_model_name = self._select_best_model()

        return self.results

    def _train_single_model(
        self,
        name: str,
        TrainerClass,
        model_config,
        X_train, y_train, X_val, y_val,
        sample_weights, feature_names
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;è¨“ç·´å–®ä¸€æ¨¡å‹&quot;&quot;&quot;
        trainer = TrainerClass(config=model_config, random_state=self.config.random_state)
        result = trainer.train(
            X_train=X_train, y_train=y_train,
            X_val=X_val, y_val=y_val,
            sample_weights=sample_weights,
            feature_names=feature_names
        )
        result['metrics'] = {
            'train': trainer.evaluate(X_train, y_train),
            'val': trainer.evaluate(X_val, y_val)
        }
        self.trainers[name] = trainer
        return result

    def _select_best_model(self) -&gt; str:
        &quot;&quot;&quot;
        é¸æ“‡æœ€ä½³æ¨¡å‹

        ç­–ç•¥:
        1. å„ªå…ˆæ¯”è¼ƒé©—è­‰é›† RÂ² åˆ†æ•¸
        2. è‹¥ RÂ² å·®è· &lt; 0.01ï¼Œé¸æ“‡è¨“ç·´æ™‚é–“è¼ƒçŸ­çš„ï¼ˆLightGBM &gt; XGBoost &gt; RFï¼‰
        3. è‹¥ RF çš„ OOB åˆ†æ•¸èˆ‡é©—è­‰é›†å·®è·éå¤§ï¼ˆ&gt;0.1ï¼‰ï¼Œå¯èƒ½è¡¨ç¤ºè³‡æ–™æ´©æ¼ï¼Œé™ä½æ’å
        &quot;&quot;&quot;
        valid_results = {
            name: res for name, res in self.results.items() 
            if 'error' not in res and 'metrics' in res
        }

        if not valid_results:
            raise ModelTrainingError(&quot;æ‰€æœ‰æ¨¡å‹è¨“ç·´å¤±æ•—&quot;)

        # æ’åºï¼šVal R2 é«˜åˆ°ä½
        ranked = sorted(
            valid_results.items(),
            key=lambda x: x[1]['metrics']['val']['r2'],
            reverse=True
        )

        best_name, best_result = ranked[0]
        best_r2 = best_result['metrics']['val']['r2']

        self.logger.info(f&quot;ğŸ† æœ€ä½³æ¨¡å‹: {best_name} (Val RÂ²={best_r2:.4f})&quot;)

        # è¨˜éŒ„æ‰€æœ‰æ¨¡å‹æ¯”è¼ƒ
        for name, result in ranked:
            r2 = result['metrics']['val']['r2']
            rmse = result['metrics']['val']['rmse']
            self.logger.info(f&quot;   {name}: RÂ²={r2:.4f}, RMSE={rmse:.4f}&quot;)

        return best_name

    def get_best_model(self) -&gt; Tuple[str, BaseModelTrainer, Dict]:
        &quot;&quot;&quot;å–å¾—æœ€ä½³æ¨¡å‹åŠå…¶çµæœ&quot;&quot;&quot;
        if self.best_model_name is None:
            raise RuntimeError(&quot;å°šæœªåŸ·è¡Œæ¨¡å‹é¸æ“‡&quot;)
        return (
            self.best_model_name,
            self.trainers[self.best_model_name],
            self.results[self.best_model_name]
        )

    def predict_ensemble(self, X: np.ndarray, weights: Optional[Dict[str, float]] = None) -&gt; np.ndarray:
        &quot;&quot;&quot;
        Ensemble é æ¸¬ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰

        è‹¥ weights ç‚º Noneï¼Œä½¿ç”¨é©—è­‰é›† RÂ² ä½œç‚ºæ¬Šé‡åŸºç¤ã€‚
        &quot;&quot;&quot;
        if not self.trainers:
            raise RuntimeError(&quot;å°šæœªè¨“ç·´æ¨¡å‹&quot;)

        predictions = []
        model_weights = []

        for name, trainer in self.trainers.items():
            if 'error' in self.results[name]:
                continue
            pred = trainer.predict(X)
            predictions.append(pred)

            if weights and name in weights:
                model_weights.append(weights[name])
            else:
                # ä½¿ç”¨ Val RÂ² ä½œç‚ºæ¬Šé‡ï¼ˆéœ€æ­£è¦åŒ–ï¼‰
                r2 = max(0, self.results[name]['metrics']['val']['r2'])  # é¿å…è² å€¼
                model_weights.append(r2)

        # åŠ æ¬Šå¹³å‡
        weights_arr = np.array(model_weights) / sum(model_weights)
        ensemble_pred = np.average(predictions, axis=0, weights=weights_arr)

        return ensemble_pred
</code></pre>
<hr />
<h3 id="phase-3-day-5">Phase 3: å®Œæ•´è¨“ç·´æµç¨‹èˆ‡ç”¢å‡º (Day 5)</h3>
<h4 id="step-31">Step 3.1: å®Œæ•´è¨“ç·´æµç¨‹ï¼ˆæ•´åˆä¸‰æ¨¡å‹ï¼‰</h4>
<p><strong>æ–¹æ³•</strong>: <code>train(data: TrainingInputContract) -&gt; MultiModelArtifact</code></p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">def train(self, data: Dict) -&gt; 'MultiModelArtifact':
    &quot;&quot;&quot;
    åŸ·è¡Œå®Œæ•´å¤šæ¨¡å‹è¨“ç·´æµç¨‹

    Returns:
        MultiModelArtifact: åŒ…å«ä¸‰æ¨¡å‹çµæœèˆ‡æœ€ä½³æ¨¡å‹é¸æ“‡
    &quot;&quot;&quot;
    # Step 1-4: è³‡æ–™æº–å‚™ï¼ˆèˆ‡å…ˆå‰ç›¸åŒï¼Œç•¥ï¼‰
    self._validate_input_contract(data)
    df = data['feature_matrix']
    target_col = data['target_variable']

    train_df, val_df, test_df, y_train, y_val, y_test = self._temporal_split(df, target_col)
    sample_weights, seasonal_mask = self._compute_sample_weights_and_masks(train_df)

    # æ‡‰ç”¨é®ç½©
    if np.any(seasonal_mask == False):
        train_df = train_df.filter(pl.Series(seasonal_mask))
        y_train = y_train.filter(pl.Series(seasonal_mask))
        sample_weights = sample_weights[seasonal_mask]

    X_train, X_val, X_test, feature_cols = self._preprocess_features(train_df, val_df, test_df)

    # Step 5: å¤šæ¨¡å‹è¨“ç·´
    self.train_all_models(
        X_train=X_train, y_train=y_train.to_numpy(),
        X_val=X_val, y_val=y_val.to_numpy(),
        sample_weights=sample_weights,
        feature_names=feature_cols
    )

    # Step 6: æ¸¬è©¦é›†æœ€çµ‚è©•ä¼°ï¼ˆåƒ…æœ€ä½³æ¨¡å‹ï¼‰
    best_name, best_trainer, best_result = self.get_best_model()
    test_metrics = best_trainer.evaluate(X_test, y_test.to_numpy())

    self.logger.info(f&quot;ğŸ§ª æœ€ä½³æ¨¡å‹æ¸¬è©¦é›†è¡¨ç¾: RÂ²={test_metrics['r2']:.4f}, RMSE={test_metrics['rmse']:.4f}&quot;)

    # Step 7: å»ºç«‹å¤šæ¨¡å‹ç”¢å‡ºç‰©
    artifact = MultiModelArtifact(
        trainers=self.trainers,
        results=self.results,
        best_model_name=best_name,
        test_metrics=test_metrics,
        training_metadata=self._build_training_metadata(data, test_metrics),
        annotation_context=data['annotation_context'],
        feature_names=feature_cols,
        config=self.config
    )

    return artifact
</code></pre>
<h4 id="step-32">Step 3.2: å¤šæ¨¡å‹ç”¢å‡ºç‰©å®šç¾©</h4>
<p><strong>æª”æ¡ˆ</strong>: <code>src/modeling/artifacts.py</code>ï¼ˆæ›´æ–°ï¼‰</p>
<p><strong>å¯¦ä½œå…§å®¹</strong>:</p>
<pre><code class="language-python">@dataclass
class MultiModelArtifact:
    &quot;&quot;&quot;
    å¤šæ¨¡å‹è¨“ç·´ç”¢å‡ºç‰©

    å„²å­˜çµæ§‹:
    models/
    â””â”€â”€ {site_id}/
        â”œâ”€â”€ ensemble_manifest.json           # çµ±ä¸€å…¥å£
        â”œâ”€â”€ xgboost_model.joblib
        â”œâ”€â”€ xgboost_metadata.json
        â”œâ”€â”€ lightgbm_model.joblib
        â”œâ”€â”€ lightgbm_metadata.json
        â”œâ”€â”€ random_forest_model.joblib
        â””â”€â”€ random_forest_metadata.json
    &quot;&quot;&quot;

    trainers: Dict[str, BaseModelTrainer]
    results: Dict[str, Dict[str, Any]]
    best_model_name: str
    test_metrics: Dict[str, float]
    training_metadata: Dict[str, Any]
    annotation_context: Dict[str, Any]
    feature_names: List[str]
    config: ModelTrainingConfig

    def save(self, output_dir: Path) -&gt; Dict[str, Path]:
        &quot;&quot;&quot;å„²å­˜æ‰€æœ‰æ¨¡å‹èˆ‡å…ƒè³‡æ–™&quot;&quot;&quot;
        output_dir = Path(output_dir) / self.training_metadata['site_id']
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
        saved_files = {'ensemble_manifest': output_dir / f&quot;{timestamp}_ensemble_manifest.json&quot;}

        ensemble_data = {
            'timestamp': timestamp,
            'best_model': self.best_model_name,
            'test_metrics': self.test_metrics,
            'models': {}
        }

        # å„²å­˜æ¯å€‹æ¨¡å‹
        for name, trainer in self.trainers.items():
            if 'error' in self.results[name]:
                continue

            model_path = output_dir / f&quot;{timestamp}_{name}_model.joblib&quot;
            metadata_path = output_dir / f&quot;{timestamp}_{name}_metadata.json&quot;

            # å„²å­˜æ¨¡å‹
            joblib.dump({
                'model': trainer.model,
                'scaler': getattr(trainer, 'scaler', None),
                'feature_names': self.feature_names
            }, model_path)

            # å„²å­˜è©²æ¨¡å‹å…ƒè³‡æ–™
            model_meta = {
                'name': name,
                'metrics': self.results[name]['metrics'],
                'feature_importance': trainer.get_feature_importance(),
                'training_history': self.results[name].get('training_history', {}),
                'best_iteration': self.results[name].get('best_iteration'),
                'oob_score': self.results[name].get('oob_score')
            }

            with open(metadata_path, 'w') as f:
                json.dump(model_meta, f, indent=2, default=str)

            ensemble_data['models'][name] = {
                'model_file': str(model_path.name),
                'metadata_file': str(metadata_path.name),
                'val_r2': self.results[name]['metrics']['val']['r2'],
                'test_r2': self.test_metrics['r2'] if name == self.best_model_name else None
            }

        # å„²å­˜ Ensemble Manifest
        ensemble_data['training_metadata'] = self.training_metadata
        ensemble_data['annotation_context'] = self.annotation_context

        with open(saved_files['ensemble_manifest'], 'w') as f:
            json.dump(ensemble_data, f, indent=2, default=str)

        return saved_files

    @classmethod
    def load(cls, ensemble_manifest_path: Path, model_name: Optional[str] = None):
        &quot;&quot;&quot;è¼‰å…¥æŒ‡å®šæ¨¡å‹æˆ–æœ€ä½³æ¨¡å‹&quot;&quot;&quot;
        with open(ensemble_manifest_path, 'r') as f:
            manifest = json.load(f)

        model_to_load = model_name or manifest['best_model']
        model_info = manifest['models'][model_to_load]

        # è¼‰å…¥å…·é«”æ¨¡å‹
        model_dir = ensemble_manifest_path.parent
        model_data = joblib.load(model_dir / model_info['model_file'])

        return model_data, manifest
</code></pre>
<hr />
<h2 id="4-error-codes">4. éŒ¯èª¤ä»£ç¢¼å°ç…§è¡¨ (Error Codes)</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">éŒ¯èª¤ä»£ç¢¼</th>
<th style="text-align: left;">åç¨±</th>
<th style="text-align: center;">ç™¼ç”Ÿéšæ®µ</th>
<th style="text-align: left;">èªªæ˜</th>
<th style="text-align: left;">è™•ç†å»ºè­°</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>E601</strong></td>
<td style="text-align: left;"><code>ANNOTATION_CONTEXT_MISSING</code></td>
<td style="text-align: center;">Step 1.1</td>
<td style="text-align: left;">ç¼ºå°‘ annotation_context</td>
<td style="text-align: left;">ç¢ºèª Feature Engineer v1.3+</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E602</strong></td>
<td style="text-align: left;"><code>SCHEMA_VERSION_MISMATCH</code></td>
<td style="text-align: center;">Step 1.1</td>
<td style="text-align: left;">Annotation ç‰ˆæœ¬ä¸ç¬¦</td>
<td style="text-align: left;">é‡æ–°è¨“ç·´æˆ–é™ç´š Annotation</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E603</strong></td>
<td style="text-align: left;"><code>TARGET_VARIABLE_MISSING</code></td>
<td style="text-align: center;">Step 1.1</td>
<td style="text-align: left;">ç›®æ¨™è®Šæ•¸ä¸å­˜åœ¨</td>
<td style="text-align: left;">æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹è¼¸å‡º</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E604</strong></td>
<td style="text-align: left;"><code>TIMESTAMP_INVALID</code></td>
<td style="text-align: center;">Step 1.1</td>
<td style="text-align: left;">æ™‚é–“æˆ³æ ¼å¼éŒ¯èª¤</td>
<td style="text-align: left;">æª¢æŸ¥ Feature Engineer</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E701</strong></td>
<td style="text-align: left;"><code>DEVICE_ROLE_AS_FEATURE</code></td>
<td style="text-align: center;">Step 0.1</td>
<td style="text-align: left;">è¨­å®šéŒ¯èª¤å˜—è©¦å°‡ device_role ä½œç‚ºç‰¹å¾µ</td>
<td style="text-align: left;">ä¿®æ”¹è¨­å®š</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E702</strong></td>
<td style="text-align: left;"><code>INSUFFICIENT_SAMPLES</code></td>
<td style="text-align: center;">Step 3</td>
<td style="text-align: left;">æ¨£æœ¬ä¸è¶³ï¼ˆ&lt;100ï¼‰</td>
<td style="text-align: left;">æª¢æŸ¥è³‡æ–™é®ç½©é‚è¼¯</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E703</strong></td>
<td style="text-align: left;"><code>ALL_MODELS_FAILED</code></td>
<td style="text-align: center;">Step 5</td>
<td style="text-align: left;">ä¸‰æ¨¡å‹å…¨éƒ¨è¨“ç·´å¤±æ•—</td>
<td style="text-align: left;">æª¢æŸ¥è³‡æ–™å“è³ªæˆ–ç‰¹å¾µå·¥ç¨‹</td>
</tr>
<tr>
<td style="text-align: left;"><strong>E704</strong></td>
<td style="text-align: left;"><code>XGBOOST_IMPORT_ERROR</code></td>
<td style="text-align: center;">Step 1.1</td>
<td style="text-align: left;">XGBoost æœªå®‰è£</td>
<td style="text-align: left;"><code>pip install xgboost</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>E705</strong></td>
<td style="text-align: left;"><code>LIGHTGBM_IMPORT_ERROR</code></td>
<td style="text-align: center;">Step 1.1</td>
<td style="text-align: left;">LightGBM æœªå®‰è£</td>
<td style="text-align: left;"><code>pip install lightgbm</code></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="5-test-plan">5. æ¸¬è©¦èˆ‡é©—è­‰è¨ˆç•« (Test Plan)</h2>
<h3 id="51">5.1 å–®å…ƒæ¸¬è©¦ï¼ˆæ¯å€‹æ¨¡å‹ç¨ç«‹æ¸¬è©¦ï¼‰</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">æ¸¬è©¦æ¡ˆä¾‹ ID</th>
<th style="text-align: left;">æè¿°</th>
<th style="text-align: center;">é©—è­‰ç›®æ¨™</th>
<th style="text-align: center;">æ¨¡å‹</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MT-XGB-001</td>
<td style="text-align: left;">XGBoost åŸºæœ¬è¨“ç·´</td>
<td style="text-align: center;">æ”¶æ•›ã€æ—©åœç”Ÿæ•ˆã€ç‰¹å¾µé‡è¦æ€§åˆç†</td>
<td style="text-align: center;">XGBoost</td>
</tr>
<tr>
<td style="text-align: left;">MT-XGB-002</td>
<td style="text-align: left;">XGBoost æ¨£æœ¬æ¬Šé‡</td>
<td style="text-align: center;">é«˜æ¬Šé‡æ¨£æœ¬å½±éŸ¿æ›´å¤§</td>
<td style="text-align: center;">XGBoost</td>
</tr>
<tr>
<td style="text-align: left;">MT-LGB-001</td>
<td style="text-align: left;">LightGBM é€Ÿåº¦æ¸¬è©¦</td>
<td style="text-align: center;">ç›¸åŒè³‡æ–™è¨“ç·´æ™‚é–“ &lt; XGBoost 50%</td>
<td style="text-align: center;">LightGBM</td>
</tr>
<tr>
<td style="text-align: left;">MT-LGB-002</td>
<td style="text-align: left;">Leaf-wise éæ“¬åˆé˜²è­·</td>
<td style="text-align: center;">num_leaves æ§åˆ¶æœ‰æ•ˆ</td>
<td style="text-align: center;">LightGBM</td>
</tr>
<tr>
<td style="text-align: left;">MT-RF-001</td>
<td style="text-align: left;">OOB åˆ†æ•¸é©—è­‰</td>
<td style="text-align: center;">OOB â‰ˆ Val Scoreï¼ˆå·®è· &lt; 5%ï¼‰</td>
<td style="text-align: center;">Random Forest</td>
</tr>
<tr>
<td style="text-align: left;">MT-RF-002</td>
<td style="text-align: left;">é æ¸¬å€é–“è¼¸å‡º</td>
<td style="text-align: center;">lower &lt; mean &lt; upperï¼Œstd &gt; 0</td>
<td style="text-align: center;">Random Forest</td>
</tr>
<tr>
<td style="text-align: left;">MT-ENS-001</td>
<td style="text-align: left;">Ensemble åŠ æ¬Šå¹³å‡</td>
<td style="text-align: center;">åŠ æ¬Šé æ¸¬ä»‹æ–¼å„æ¨¡å‹ä¹‹é–“</td>
<td style="text-align: center;">Ensemble</td>
</tr>
<tr>
<td style="text-align: left;">MT-SEL-001</td>
<td style="text-align: left;">è‡ªå‹•æ¨¡å‹é¸æ“‡</td>
<td style="text-align: center;">æ­£ç¢ºé¸æ“‡ Val RÂ² æœ€é«˜è€…</td>
<td style="text-align: center;">Auto Select</td>
</tr>
</tbody>
</table>
<h3 id="52">5.2 æ•´åˆæ¸¬è©¦ï¼ˆä¸‰æ¨¡å‹æ¯”è¼ƒï¼‰</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">æ¸¬è©¦æ¡ˆä¾‹ ID</th>
<th style="text-align: left;">æè¿°</th>
<th style="text-align: left;">é©—è­‰ç›®æ¨™</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">INT-MT-001</td>
<td style="text-align: left;">ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´</td>
<td style="text-align: left;">åŒæ™‚å®Œæˆï¼Œç„¡è¨˜æ†¶é«”è¡çª</td>
</tr>
<tr>
<td style="text-align: left;">INT-MT-002</td>
<td style="text-align: left;">HVAC çœŸå¯¦è³‡æ–™æ¸¬è©¦</td>
<td style="text-align: left;">è‡³å°‘ä¸€æ¨¡å‹é”åˆ° RÂ² &gt; 0.85</td>
</tr>
<tr>
<td style="text-align: left;">INT-MT-003</td>
<td style="text-align: left;">Device Role æ¬Šé‡å½±éŸ¿</td>
<td style="text-align: left;">Backup æ¨£æœ¬æ¬Šé‡èª¿æ•´å¾Œï¼Œæ¨¡å‹é æ¸¬ç©©å®š</td>
</tr>
<tr>
<td style="text-align: left;">INT-MT-004</td>
<td style="text-align: left;">ç‰ˆæœ¬ç¶å®šé©—è­‰</td>
<td style="text-align: left;">å„²å­˜çš„ Manifest åŒ…å«æ­£ç¢º yaml_checksum</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="6">6. ç‰ˆæœ¬ç›¸å®¹æ€§èˆ‡ä¾è³´</h2>
<h3 id="61-python">6.1 Python å¥—ä»¶ä¾è³´</h3>
<pre><code class="language-toml">[project.optional-dependencies]
modeling = [
    &quot;xgboost&gt;=1.7.0&quot;,      # æ”¯æ´ early stopping callback
    &quot;lightgbm&gt;=4.0.0&quot;,     # æ–°ç‰ˆ API
    &quot;scikit-learn&gt;=1.3.0&quot;, # Random Forest, è©•ä¼°æŒ‡æ¨™
    &quot;optuna&gt;=3.0.0&quot;,       # å¯é¸ï¼Œè¶…åƒæ•¸æœå°‹
    &quot;joblib&gt;=1.3.0&quot;,       # æ¨¡å‹å„²å­˜
]
</code></pre>
<h3 id="62">6.2 ç¡¬é«”å»ºè­°</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">æ¨¡å‹</th>
<th style="text-align: center;">è¨˜æ†¶é«”éœ€æ±‚</th>
<th style="text-align: center;">CPU æ ¸å¿ƒ</th>
<th style="text-align: center;">GPU åŠ é€Ÿ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">XGBoost</td>
<td style="text-align: center;">ä¸­ç­‰</td>
<td style="text-align: center;">4-8</td>
<td style="text-align: center;">å¯é¸ (CUDA)</td>
</tr>
<tr>
<td style="text-align: left;">LightGBM</td>
<td style="text-align: center;">ä½</td>
<td style="text-align: center;">4-8</td>
<td style="text-align: center;">ä¸å»ºè­°ï¼ˆCPU å·²æ¥µå¿«ï¼‰</td>
</tr>
<tr>
<td style="text-align: left;">Random Forest</td>
<td style="text-align: center;">é«˜ï¼ˆå¹³è¡Œæ¨¹ï¼‰</td>
<td style="text-align: center;">8+</td>
<td style="text-align: center;">ä¸æ”¯æ´</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="7-sign-off-checklist">7. é©—æ”¶ç°½æ ¸ (Sign-off Checklist)</h2>
<ul>
<li>[ ] <strong>ä¸‰æ¨¡å‹å¯¦ä½œ</strong>: XGBoostã€LightGBMã€Random Forest çš†å¯ç¨ç«‹è¨“ç·´</li>
<li>[ ] <strong>å¹³è¡Œè¨“ç·´</strong>: <code>parallel_training=True</code> æ™‚ï¼Œä¸‰æ¨¡å‹åŒæ™‚è¨“ç·´å®Œæˆ</li>
<li>[ ] <strong>è‡ªå‹•é¸æ“‡</strong>: ä¾ Val RÂ² è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ï¼Œè¨˜éŒ„é¸æ“‡ç†ç”±</li>
<li>[ ] <strong>æ¨£æœ¬æ¬Šé‡</strong>: ä¸‰æ¨¡å‹çš†æ­£ç¢ºè™•ç† Device Role æ¬Šé‡ï¼ˆBackup=0.3ï¼‰</li>
<li>[ ] <strong>ç‰¹å¾µé‡è¦æ€§</strong>: æ¯å€‹æ¨¡å‹è¼¸å‡ºæ¨™æº–åŒ–é‡è¦æ€§ï¼ˆç¸½å’Œç‚º1ï¼‰</li>
<li>[ ] <strong>RF å€é–“é æ¸¬</strong>: Random Forest æ”¯æ´ <code>predict_with_interval()</code> è¼¸å‡º Q10/Q90</li>
<li>[ ] <strong>Ensemble æ”¯æ´</strong>: å¯è¼¸å‡ºä¸‰æ¨¡å‹åŠ æ¬Šå¹³å‡é æ¸¬</li>
<li>[ ] <strong>ç‰ˆæœ¬ç¶å®š</strong>: å„²å­˜çš„ Manifest åŒ…å« Annotation yaml_checksum</li>
<li>[ ] <strong>éŒ¯èª¤è™•ç†</strong>: å–®ä¸€æ¨¡å‹å¤±æ•—ä¸å½±éŸ¿å…¶ä»–æ¨¡å‹è¨“ç·´</li>
<li>[ ] <strong>æ¸¬è©¦è¦†è“‹</strong>: MT-XGB/LGB/RF ç³»åˆ—æ¸¬è©¦å…¨éƒ¨é€šé</li>
</ul>
<hr />
<p><strong>é—œéµè¨­è¨ˆç¢ºèª</strong>:<br />
1. <strong>ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´</strong>: åŒæ™‚è¨“ç·´ XGBoostï¼ˆç²¾åº¦ï¼‰ã€LightGBMï¼ˆé€Ÿåº¦ï¼‰ã€Random Forestï¼ˆé²æ£’æ€§ï¼‰<br />
2. <strong>è‡ªå‹•é¸æ“‡æ©Ÿåˆ¶</strong>: ä¾ Val RÂ² è‡ªå‹•é¸æ“‡ï¼Œé¿å…äººå·¥é¸æ“‡åèª¤<br />
3. <strong>RF é æ¸¬å€é–“</strong>: åˆ©ç”¨ Bagging ç‰¹æ€§è¼¸å‡ºé æ¸¬ä¸ç¢ºå®šæ€§ï¼Œä¾› Optimization Engine åšé¢¨éšªè©•ä¼°<br />
4. <strong>Device Role çµ±ä¸€è™•ç†</strong>: ä¸‰æ¨¡å‹å…±ç”¨ç›¸åŒæ¨£æœ¬æ¬Šé‡é‚è¼¯ï¼Œç¢ºä¿ä¸€è‡´æ€§</p>
</body>
</html>