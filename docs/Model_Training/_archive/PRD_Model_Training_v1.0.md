# PRD v1.0: æ¨¡å‹è¨“ç·´ç®¡ç·šå¯¦ä½œæŒ‡å— (Model Training Pipeline Implementation Guide)

**æ–‡ä»¶ç‰ˆæœ¬:** v1.0 (Multi-Model Ensemble & Temporal-Aware Training)  
**æ—¥æœŸ:** 2026-02-13  
**è² è²¬äºº:** Oscar Chang  
**ç›®æ¨™æ¨¡çµ„:** `src/modeling/training_pipeline.py`, `src/modeling/trainers/`  
**ä¸Šæ¸¸å¥‘ç´„:** `src/etl/feature_engineer.py` (v1.3-FA+, æª¢æŸ¥é» #4)  
**ä¸‹æ¸¸å¥‘ç´„:** `src/optimization/engine.py` (v1.0+, è¼¸å…¥æª¢æŸ¥é»)  
**æ”¯æ´æ¨¡å‹:** 
- **XGBoost** (Extreme Gradient Boosting) - é«˜ç²¾åº¦ã€å¯è§£é‡‹æ€§å¼·
- **LightGBM** (Light Gradient Boosting Machine) - å¤§è¦æ¨¡è³‡æ–™ã€è¨“ç·´æ¥µé€Ÿ  
- **Random Forest** (Bagging Ensemble) - é«˜é²æ£’æ€§ã€æŠ—éæ“¬åˆã€åŸºå‡†æ¨¡å‹  
**é ä¼°å·¥æ™‚:** 6 ~ 7 å€‹å·¥ç¨‹å¤©ï¼ˆå«ä¸‰æ¨¡å‹æ•´åˆã€è¶…åƒæ•¸æœå°‹ã€æ¨¡å‹é¸æ“‡æ©Ÿåˆ¶ï¼‰

---

## 1. åŸ·è¡Œç¸½ç¶±èˆ‡è¨­è¨ˆå“²å­¸

### 1.1 æ ¸å¿ƒç›®æ¨™

å»ºç«‹**å¤šæ¨¡å‹å¹³è¡Œè¨“ç·´ (Multi-Model Training)**ã€**è‡ªå‹•æ¨¡å‹é¸æ“‡ (AutoML Selection)**ã€**æ™‚åºæ„ŸçŸ¥ (Temporal-Aware)** çš„è¨“ç·´ç®¡ç·šï¼š

1. **ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´**: åŒæ™‚è¨“ç·´ XGBoostã€LightGBMã€Random Forestï¼Œè‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹
2. **é›¶è³‡æ–™æ´©æ¼ (Zero Data Leakage)**: åš´æ ¼éµå®ˆ `temporal_cutoff`ï¼Œè¨“ç·´è³‡æ–™çµ•ä¸åŒ…å«é©—è­‰/æ¸¬è©¦æœŸçš„æœªä¾†è³‡è¨Š
3. **Device Role æ„ŸçŸ¥**: æ­£ç¢ºè™•ç† `device_role`ï¼ˆprimary/backup/seasonalï¼‰ï¼Œä½œç‚º**æ¨£æœ¬æ¬Šé‡ (Sample Weighting)** èˆ‡**åˆ†å±¤ä¾æ“š (Stratification Basis)**
4. **è¶…åƒæ•¸è‡ªå‹•æœå°‹**: æ”¯æ´ Optuna è‡ªå‹•åŒ–è¶…åƒæ•¸èª¿å„ªï¼ˆå¯é¸ï¼‰
5. **ç‰ˆæœ¬å¯è¿½æº¯ (Version Traceability)**: æ¯å€‹è¨“ç·´ç”¢å‡ºçš„æ¨¡å‹å¿…é ˆç¶å®šç•¶æ™‚çš„ `schema_version`ã€`inheritance_chain` èˆ‡ `yaml_checksum`

### 1.2 ä¸‰æ¨¡å‹ç‰¹æ€§æ¯”è¼ƒèˆ‡é©ç”¨å ´æ™¯

| æ¨¡å‹ | æ¼”ç®—æ³•é¡å‹ | å„ªå‹¢ | æœ€ä½³é©ç”¨å ´æ™¯ | ç‰¹å¾µé‡è¦æ€§ |
|:---|:---:|:---|:---|:---:|
| **XGBoost** | Gradient Boosting (Level-wise) | ç²¾åº¦æ¥µé«˜ã€æ­£å‰‡åŒ–å¼·ã€ä¸æ˜“éæ“¬åˆã€ç”Ÿæ…‹ç³»å®Œæ•´ | ä¸­ç­‰è³‡æ–™é‡ (<100è¬ç­†)ã€é«˜ç¶­åº¦ç‰¹å¾µã€éœ€å¯è§£é‡‹æ€§ | Gain-based |
| **LightGBM** | Gradient Boosting (Leaf-wise) | è¨“ç·´é€Ÿåº¦æ¥µå¿«ã€è¨˜æ†¶é«”æ•ˆç‡é«˜ã€æ”¯æ´é¡åˆ¥ç‰¹å¾µè‡ªå‹•ç·¨ç¢¼ | å¤§è¦æ¨¡è³‡æ–™ (>10è¬ç­†)ã€å³æ™‚è¨“ç·´éœ€æ±‚ã€é«˜åŸºæ•¸é¡åˆ¥ç‰¹å¾µ | Split-based |
| **Random Forest** | Bagging (Parallel Trees) | æ¥µé«˜é²æ£’æ€§ã€å¤©ç„¶æ”¯æ´å¹³è¡Œé‹ç®—ã€å°ç•°å¸¸å€¼ä¸æ•æ„Ÿã€ç„¡éœ€å¤§é‡èª¿åƒ | å¿«é€ŸåŸºå‡†æ¸¬è©¦ã€è³‡æ–™å«å™ªéŸ³ã€éœ€ç©©å®šé æ¸¬å€é–“ | Mean Decrease Impurity |

**é è¨­ç­–ç•¥**: ä¸‰æ¨¡å‹åŒæ™‚è¨“ç·´ï¼Œä¾é©—è­‰é›† RÂ² åˆ†æ•¸è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ï¼Œæˆ–ä¿ç•™ä¸‰æ¨¡å‹åš Ensemble æŠ•ç¥¨ã€‚

---

## 2. ä»‹é¢å¥‘ç´„è¦ç¯„ (Interface Contracts)

### 2.1 è¼¸å…¥å¥‘ç´„ (Input Contract from Feature Engineer v1.3)

**æª¢æŸ¥é» #7: Feature Engineer â†’ Model Training**

```python
class TrainingInputContract(BaseModel):
    """æ¨¡å‹è¨“ç·´è¼¸å…¥è³‡æ–™è¦ç¯„"""
    
    # 1. ç‰¹å¾µçŸ©é™£ (ä¾†è‡ª Feature Engineer)
    feature_matrix: pl.DataFrame
    
    # 2. ç›®æ¨™è®Šæ•¸è³‡è¨Š
    target_variable: str
    target_metadata: FeatureMetadata
    
    # 3. æ™‚é–“æˆ³è¨˜
    timestamp_col: str = "timestamp"
    time_range: Dict[str, str]
    
    # 4. Annotation ä¸Šä¸‹æ–‡ï¼ˆç‰ˆæœ¬ç¶å®šï¼‰
    annotation_context: Dict = {
        "schema_version": "1.2",
        "inheritance_chain": "base -> cgmh_ty",
        "yaml_checksum": "sha256:abc123...",
        "group_policies_applied": ["chillers", "towers"],
        "feature_engineer_version": "1.3-FA"
    }
    
    # 5. ç‰¹å¾µå…ƒè³‡æ–™ï¼ˆä¸å« device_roleï¼‰
    feature_metadata: Dict[str, FeatureMetadata]
    
    # 6. Quality Flag ç‰¹å¾µåˆ—è¡¨
    quality_flag_features: List[str]
    
    # 7. é˜² Data Leakage è³‡è¨Š
    train_test_split_info: Dict = {
        "temporal_cutoff": "2025-10-01T00:00:00Z",
        "strict_past_only": True
    }
    
    # 8. æ¨£æœ¬æ¬Šé‡å»ºè­°ï¼ˆå¯é¸ï¼‰
    suggested_sample_weights: Optional[pl.Series] = None
    
    # 9. è³‡æ–™è¦æ¨¡æ¨™è¨˜ï¼ˆç”¨æ–¼æ¨¡å‹é¸æ“‡å»ºè­°ï¼‰
    n_samples: int
    n_features: int
```

| æª¢æŸ¥é … | è¦æ ¼ | éŒ¯èª¤ä»£ç¢¼ | è™•ç† |
|:---|:---|:---:|:---|
| **Annotation Context å­˜åœ¨æ€§** | å¿…é ˆéç©ºä¸”åŒ…å« `schema_version`, `inheritance_chain`, `yaml_checksum` | E601 | æ‹’çµ•è¨“ç·´ |
| **Schema ç‰ˆæœ¬ç›¸å®¹** | `schema_version` å¿…é ˆç­‰æ–¼ç•¶å‰ `FEATURE_ANNOTATION_CONSTANTS['expected_schema_version']` | E602 | æ‹’çµ•è¨“ç·´ |
| **ç›®æ¨™è®Šæ•¸å­˜åœ¨** | `target_variable` å¿…é ˆå­˜åœ¨æ–¼ `feature_matrix` æ¬„ä½ä¸­ | E603 | æ‹’çµ•è¨“ç·´ |
| **æ™‚é–“æˆ³å‹åˆ¥** | `timestamp` å¿…é ˆç‚º `Datetime(ns, UTC)` | E604 | æ‹’çµ•è¨“ç·´ |
| **è³‡æ–™è¦æ¨¡æª¢æŸ¥** | `n_samples` å¿…é ˆ >= 100ï¼ˆæ¯å€‹æ¨¡å‹æœ€ä½éœ€æ±‚ï¼‰ | E607 | æ‹’çµ•è¨“ç·´ |

---

## 3. åˆ†éšæ®µå¯¦ä½œè¨ˆç•« (Phase-Based Implementation)

### Phase 0: åŸºç¤å»ºè¨­èˆ‡å¤šæ¨¡å‹æ¶æ§‹ (Day 1)

#### Step 0.1: çµ±ä¸€è¨“ç·´é…ç½®æ¨¡å‹ï¼ˆä¸‰æ¨¡å‹æ”¯æ´ï¼‰

**æª”æ¡ˆ**: `src/modeling/config_models.py`

**å¯¦ä½œå…§å®¹**:
```python
from typing import Dict, List, Optional, Literal, Final, Union
from pydantic import BaseModel, Field, validator
from datetime import datetime

from src.etl.config_models import (
    VALID_QUALITY_FLAGS,
    TIMESTAMP_CONFIG,
    FEATURE_ANNOTATION_CONSTANTS
)

EXPECTED_SCHEMA_VERSION: Final[str] = FEATURE_ANNOTATION_CONSTANTS['expected_schema_version']

# ==========================================
# æ¨¡å‹ç‰¹å®šè¶…åƒæ•¸é…ç½®
# ==========================================

class XGBoostConfig(BaseModel):
    """XGBoost å°ˆå±¬é…ç½® - Level-wise ç”Ÿé•·ç­–ç•¥ï¼Œç²¾åº¦å°å‘"""
    n_estimators: int = 1000
    learning_rate: float = 0.05
    max_depth: int = 6
    min_child_weight: int = 1
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    reg_alpha: float = 0.1  # L1 æ­£å‰‡
    reg_lambda: float = 1.0  # L2 æ­£å‰‡
    gamma: float = 0  # ç¯€é»åˆ†è£‚æœ€å°æå¤±æ¸›å°‘
    early_stopping_rounds: int = 50
    eval_metric: str = "rmse"
    tree_method: str = "hist"  # 'exact', 'approx', 'hist'
    
    # é€²éšåŠŸèƒ½
    enable_monotonic_constraints: bool = False  # è‹¥ç‰©ç†é—œä¿‚å·²çŸ¥ï¼ˆå¦‚æº«åº¦è¶Šé«˜è€—é›»è¶Šé«˜ï¼‰
    monotone_constraints: Optional[Dict[str, int]] = None  # {"temp_outdoor": 1, "efficiency": -1}

class LightGBMConfig(BaseModel):
    """LightGBM å°ˆå±¬é…ç½® - Leaf-wise ç”Ÿé•·ç­–ç•¥ï¼Œé€Ÿåº¦å°å‘"""
    n_estimators: int = 1000
    learning_rate: float = 0.05
    num_leaves: int = 31  # æ§åˆ¶æ¨¡å‹è¤‡é›œåº¦ï¼Œç›¸ç•¶æ–¼ 2^max_depth
    max_depth: int = -1  # -1 è¡¨ç¤ºç„¡é™åˆ¶ï¼Œç”± num_leaves æ§åˆ¶
    min_child_samples: int = 20
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    reg_alpha: float = 0.1
    reg_lambda: float = 1.0
    early_stopping_rounds: int = 50
    eval_metric: str = "rmse"
    boosting_type: str = "gbdt"  # 'gbdt', 'dart', 'goss'
    
    # å¤§è¦æ¨¡è³‡æ–™å„ªåŒ–
    feature_pre_filter: bool = False
    histogram_pool_size: Optional[int] = None  # è¨˜æ†¶é«”é™åˆ¶æ™‚è¨­å®š

class RandomForestConfig(BaseModel):
    """Random Forest å°ˆå±¬é…ç½® - Bagging ç­–ç•¥ï¼Œé²æ£’æ€§å°å‘"""
    n_estimators: int = 500
    max_depth: Optional[int] = None  # None è¡¨ç¤ºå®Œå…¨ç”Ÿé•·
    min_samples_split: int = 5
    min_samples_leaf: int = 2
    max_features: str = "sqrt"  # 'sqrt', 'log2', None
    bootstrap: bool = True
    oob_score: bool = True  # Out-of-Bag é©—è­‰
    n_jobs: int = -1  # ä½¿ç”¨æ‰€æœ‰ CPU
    warm_start: bool = False  # å¯å¢é‡è¨“ç·´
    
    # å€é–“é æ¸¬ï¼ˆä½¿ç”¨æ¨¹çš„è‘‰ç¯€é»çµ±è¨ˆï¼‰
    quantile_regression: bool = False  # è‹¥å•Ÿç”¨ï¼Œè¨“ç·´ä¸‰å€‹æ¨¡å‹ (Q10, Q50, Q90)

# ==========================================
# è¨“ç·´ç®¡ç·šä¸»é…ç½®
# ==========================================

class ModelTrainingConfig(BaseModel):
    """æ¨¡å‹è¨“ç·´çµ±ä¸€é…ç½®ï¼ˆæ”¯æ´ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´ï¼‰"""
    
    # åŸºæœ¬é…ç½®
    random_state: int = 42
    parallel_training: bool = True  # æ˜¯å¦åŒæ™‚è¨“ç·´ä¸‰æ¨¡å‹
    auto_select_best: bool = True   # è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹
    ensemble_voting: bool = False   # æ˜¯å¦ä¿ç•™ä¸‰æ¨¡å‹åšæŠ•ç¥¨å¹³å‡
    
    # æ™‚åºé…ç½®
    temporal_split: TemporalSplitConfig = TemporalSplitConfig()
    
    # Device Role è™•ç†
    device_role_handling: DeviceRoleHandlingConfig = DeviceRoleHandlingConfig()
    
    # ç‰¹å¾µå·¥ç¨‹ï¼ˆè¨“ç·´æœŸï¼‰
    handle_missing_values: Literal["drop", "impute_mean", "impute_median"] = "impute_median"
    scale_features: bool = True  # å° SVM/NN å¿…è¦ï¼Œå°æ¨¹æ¨¡å‹å¯é¸ä½†å»ºè­°çµ±ä¸€
    
    # Quality Flags è™•ç†
    use_quality_flags_as_features: bool = True
    exclude_bad_quality_samples: bool = True
    
    # ä¸‰æ¨¡å‹é…ç½®
    xgboost: XGBoostConfig = XGBoostConfig()
    lightgbm: LightGBMConfig = LightGBMConfig()
    random_forest: RandomForestConfig = RandomForestConfig()
    
    # è¶…åƒæ•¸æœå°‹ï¼ˆå¯é¸ï¼‰
    enable_hyperparameter_search: bool = False
    hyperparameter_trials: int = 50  # Optuna trials
    hyperparameter_timeout: int = 3600  # ç§’
    
    # è¼¸å‡º
    model_output_dir: str = "models/trained"
    metadata_output_dir: str = "models/metadata"
    
    @validator('device_role_handling')
    def validate_no_feature_leakage(cls, v):
        if v.use_as_feature:
            raise ValueError("E701: device_role ç¦æ­¢ä½œç‚ºç›´æ¥ç‰¹å¾µè¼¸å…¥")
        return v
```

#### Step 0.2: å¤šæ¨¡å‹è¨“ç·´å™¨åŸºç¤é¡åˆ¥

**æª”æ¡ˆ**: `src/modeling/trainers/base_trainer.py`

**å¯¦ä½œå…§å®¹**:
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple, Optional
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

class BaseModelTrainer(ABC):
    """
    æ¨¡å‹è¨“ç·´å™¨æŠ½è±¡åŸºç¤é¡åˆ¥
    
    æ‰€æœ‰å…·é«”æ¨¡å‹è¨“ç·´å™¨ï¼ˆXGBoostTrainer, LightGBMTrainer, RandomForestTrainerï¼‰
    å¿…é ˆå¯¦ä½œä»¥ä¸‹ä»‹é¢ã€‚
    """
    
    def __init__(self, config: Any, random_state: int = 42):
        self.config = config
        self.random_state = random_state
        self.model = None
        self.feature_importance = {}
        self.training_history = {}
    
    @abstractmethod
    def train(
        self, 
        X_train: np.ndarray, 
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ¨¡å‹è¨“ç·´
        
        Returns:
            Dict åŒ…å«:
            - model: è¨“ç·´å¥½çš„æ¨¡å‹ç‰©ä»¶
            - best_iteration: æœ€ä½³è¿­ä»£æ¬¡æ•¸ï¼ˆæ¢¯åº¦æå‡é¡ï¼‰
            - training_history: è¨“ç·´éç¨‹æŒ‡æ¨™
            - feature_importance: ç‰¹å¾µé‡è¦æ€§å­—å…¸
            - oob_score: Out-of-Bag åˆ†æ•¸ï¼ˆè‹¥æœ‰ï¼‰
        """
        pass
    
    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """åŸ·è¡Œé æ¸¬"""
        pass
    
    @abstractmethod
    def get_feature_importance(self) -> Dict[str, float]:
        """å–å¾—æ¨™æº–åŒ–ç‰¹å¾µé‡è¦æ€§ï¼ˆç¸½å’Œç‚º1ï¼‰"""
        pass
    
    def evaluate(self, X: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:
        """çµ±ä¸€è©•ä¼°æŒ‡æ¨™"""
        y_pred = self.predict(X)
        return {
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred),
            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        }
```

---

### Phase 1: ä¸‰æ¨¡å‹å…·é«”å¯¦ä½œ (Day 2-3)

#### Step 1.1: XGBoost è¨“ç·´å™¨å¯¦ä½œ

**æª”æ¡ˆ**: `src/modeling/trainers/xgboost_trainer.py`

**å¯¦ä½œå…§å®¹**:
```python
import xgboost as xgb
import numpy as np
from typing import Dict, Any, Optional, List
from src.modeling.trainers.base_trainer import BaseModelTrainer

class XGBoostTrainer(BaseModelTrainer):
    """
    XGBoost è¨“ç·´å™¨å¯¦ä½œ
    
    ç‰¹æ€§:
    - Level-wise æ¨¹ç”Ÿé•·ï¼ˆå¹³è¡¡æ¨¹æ·±åº¦ï¼‰
    - å…§å»ºæ—©åœæ©Ÿåˆ¶ (Early Stopping)
    - æ”¯æ´æ¨£æœ¬æ¬Šé‡ (Sample Weight)
    - ç‰¹å¾µé‡è¦æ€§åŸºæ–¼ Gainï¼ˆåˆ†è£‚æå¤±æ”¹å–„ï¼‰
    """
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ XGBoost è¨“ç·´"""
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = xgb.XGBRegressor(
            n_estimators=self.config.n_estimators,
            learning_rate=self.config.learning_rate,
            max_depth=self.config.max_depth,
            min_child_weight=self.config.min_child_weight,
            subsample=self.config.subsample,
            colsample_bytree=self.config.colsample_bytree,
            reg_alpha=self.config.reg_alpha,
            reg_lambda=self.config.reg_lambda,
            gamma=self.config.gamma,
            eval_metric=self.config.eval_metric,
            tree_method=self.config.tree_method,
            random_state=self.random_state,
            n_jobs=-1,
            verbosity=0
        )
        
        # è‹¥æœ‰å–®èª¿æ€§ç´„æŸï¼ˆç‰©ç†é—œä¿‚ï¼‰
        if self.config.enable_monotonic_constraints and self.config.monotone_constraints:
            # è½‰æ›ç‚º XGBoost æ ¼å¼: (0: ç„¡, 1: æ­£ç›¸é—œ, -1: è² ç›¸é—œ)
            mono_constraints = [self.config.monotone_constraints.get(f, 0) for f in feature_names]
            self.model.set_params(monotone_constraints=mono_constraints)
        
        # è¨“ç·´ï¼ˆå«æ—©åœï¼‰
        eval_set = [(X_train, y_train), (X_val, y_val)]
        eval_metric = self.config.eval_metric
        
        self.model.fit(
            X_train, y_train,
            sample_weight=sample_weights,
            eval_set=eval_set,
            eval_metric=eval_metric,
            early_stopping_rounds=self.config.early_stopping_rounds,
            verbose=False
        )
        
        # æå–è¨“ç·´æ­·å²
        results = self.model.evals_result()
        self.training_history = {
            'train_rmse': results['validation_0'][eval_metric],
            'val_rmse': results['validation_1'][eval_metric],
            'best_iteration': self.model.best_iteration,
            'best_score': self.model.best_score
        }
        
        # æå–ç‰¹å¾µé‡è¦æ€§ (Gain-based)
        importance = self.model.feature_importances_
        if feature_names:
            self.feature_importance = dict(zip(feature_names, importance))
        else:
            self.feature_importance = {f"feat_{i}": imp for i, imp in enumerate(importance)}
        
        return {
            'model': self.model,
            'best_iteration': self.model.best_iteration,
            'training_history': self.training_history,
            'feature_importance': self.feature_importance,
            'oob_score': None  # XGBoost ä¸æ”¯æ´ OOB
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè¨“ç·´")
        return self.model.predict(X, iteration_range=(0, self.model.best_iteration + 1))
    
    def get_feature_importance(self) -> Dict[str, float]:
        # æ¨™æº–åŒ–è‡³ç¸½å’Œç‚º1
        total = sum(self.feature_importance.values())
        return {k: v/total for k, v in self.feature_importance.items()}
```

#### Step 1.2: LightGBM è¨“ç·´å™¨å¯¦ä½œ

**æª”æ¡ˆ**: `src/modeling/trainers/lightgbm_trainer.py`

**å¯¦ä½œå…§å®¹**:
```python
import lightgbm as lgb
import numpy as np
from typing import Dict, Any, Optional, List
from src.modeling.trainers.base_trainer import BaseModelTrainer

class LightGBMTrainer(BaseModelTrainer):
    """
    LightGBM è¨“ç·´å™¨å¯¦ä½œ
    
    ç‰¹æ€§:
    - Leaf-wise æ¨¹ç”Ÿé•·ï¼ˆæ›´é«˜æ•ˆï¼Œä½†éœ€æ§åˆ¶ max_depth é¿å…éæ“¬åˆï¼‰
    - åŸç”Ÿæ”¯æ´é¡åˆ¥ç‰¹å¾µï¼ˆä½† HVAC å¤šç‚ºæ•¸å€¼ï¼‰
    - è¨“ç·´é€Ÿåº¦æ¥µå¿«ï¼Œè¨˜æ†¶é«”æ•ˆç‡é«˜
    - ç‰¹å¾µé‡è¦æ€§åŸºæ–¼ Split æ¬¡æ•¸
    """
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ LightGBM è¨“ç·´"""
        
        # å»ºç«‹ Datasetï¼ˆLightGBM å°ˆç”¨è³‡æ–™çµæ§‹ï¼Œè¨˜æ†¶é«”æ•ˆç‡é«˜ï¼‰
        train_data = lgb.Dataset(
            X_train, 
            label=y_train, 
            weight=sample_weights,
            feature_name=feature_names,
            free_raw_data=False
        )
        val_data = lgb.Dataset(
            X_val, 
            label=y_val,
            reference=train_data,
            feature_name=feature_names
        )
        
        # è¶…åƒæ•¸
        params = {
            'objective': 'regression',
            'metric': self.config.eval_metric,
            'boosting_type': self.config.boosting_type,
            'num_leaves': self.config.num_leaves,
            'max_depth': self.config.max_depth,
            'learning_rate': self.config.learning_rate,
            'feature_fraction': self.config.colsample_bytree,
            'bagging_fraction': self.config.subsample,
            'bagging_freq': 5,
            'lambda_l1': self.config.reg_alpha,
            'lambda_l2': self.config.reg_lambda,
            'min_child_samples': self.config.min_child_samples,
            'verbose': -1,
            'random_state': self.random_state,
            'feature_pre_filter': self.config.feature_pre_filter
        }
        
        if self.config.histogram_pool_size:
            params['histogram_pool_size'] = self.config.histogram_pool_size
        
        # è¨“ç·´ï¼ˆå«æ—©åœï¼‰
        self.model = lgb.train(
            params,
            train_data,
            num_boost_round=self.config.n_estimators,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'val'],
            callbacks=[lgb.early_stopping(stopping_rounds=self.config.early_stopping_rounds, verbose=False)]
        )
        
        # æå–è¨“ç·´æ­·å²
        self.training_history = {
            'train_rmse': self.model.params.get('train', {}).get('rmse', []),
            'val_rmse': self.model.params.get('val', {}).get('rmse', []),
            'best_iteration': self.model.best_iteration,
            'best_score': self.model.best_score
        }
        
        # ç‰¹å¾µé‡è¦æ€§ (Split-based)
        importance_split = self.model.feature_importance(importance_type='split')
        importance_gain = self.model.feature_importance(importance_type='gain')
        
        if feature_names:
            self.feature_importance = dict(zip(feature_names, importance_gain))  # ä½¿ç”¨ Gain è¼ƒç©©å®š
        else:
            self.feature_importance = {f"feat_{i}": imp for i, imp in enumerate(importance_gain)}
        
        return {
            'model': self.model,
            'best_iteration': self.model.best_iteration,
            'training_history': self.training_history,
            'feature_importance': self.feature_importance,
            'oob_score': None
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè¨“ç·´")
        return self.model.predict(X, num_iteration=self.model.best_iteration)
    
    def get_feature_importance(self) -> Dict[str, float]:
        total = sum(self.feature_importance.values())
        return {k: v/total for k, v in self.feature_importance.items()}
```

#### Step 1.3: Random Forest è¨“ç·´å™¨å¯¦ä½œ

**æª”æ¡ˆ**: `src/modeling/trainers/random_forest_trainer.py`

**å¯¦ä½œå…§å®¹**:
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
import numpy as np
from typing import Dict, Any, Optional, List
from src.modeling.trainers.base_trainer import BaseModelTrainer

class RandomForestTrainer(BaseModelTrainer):
    """
    Random Forest è¨“ç·´å™¨å¯¦ä½œ
    
    ç‰¹æ€§:
    - Bagging ç­–ç•¥ï¼ˆå¹³è¡Œæ¨¹ï¼Œé™ä½æ–¹å·®ï¼‰
    - å¤©ç„¶æ”¯æ´ OOB (Out-of-Bag) é©—è­‰ï¼ˆç„¡éœ€ç¨ç«‹é©—è­‰é›†ï¼‰
    - å°ç•°å¸¸å€¼é²æ£’
    - å¯è¼¸å‡ºé æ¸¬å€é–“ï¼ˆä½¿ç”¨æ‰€æœ‰æ¨¹çš„é æ¸¬åˆ†ä½ˆï¼‰
    - ç‰¹å¾µé‡è¦æ€§åŸºæ–¼ Mean Decrease Impurity (MDI)
    """
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray = None,  # RF å¯ä¸ä½¿ç”¨ç¨ç«‹é©—è­‰é›†ï¼ˆä½¿ç”¨ OOBï¼‰
        y_val: np.ndarray = None,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ Random Forest è¨“ç·´"""
        
        self.model = RandomForestRegressor(
            n_estimators=self.config.n_estimators,
            max_depth=self.config.max_depth,
            min_samples_split=self.config.min_samples_split,
            min_samples_leaf=self.config.min_samples_leaf,
            max_features=self.config.max_features,
            bootstrap=self.config.bootstrap,
            oob_score=self.config.oob_score,
            n_jobs=self.config.n_jobs,
            random_state=self.random_state,
            warm_start=self.config.warm_start,
            verbose=0
        )
        
        # è¨“ç·´ï¼ˆRF ä¸æ”¯æ´æ—©åœï¼Œä½†æ”¯æ´ warm_start å¢é‡è¨“ç·´ï¼‰
        self.model.fit(X_train, y_train, sample_weight=sample_weights)
        
        # OOB åˆ†æ•¸ï¼ˆè‹¥å•Ÿç”¨ bootstrapï¼‰
        oob_score = self.model.oob_score_ if self.config.oob_score and self.config.bootstrap else None
        
        # è¨“ç·´æ­·å²ï¼ˆRF ç„¡è¿­ä»£æ­·å²ï¼Œè¨˜éŒ„æœ€çµ‚æ€§èƒ½ï¼‰
        train_metrics = self.evaluate(X_train, y_train)
        val_metrics = self.evaluate(X_val, y_val) if X_val is not None else {}
        
        self.training_history = {
            'train_rmse': train_metrics['rmse'],
            'val_rmse': val_metrics.get('rmse'),
            'oob_r2': oob_score,
            'n_estimators': self.config.n_estimators
        }
        
        # ç‰¹å¾µé‡è¦æ€§ (MDI - Mean Decrease Impurity)
        importance = self.model.feature_importances_
        if feature_names:
            self.feature_importance = dict(zip(feature_names, importance))
        else:
            self.feature_importance = {f"feat_{i}": imp for i, imp in enumerate(importance)}
        
        return {
            'model': self.model,
            'best_iteration': None,  # RF ç„¡è¿­ä»£æ¦‚å¿µ
            'training_history': self.training_history,
            'feature_importance': self.feature_importance,
            'oob_score': oob_score
        }
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè¨“ç·´")
        return self.model.predict(X)
    
    def predict_with_interval(self, X: np.ndarray, confidence: float = 0.9) -> Dict[str, np.ndarray]:
        """
        è¼¸å‡ºé æ¸¬å€é–“ï¼ˆä½¿ç”¨æ‰€æœ‰æ¨¹çš„é æ¸¬åˆ†ä½ˆï¼‰
        
        Returns:
            {
                'mean': å¹³å‡é æ¸¬å€¼,
                'lower': ä¸‹ç•Œ (Q5),
                'upper': ä¸Šç•Œ (Q95),
                'std': æ¨™æº–å·®
            }
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        # å–å¾—æ‰€æœ‰æ¨¹çš„é æ¸¬ (n_samples, n_trees)
        all_predictions = np.array([tree.predict(X) for tree in self.model.estimators_])
        
        mean_pred = np.mean(all_predictions, axis=0)
        std_pred = np.std(all_predictions, axis=0)
        
        # è¨ˆç®—åˆ†ä½æ•¸
        lower = np.percentile(all_predictions, (1 - confidence) * 100 / 2, axis=0)
        upper = np.percentile(all_predictions, 100 - (1 - confidence) * 100 / 2, axis=0)
        
        return {
            'mean': mean_pred,
            'lower': lower,
            'upper': upper,
            'std': std_pred
        }
    
    def get_feature_importance(self) -> Dict[str, float]:
        total = sum(self.feature_importance.values())
        return {k: v/total for k, v in self.feature_importance.items()}
```

---

### Phase 2: å¤šæ¨¡å‹è¨“ç·´ç®¡ç·šæ•´åˆ (Day 4)

#### Step 2.1: å¹³è¡Œè¨“ç·´èˆ‡æ¨¡å‹é¸æ“‡é‚è¼¯

**æª”æ¡ˆ**: `src/modeling/training_pipeline.py`ï¼ˆæ ¸å¿ƒæ›´æ–°ï¼‰

**å¯¦ä½œå…§å®¹**:
```python
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Tuple, Any
import numpy as np

from src.modeling.trainers.xgboost_trainer import XGBoostTrainer
from src.modeling.trainers.lightgbm_trainer import LightGBMTrainer
from src.modeling.trainers.random_forest_trainer import RandomForestTrainer

class TrainingPipeline:
    """
    å¤šæ¨¡å‹è¨“ç·´ç®¡ç·š v1.0
    
    åŒæ™‚è¨“ç·´ XGBoostã€LightGBMã€Random Forestï¼Œ
    ä¸¦ä¾é©—è­‰æŒ‡æ¨™è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹æˆ–ä¿ç•™ Ensembleã€‚
    """
    
    def __init__(self, config: ModelTrainingConfig, site_id: str, yaml_base_dir: str = "config/features/sites"):
        self.config = config
        self.site_id = site_id
        self.annotation_manager = FeatureAnnotationManager(site_id=site_id, yaml_base_dir=yaml_base_dir)
        self._validate_annotation_compatibility()
        
        self.trainers = {}
        self.results = {}
        self.best_model_name = None
        
    def train_all_models(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        sample_weights: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        è¨“ç·´æ‰€æœ‰ä¸‰å€‹æ¨¡å‹
        
        è‹¥ parallel_training=Trueï¼Œä½¿ç”¨å¤šé€²ç¨‹å¹³è¡Œè¨“ç·´ï¼›
        å¦å‰‡ä¾åºè¨“ç·´ï¼ˆé©åˆè¨˜æ†¶é«”å—é™ç’°å¢ƒï¼‰ã€‚
        """
        trainers_config = {
            'xgboost': (XGBoostTrainer, self.config.xgboost),
            'lightgbm': (LightGBMTrainer, self.config.lightgbm),
            'random_forest': (RandomForestTrainer, self.config.random_forest)
        }
        
        if self.config.parallel_training:
            # å¹³è¡Œè¨“ç·´ï¼ˆæ³¨æ„ï¼šXGBoost èˆ‡ LightGBM å„è‡ªæœƒä½¿ç”¨å¤šåŸ·è¡Œç·’ï¼Œéœ€æ§åˆ¶ç¸½è³‡æºï¼‰
            with ProcessPoolExecutor(max_workers=3) as executor:
                futures = {}
                for name, (TrainerClass, model_config) in trainers_config.items():
                    future = executor.submit(
                        self._train_single_model,
                        name, TrainerClass, model_config,
                        X_train, y_train, X_val, y_val,
                        sample_weights, feature_names
                    )
                    futures[future] = name
                
                for future in as_completed(futures):
                    name = futures[future]
                    try:
                        self.results[name] = future.result()
                        self.logger.info(f"âœ… {name} è¨“ç·´å®Œæˆ")
                    except Exception as e:
                        self.logger.error(f"âŒ {name} è¨“ç·´å¤±æ•—: {e}")
                        self.results[name] = {'error': str(e)}
        else:
            # ä¾åºè¨“ç·´
            for name, (TrainerClass, model_config) in trainers_config.items():
                try:
                    self.results[name] = self._train_single_model(
                        name, TrainerClass, model_config,
                        X_train, y_train, X_val, y_val,
                        sample_weights, feature_names
                    )
                    self.logger.info(f"âœ… {name} è¨“ç·´å®Œæˆ")
                except Exception as e:
                    self.logger.error(f"âŒ {name} è¨“ç·´å¤±æ•—: {e}")
                    self.results[name] = {'error': str(e)}
        
        # é¸æ“‡æœ€ä½³æ¨¡å‹
        if self.config.auto_select_best:
            self.best_model_name = self._select_best_model()
        
        return self.results
    
    def _train_single_model(
        self,
        name: str,
        TrainerClass,
        model_config,
        X_train, y_train, X_val, y_val,
        sample_weights, feature_names
    ) -> Dict[str, Any]:
        """è¨“ç·´å–®ä¸€æ¨¡å‹"""
        trainer = TrainerClass(config=model_config, random_state=self.config.random_state)
        result = trainer.train(
            X_train=X_train, y_train=y_train,
            X_val=X_val, y_val=y_val,
            sample_weights=sample_weights,
            feature_names=feature_names
        )
        result['metrics'] = {
            'train': trainer.evaluate(X_train, y_train),
            'val': trainer.evaluate(X_val, y_val)
        }
        self.trainers[name] = trainer
        return result
    
    def _select_best_model(self) -> str:
        """
        é¸æ“‡æœ€ä½³æ¨¡å‹
        
        ç­–ç•¥:
        1. å„ªå…ˆæ¯”è¼ƒé©—è­‰é›† RÂ² åˆ†æ•¸
        2. è‹¥ RÂ² å·®è· < 0.01ï¼Œé¸æ“‡è¨“ç·´æ™‚é–“è¼ƒçŸ­çš„ï¼ˆLightGBM > XGBoost > RFï¼‰
        3. è‹¥ RF çš„ OOB åˆ†æ•¸èˆ‡é©—è­‰é›†å·®è·éå¤§ï¼ˆ>0.1ï¼‰ï¼Œå¯èƒ½è¡¨ç¤ºè³‡æ–™æ´©æ¼ï¼Œé™ä½æ’å
        """
        valid_results = {
            name: res for name, res in self.results.items() 
            if 'error' not in res and 'metrics' in res
        }
        
        if not valid_results:
            raise ModelTrainingError("æ‰€æœ‰æ¨¡å‹è¨“ç·´å¤±æ•—")
        
        # æ’åºï¼šVal R2 é«˜åˆ°ä½
        ranked = sorted(
            valid_results.items(),
            key=lambda x: x[1]['metrics']['val']['r2'],
            reverse=True
        )
        
        best_name, best_result = ranked[0]
        best_r2 = best_result['metrics']['val']['r2']
        
        self.logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_name} (Val RÂ²={best_r2:.4f})")
        
        # è¨˜éŒ„æ‰€æœ‰æ¨¡å‹æ¯”è¼ƒ
        for name, result in ranked:
            r2 = result['metrics']['val']['r2']
            rmse = result['metrics']['val']['rmse']
            self.logger.info(f"   {name}: RÂ²={r2:.4f}, RMSE={rmse:.4f}")
        
        return best_name
    
    def get_best_model(self) -> Tuple[str, BaseModelTrainer, Dict]:
        """å–å¾—æœ€ä½³æ¨¡å‹åŠå…¶çµæœ"""
        if self.best_model_name is None:
            raise RuntimeError("å°šæœªåŸ·è¡Œæ¨¡å‹é¸æ“‡")
        return (
            self.best_model_name,
            self.trainers[self.best_model_name],
            self.results[self.best_model_name]
        )
    
    def predict_ensemble(self, X: np.ndarray, weights: Optional[Dict[str, float]] = None) -> np.ndarray:
        """
        Ensemble é æ¸¬ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
        
        è‹¥ weights ç‚º Noneï¼Œä½¿ç”¨é©—è­‰é›† RÂ² ä½œç‚ºæ¬Šé‡åŸºç¤ã€‚
        """
        if not self.trainers:
            raise RuntimeError("å°šæœªè¨“ç·´æ¨¡å‹")
        
        predictions = []
        model_weights = []
        
        for name, trainer in self.trainers.items():
            if 'error' in self.results[name]:
                continue
            pred = trainer.predict(X)
            predictions.append(pred)
            
            if weights and name in weights:
                model_weights.append(weights[name])
            else:
                # ä½¿ç”¨ Val RÂ² ä½œç‚ºæ¬Šé‡ï¼ˆéœ€æ­£è¦åŒ–ï¼‰
                r2 = max(0, self.results[name]['metrics']['val']['r2'])  # é¿å…è² å€¼
                model_weights.append(r2)
        
        # åŠ æ¬Šå¹³å‡
        weights_arr = np.array(model_weights) / sum(model_weights)
        ensemble_pred = np.average(predictions, axis=0, weights=weights_arr)
        
        return ensemble_pred
```

---

### Phase 3: å®Œæ•´è¨“ç·´æµç¨‹èˆ‡ç”¢å‡º (Day 5)

#### Step 3.1: å®Œæ•´è¨“ç·´æµç¨‹ï¼ˆæ•´åˆä¸‰æ¨¡å‹ï¼‰

**æ–¹æ³•**: `train(data: TrainingInputContract) -> MultiModelArtifact`

**å¯¦ä½œå…§å®¹**:
```python
def train(self, data: Dict) -> 'MultiModelArtifact':
    """
    åŸ·è¡Œå®Œæ•´å¤šæ¨¡å‹è¨“ç·´æµç¨‹
    
    Returns:
        MultiModelArtifact: åŒ…å«ä¸‰æ¨¡å‹çµæœèˆ‡æœ€ä½³æ¨¡å‹é¸æ“‡
    """
    # Step 1-4: è³‡æ–™æº–å‚™ï¼ˆèˆ‡å…ˆå‰ç›¸åŒï¼Œç•¥ï¼‰
    self._validate_input_contract(data)
    df = data['feature_matrix']
    target_col = data['target_variable']
    
    train_df, val_df, test_df, y_train, y_val, y_test = self._temporal_split(df, target_col)
    sample_weights, seasonal_mask = self._compute_sample_weights_and_masks(train_df)
    
    # æ‡‰ç”¨é®ç½©
    if np.any(seasonal_mask == False):
        train_df = train_df.filter(pl.Series(seasonal_mask))
        y_train = y_train.filter(pl.Series(seasonal_mask))
        sample_weights = sample_weights[seasonal_mask]
    
    X_train, X_val, X_test, feature_cols = self._preprocess_features(train_df, val_df, test_df)
    
    # Step 5: å¤šæ¨¡å‹è¨“ç·´
    self.train_all_models(
        X_train=X_train, y_train=y_train.to_numpy(),
        X_val=X_val, y_val=y_val.to_numpy(),
        sample_weights=sample_weights,
        feature_names=feature_cols
    )
    
    # Step 6: æ¸¬è©¦é›†æœ€çµ‚è©•ä¼°ï¼ˆåƒ…æœ€ä½³æ¨¡å‹ï¼‰
    best_name, best_trainer, best_result = self.get_best_model()
    test_metrics = best_trainer.evaluate(X_test, y_test.to_numpy())
    
    self.logger.info(f"ğŸ§ª æœ€ä½³æ¨¡å‹æ¸¬è©¦é›†è¡¨ç¾: RÂ²={test_metrics['r2']:.4f}, RMSE={test_metrics['rmse']:.4f}")
    
    # Step 7: å»ºç«‹å¤šæ¨¡å‹ç”¢å‡ºç‰©
    artifact = MultiModelArtifact(
        trainers=self.trainers,
        results=self.results,
        best_model_name=best_name,
        test_metrics=test_metrics,
        training_metadata=self._build_training_metadata(data, test_metrics),
        annotation_context=data['annotation_context'],
        feature_names=feature_cols,
        config=self.config
    )
    
    return artifact
```

#### Step 3.2: å¤šæ¨¡å‹ç”¢å‡ºç‰©å®šç¾©

**æª”æ¡ˆ**: `src/modeling/artifacts.py`ï¼ˆæ›´æ–°ï¼‰

**å¯¦ä½œå…§å®¹**:
```python
@dataclass
class MultiModelArtifact:
    """
    å¤šæ¨¡å‹è¨“ç·´ç”¢å‡ºç‰©
    
    å„²å­˜çµæ§‹:
    models/
    â””â”€â”€ {site_id}/
        â”œâ”€â”€ ensemble_manifest.json           # çµ±ä¸€å…¥å£
        â”œâ”€â”€ xgboost_model.joblib
        â”œâ”€â”€ xgboost_metadata.json
        â”œâ”€â”€ lightgbm_model.joblib
        â”œâ”€â”€ lightgbm_metadata.json
        â”œâ”€â”€ random_forest_model.joblib
        â””â”€â”€ random_forest_metadata.json
    """
    
    trainers: Dict[str, BaseModelTrainer]
    results: Dict[str, Dict[str, Any]]
    best_model_name: str
    test_metrics: Dict[str, float]
    training_metadata: Dict[str, Any]
    annotation_context: Dict[str, Any]
    feature_names: List[str]
    config: ModelTrainingConfig
    
    def save(self, output_dir: Path) -> Dict[str, Path]:
        """å„²å­˜æ‰€æœ‰æ¨¡å‹èˆ‡å…ƒè³‡æ–™"""
        output_dir = Path(output_dir) / self.training_metadata['site_id']
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {'ensemble_manifest': output_dir / f"{timestamp}_ensemble_manifest.json"}
        
        ensemble_data = {
            'timestamp': timestamp,
            'best_model': self.best_model_name,
            'test_metrics': self.test_metrics,
            'models': {}
        }
        
        # å„²å­˜æ¯å€‹æ¨¡å‹
        for name, trainer in self.trainers.items():
            if 'error' in self.results[name]:
                continue
            
            model_path = output_dir / f"{timestamp}_{name}_model.joblib"
            metadata_path = output_dir / f"{timestamp}_{name}_metadata.json"
            
            # å„²å­˜æ¨¡å‹
            joblib.dump({
                'model': trainer.model,
                'scaler': getattr(trainer, 'scaler', None),
                'feature_names': self.feature_names
            }, model_path)
            
            # å„²å­˜è©²æ¨¡å‹å…ƒè³‡æ–™
            model_meta = {
                'name': name,
                'metrics': self.results[name]['metrics'],
                'feature_importance': trainer.get_feature_importance(),
                'training_history': self.results[name].get('training_history', {}),
                'best_iteration': self.results[name].get('best_iteration'),
                'oob_score': self.results[name].get('oob_score')
            }
            
            with open(metadata_path, 'w') as f:
                json.dump(model_meta, f, indent=2, default=str)
            
            ensemble_data['models'][name] = {
                'model_file': str(model_path.name),
                'metadata_file': str(metadata_path.name),
                'val_r2': self.results[name]['metrics']['val']['r2'],
                'test_r2': self.test_metrics['r2'] if name == self.best_model_name else None
            }
        
        # å„²å­˜ Ensemble Manifest
        ensemble_data['training_metadata'] = self.training_metadata
        ensemble_data['annotation_context'] = self.annotation_context
        
        with open(saved_files['ensemble_manifest'], 'w') as f:
            json.dump(ensemble_data, f, indent=2, default=str)
        
        return saved_files
    
    @classmethod
    def load(cls, ensemble_manifest_path: Path, model_name: Optional[str] = None):
        """è¼‰å…¥æŒ‡å®šæ¨¡å‹æˆ–æœ€ä½³æ¨¡å‹"""
        with open(ensemble_manifest_path, 'r') as f:
            manifest = json.load(f)
        
        model_to_load = model_name or manifest['best_model']
        model_info = manifest['models'][model_to_load]
        
        # è¼‰å…¥å…·é«”æ¨¡å‹
        model_dir = ensemble_manifest_path.parent
        model_data = joblib.load(model_dir / model_info['model_file'])
        
        return model_data, manifest
```

---

## 4. éŒ¯èª¤ä»£ç¢¼å°ç…§è¡¨ (Error Codes)

| éŒ¯èª¤ä»£ç¢¼ | åç¨± | ç™¼ç”Ÿéšæ®µ | èªªæ˜ | è™•ç†å»ºè­° |
|:---|:---|:---:|:---|:---|
| **E601** | `ANNOTATION_CONTEXT_MISSING` | Step 1.1 | ç¼ºå°‘ annotation_context | ç¢ºèª Feature Engineer v1.3+ |
| **E602** | `SCHEMA_VERSION_MISMATCH` | Step 1.1 | Annotation ç‰ˆæœ¬ä¸ç¬¦ | é‡æ–°è¨“ç·´æˆ–é™ç´š Annotation |
| **E603** | `TARGET_VARIABLE_MISSING` | Step 1.1 | ç›®æ¨™è®Šæ•¸ä¸å­˜åœ¨ | æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹è¼¸å‡º |
| **E604** | `TIMESTAMP_INVALID` | Step 1.1 | æ™‚é–“æˆ³æ ¼å¼éŒ¯èª¤ | æª¢æŸ¥ Feature Engineer |
| **E701** | `DEVICE_ROLE_AS_FEATURE` | Step 0.1 | è¨­å®šéŒ¯èª¤å˜—è©¦å°‡ device_role ä½œç‚ºç‰¹å¾µ | ä¿®æ”¹è¨­å®š |
| **E702** | `INSUFFICIENT_SAMPLES` | Step 3 | æ¨£æœ¬ä¸è¶³ï¼ˆ<100ï¼‰ | æª¢æŸ¥è³‡æ–™é®ç½©é‚è¼¯ |
| **E703** | `ALL_MODELS_FAILED` | Step 5 | ä¸‰æ¨¡å‹å…¨éƒ¨è¨“ç·´å¤±æ•— | æª¢æŸ¥è³‡æ–™å“è³ªæˆ–ç‰¹å¾µå·¥ç¨‹ |
| **E704** | `XGBOOST_IMPORT_ERROR` | Step 1.1 | XGBoost æœªå®‰è£ | `pip install xgboost` |
| **E705** | `LIGHTGBM_IMPORT_ERROR` | Step 1.1 | LightGBM æœªå®‰è£ | `pip install lightgbm` |

---

## 5. æ¸¬è©¦èˆ‡é©—è­‰è¨ˆç•« (Test Plan)

### 5.1 å–®å…ƒæ¸¬è©¦ï¼ˆæ¯å€‹æ¨¡å‹ç¨ç«‹æ¸¬è©¦ï¼‰

| æ¸¬è©¦æ¡ˆä¾‹ ID | æè¿° | é©—è­‰ç›®æ¨™ | æ¨¡å‹ |
|:---|:---|:---:|:---:|
| MT-XGB-001 | XGBoost åŸºæœ¬è¨“ç·´ | æ”¶æ•›ã€æ—©åœç”Ÿæ•ˆã€ç‰¹å¾µé‡è¦æ€§åˆç† | XGBoost |
| MT-XGB-002 | XGBoost æ¨£æœ¬æ¬Šé‡ | é«˜æ¬Šé‡æ¨£æœ¬å½±éŸ¿æ›´å¤§ | XGBoost |
| MT-LGB-001 | LightGBM é€Ÿåº¦æ¸¬è©¦ | ç›¸åŒè³‡æ–™è¨“ç·´æ™‚é–“ < XGBoost 50% | LightGBM |
| MT-LGB-002 | Leaf-wise éæ“¬åˆé˜²è­· | num_leaves æ§åˆ¶æœ‰æ•ˆ | LightGBM |
| MT-RF-001 | OOB åˆ†æ•¸é©—è­‰ | OOB â‰ˆ Val Scoreï¼ˆå·®è· < 5%ï¼‰ | Random Forest |
| MT-RF-002 | é æ¸¬å€é–“è¼¸å‡º | lower < mean < upperï¼Œstd > 0 | Random Forest |
| MT-ENS-001 | Ensemble åŠ æ¬Šå¹³å‡ | åŠ æ¬Šé æ¸¬ä»‹æ–¼å„æ¨¡å‹ä¹‹é–“ | Ensemble |
| MT-SEL-001 | è‡ªå‹•æ¨¡å‹é¸æ“‡ | æ­£ç¢ºé¸æ“‡ Val RÂ² æœ€é«˜è€… | Auto Select |

### 5.2 æ•´åˆæ¸¬è©¦ï¼ˆä¸‰æ¨¡å‹æ¯”è¼ƒï¼‰

| æ¸¬è©¦æ¡ˆä¾‹ ID | æè¿° | é©—è­‰ç›®æ¨™ |
|:---|:---|:---|
| INT-MT-001 | ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´ | åŒæ™‚å®Œæˆï¼Œç„¡è¨˜æ†¶é«”è¡çª |
| INT-MT-002 | HVAC çœŸå¯¦è³‡æ–™æ¸¬è©¦ | è‡³å°‘ä¸€æ¨¡å‹é”åˆ° RÂ² > 0.85 |
| INT-MT-003 | Device Role æ¬Šé‡å½±éŸ¿ | Backup æ¨£æœ¬æ¬Šé‡èª¿æ•´å¾Œï¼Œæ¨¡å‹é æ¸¬ç©©å®š |
| INT-MT-004 | ç‰ˆæœ¬ç¶å®šé©—è­‰ | å„²å­˜çš„ Manifest åŒ…å«æ­£ç¢º yaml_checksum |

---

## 6. ç‰ˆæœ¬ç›¸å®¹æ€§èˆ‡ä¾è³´

### 6.1 Python å¥—ä»¶ä¾è³´

```toml
[project.optional-dependencies]
modeling = [
    "xgboost>=1.7.0",      # æ”¯æ´ early stopping callback
    "lightgbm>=4.0.0",     # æ–°ç‰ˆ API
    "scikit-learn>=1.3.0", # Random Forest, è©•ä¼°æŒ‡æ¨™
    "optuna>=3.0.0",       # å¯é¸ï¼Œè¶…åƒæ•¸æœå°‹
    "joblib>=1.3.0",       # æ¨¡å‹å„²å­˜
]
```

### 6.2 ç¡¬é«”å»ºè­°

| æ¨¡å‹ | è¨˜æ†¶é«”éœ€æ±‚ | CPU æ ¸å¿ƒ | GPU åŠ é€Ÿ |
|:---|:---:|:---:|:---:|
| XGBoost | ä¸­ç­‰ | 4-8 | å¯é¸ (CUDA) |
| LightGBM | ä½ | 4-8 | ä¸å»ºè­°ï¼ˆCPU å·²æ¥µå¿«ï¼‰ |
| Random Forest | é«˜ï¼ˆå¹³è¡Œæ¨¹ï¼‰ | 8+ | ä¸æ”¯æ´ |

---

## 7. é©—æ”¶ç°½æ ¸ (Sign-off Checklist)

- [ ] **ä¸‰æ¨¡å‹å¯¦ä½œ**: XGBoostã€LightGBMã€Random Forest çš†å¯ç¨ç«‹è¨“ç·´
- [ ] **å¹³è¡Œè¨“ç·´**: `parallel_training=True` æ™‚ï¼Œä¸‰æ¨¡å‹åŒæ™‚è¨“ç·´å®Œæˆ
- [ ] **è‡ªå‹•é¸æ“‡**: ä¾ Val RÂ² è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ï¼Œè¨˜éŒ„é¸æ“‡ç†ç”±
- [ ] **æ¨£æœ¬æ¬Šé‡**: ä¸‰æ¨¡å‹çš†æ­£ç¢ºè™•ç† Device Role æ¬Šé‡ï¼ˆBackup=0.3ï¼‰
- [ ] **ç‰¹å¾µé‡è¦æ€§**: æ¯å€‹æ¨¡å‹è¼¸å‡ºæ¨™æº–åŒ–é‡è¦æ€§ï¼ˆç¸½å’Œç‚º1ï¼‰
- [ ] **RF å€é–“é æ¸¬**: Random Forest æ”¯æ´ `predict_with_interval()` è¼¸å‡º Q10/Q90
- [ ] **Ensemble æ”¯æ´**: å¯è¼¸å‡ºä¸‰æ¨¡å‹åŠ æ¬Šå¹³å‡é æ¸¬
- [ ] **ç‰ˆæœ¬ç¶å®š**: å„²å­˜çš„ Manifest åŒ…å« Annotation yaml_checksum
- [ ] **éŒ¯èª¤è™•ç†**: å–®ä¸€æ¨¡å‹å¤±æ•—ä¸å½±éŸ¿å…¶ä»–æ¨¡å‹è¨“ç·´
- [ ] **æ¸¬è©¦è¦†è“‹**: MT-XGB/LGB/RF ç³»åˆ—æ¸¬è©¦å…¨éƒ¨é€šé

---

**é—œéµè¨­è¨ˆç¢ºèª**:
1. **ä¸‰æ¨¡å‹å¹³è¡Œè¨“ç·´**: åŒæ™‚è¨“ç·´ XGBoostï¼ˆç²¾åº¦ï¼‰ã€LightGBMï¼ˆé€Ÿåº¦ï¼‰ã€Random Forestï¼ˆé²æ£’æ€§ï¼‰
2. **è‡ªå‹•é¸æ“‡æ©Ÿåˆ¶**: ä¾ Val RÂ² è‡ªå‹•é¸æ“‡ï¼Œé¿å…äººå·¥é¸æ“‡åèª¤
3. **RF é æ¸¬å€é–“**: åˆ©ç”¨ Bagging ç‰¹æ€§è¼¸å‡ºé æ¸¬ä¸ç¢ºå®šæ€§ï¼Œä¾› Optimization Engine åšé¢¨éšªè©•ä¼°
4. **Device Role çµ±ä¸€è™•ç†**: ä¸‰æ¨¡å‹å…±ç”¨ç›¸åŒæ¨£æœ¬æ¬Šé‡é‚è¼¯ï¼Œç¢ºä¿ä¸€è‡´æ€§